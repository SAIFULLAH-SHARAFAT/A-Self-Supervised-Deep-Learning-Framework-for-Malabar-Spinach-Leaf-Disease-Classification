{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12383979,"sourceType":"datasetVersion","datasetId":7808913}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nfrom typing import Iterable, Tuple, Optional, Callable, List\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision import models as tv_models\nfrom torchvision.models.resnet import ResNet, Bottleneck\nfrom torchvision.models import ResNet50_Weights\nfrom PIL import Image, ImageFilter\n\n\n# ============================================================\n# 0. Global config\n# ============================================================\n\nPRETRAIN_ROOT = \"/kaggle/input/minida/mini_output1/pretrain\" \nIMG_SIZE      = 224\nSEED          = 42\nDEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ============================================================\n# 1. Reproducibility\n# ============================================================\n\ndef seed_all(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\nseed_all(SEED)\n\n\n# ============================================================\n# 2. CBAM modules and CBAM-ResNet50\n# ============================================================\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes: int, ratio: int = 16):\n        super().__init__()\n        hidden = max(in_planes // ratio, 1)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size: int = 7):\n        super().__init__()\n        padding = kernel_size // 2\n        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(x_cat)\n        return self.sigmoid(out)\n\n\nclass CBAMBottleneck(Bottleneck):\n    \"\"\"\n    ResNet Bottleneck with CBAM after the third BN and before the residual add.\n    This means CBAM is applied inside *every* residual bottleneck block.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        planes_out = self.conv3.out_channels\n        self.ca = ChannelAttention(planes_out)\n        self.sa = SpatialAttention()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        identity = x\n\n        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n        out = self.conv3(out); out = self.bn3(out)\n\n        out = self.ca(out) * out\n        out = self.sa(out) * out\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\ndef cbam_resnet50(*, norm_layer: Optional[Callable] = None, **kwargs) -> ResNet:\n    \"\"\"\n    Return a ResNet-50 that uses CBAMBottleneck blocks everywhere.\n    \"\"\"\n    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], norm_layer=norm_layer, **kwargs)\n    # Zero-init last BN in each residual branch (improves training stability).\n    for m in model.modules():\n        if isinstance(m, Bottleneck):\n            nn.init.constant_(m.bn3.weight, 0)\n    return model\n\n\n# ============================================================\n# 3. SimSiam projection/prediction heads\n# ============================================================\n\nclass MLPHead(nn.Module):\n    def __init__(self, in_dim=2048, hidden_dim=2048, out_dim=2048):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim, bias=False),\n            nn.BatchNorm1d(out_dim, affine=False),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass PredictionHead(nn.Module):\n    def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim, bias=False),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_dim, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SimSiamCBAM(nn.Module):\n    \"\"\"\n    SimSiam with CBAM-ResNet50 backbone.\n\n    use_imagenet_init:\n      - False -> CBAM-ResNet50 from scratch\n      - True  -> CBAM-ResNet50 initialized from vanilla ResNet-50 IN1K weights\n    \"\"\"\n    def __init__(self, fix_backbone_bn: bool = True, use_imagenet_init: bool = False):\n        super().__init__()\n\n        if use_imagenet_init:\n            base = tv_models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n            base_sd = base.state_dict()\n\n            cbam = cbam_resnet50(num_classes=1000)\n            cbam_sd = cbam.state_dict()\n\n            copied = 0\n            for k in cbam_sd.keys():\n                if k in base_sd and cbam_sd[k].shape == base_sd[k].shape:\n                    cbam_sd[k] = base_sd[k]\n                    copied += 1\n            cbam.load_state_dict(cbam_sd)\n            print(f\"[SimSiam CBAM] IN1K init: copied {copied} parameters from vanilla ResNet-50.\")\n            resnet = cbam\n        else:\n            resnet = cbam_resnet50(num_classes=1000)\n            print(\"[SimSiam CBAM] Scratch init: CBAM-ResNet50 without ImageNet weights.\")\n\n        # children: conv1, bn1, relu, maxpool, layer1..4, avgpool, fc\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # up to avgpool\n        self.projector = MLPHead(2048)\n        self.predictor = PredictionHead()\n        self.fix_backbone_bn = fix_backbone_bn\n\n        # Keep track of BN layers to freeze\n        self._frozen_bn_modules: List[nn.BatchNorm2d] = []\n        if self.fix_backbone_bn:\n            for m in self.backbone.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n                    m.requires_grad_(False)\n                    self._frozen_bn_modules.append(m)\n\n    def _forward_backbone(self, x):\n        x = self.backbone(x)           # (B, 2048, 1, 1)\n        x = torch.flatten(x, 1)        # (B, 2048)\n        return x\n\n    def forward(self, x1, x2):\n        z1 = self.projector(self._forward_backbone(x1))\n        z2 = self.projector(self._forward_backbone(x2))\n        p1 = self.predictor(z1)\n        p2 = self.predictor(z2)\n        # stop-grad on targets (SimSiam)\n        return p1, p2, z1.detach(), z2.detach()\n\n    def train(self, mode: bool = True):\n        super().train(mode)\n        if self.fix_backbone_bn:\n            for m in self._frozen_bn_modules:\n                m.eval()\n        return self\n\n\n# ============================================================\n# 4. Dataset & Transforms (two views)\n# ============================================================\n\nclass UnlabeledDataset(Dataset):\n    \"\"\"\n    Recursively load all images under PRETRAIN_ROOT. No labels.\n    \"\"\"\n    def __init__(self, root_dir: str, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\", \".tif\", \".tiff\")\n        filepaths = []\n        for dp, _, fns in os.walk(root_dir):\n            for fn in fns:\n                if fn.lower().endswith(exts):\n                    filepaths.append(os.path.join(dp, fn))\n        self.filepaths = sorted(filepaths)\n        if len(self.filepaths) == 0:\n            raise RuntimeError(f\"No images found under {root_dir}\")\n\n    def __len__(self):\n        return len(self.filepaths)\n\n    def __getitem__(self, idx: int):\n        img = Image.open(self.filepaths[idx]).convert(\"RGB\")\n        if self.transform is None:\n            raise RuntimeError(\"Transform must be provided.\")\n        v1 = self.transform(img)\n        v2 = self.transform(img)\n        return v1, v2\n\n\nclass PILGaussianBlur(object):\n    \"\"\"\n    Gaussian blur using PIL.ImageFilter to avoid numpy/torchvision np.bool issues.\n    \"\"\"\n    def __init__(self, radius_min=0.1, radius_max=2.0):\n        self.radius_min = radius_min\n        self.radius_max = radius_max\n\n    def __call__(self, img: Image.Image):\n        radius = random.uniform(self.radius_min, self.radius_max)\n        return img.filter(ImageFilter.GaussianBlur(radius=radius))\n\n\ndef get_ssl_transform(img_size: int = 224):\n    \"\"\"\n    SimSiam-style augmentations:\n    - RandomResizedCrop\n    - RandomHorizontalFlip\n    - ColorJitter (p=0.8)\n    - RandomGrayscale\n    - Random Gaussian blur\n    - ToTensor + Normalize\n    \"\"\"\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n\n    color_jitter = transforms.ColorJitter(\n        brightness=0.4,\n        contrast=0.4,\n        saturation=0.4,\n        hue=0.1,\n    )\n\n    transform = transforms.Compose([\n        transforms.RandomResizedCrop(img_size, scale=(0.2, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomApply([color_jitter], p=0.8),\n        transforms.RandomGrayscale(p=0.2),\n        PILGaussianBlur(radius_min=0.1, radius_max=2.0),\n        transforms.ToTensor(),\n        normalize,\n    ])\n    return transform\n\n\n# ============================================================\n# 5. Loss & optimizer helpers\n# ============================================================\n\ndef negative_cosine_similarity(p, z):\n    p = F.normalize(p, dim=1)\n    z = F.normalize(z, dim=1)\n    return -(p * z).sum(dim=1).mean()\n\n\ndef exclude_from_wd(named_params: Iterable[Tuple[str, torch.nn.Parameter]], wd: float = 1e-4):\n    \"\"\"\n    Create two param groups: with and without weight decay (no wd for BN/bias/1D).\n    \"\"\"\n    wd_params, no_wd_params = [], []\n    for n, p in named_params:\n        if not p.requires_grad:\n            continue\n        if p.ndim == 1 or n.endswith(\".bias\") or \"bn\" in n.lower():\n            no_wd_params.append(p)\n        else:\n            wd_params.append(p)\n    return [\n        {\"params\": wd_params, \"weight_decay\": wd},\n        {\"params\": no_wd_params, \"weight_decay\": 0.0},\n    ]\n\n\n# ============================================================\n# 6. Pretrain function\n# ============================================================\n\ndef pretrain_variant(\n    root_path: str,\n    checkpoint_dir: str,\n    use_imagenet_init: bool,\n    epochs: int = 200,\n    batch_size: int = 64,\n    num_workers: int = 2,\n    accumulation_steps: int = 1,\n    fix_backbone_bn: bool = True,\n    seed: int = 42,\n) -> str:\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    seed_all(seed)\n\n    device = DEVICE\n    print(f\"\\n=================================================\")\n    print(f\"SimSiam+CBAM pretrain | variant = {'IN1K-init' if use_imagenet_init else 'Scratch'}\")\n    print(f\"Device: {device}\")\n    print(f\"Root:   {root_path}\")\n    print(f\"=================================================\\n\")\n\n    transform = get_ssl_transform(IMG_SIZE)\n    dataset = UnlabeledDataset(root_dir=root_path, transform=transform)\n    print(f\"Found {len(dataset)} unlabeled images.\")\n\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=torch.cuda.is_available(),\n        drop_last=True,\n    )\n\n    model = SimSiamCBAM(\n        fix_backbone_bn=fix_backbone_bn,\n        use_imagenet_init=use_imagenet_init\n    ).to(device)\n\n    # Safe torch.compile (optional)\n    use_compile = False\n    if hasattr(torch, \"compile\") and torch.cuda.is_available():\n        try:\n            major, _ = torch.cuda.get_device_capability()\n            if major >= 7:\n                use_compile = True\n        except Exception as e:\n            print(f\"Skipping torch.compile (capability query failed: {e})\")\n\n    if use_compile:\n        try:\n            model = torch.compile(model)\n        except Exception as e:\n            print(f\"torch.compile failed, using eager mode. Reason: {e}\")\n\n    # SimSiam LR scaling rule\n    global_bs = batch_size\n    base_lr = 0.05 * max(min(global_bs, 1024), 64) / 256.0\n    print(f\"Base LR: {base_lr:.5f}\")\n\n    param_groups = exclude_from_wd(model.named_parameters(), wd=1e-4)\n    optimizer = torch.optim.SGD(param_groups, lr=base_lr, momentum=0.9)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n\n    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n        optimizer.zero_grad(set_to_none=True)\n        micro_step = 0\n\n        print(f\"[Variant: {'IN1K' if use_imagenet_init else 'Scratch'}] Epoch {epoch+1}/{epochs}\")\n        for step, (x1, x2) in enumerate(tqdm(dataloader, desc=\"SSL\", leave=False), start=1):\n            x1 = x1.to(device, non_blocking=True)\n            x2 = x2.to(device, non_blocking=True)\n\n            with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n                p1, p2, z1, z2 = model(x1, x2)\n                loss_full = 0.5 * (\n                    negative_cosine_similarity(p1, z2) +\n                    negative_cosine_similarity(p2, z1)\n                )\n                loss = loss_full / max(accumulation_steps, 1)\n\n            scaler.scale(loss).backward()\n            micro_step += 1\n\n            if micro_step == accumulation_steps:\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad(set_to_none=True)\n                micro_step = 0\n\n            total_loss += loss_full.item()\n\n        if micro_step > 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n\n        avg_loss = total_loss / len(dataloader)\n        scheduler.step()\n        print(f\"  -> Avg SSL loss: {avg_loss:.4f}\")\n\n    # Save final backbone & heads for fine-tuning\n    final_path = os.path.join(checkpoint_dir, \"simsiam_cbam_pretrained_final.pth\")\n    target_model = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n    torch.save(\n        {\n            \"backbone\": target_model.backbone.state_dict(),\n            \"projector\": target_model.projector.state_dict(),\n            \"predictor\": target_model.predictor.state_dict(),\n        },\n        final_path,\n    )\n    print(f\"\\n[Done] Pretraining ({'IN1K' if use_imagenet_init else 'Scratch'}) saved to:\\n  {final_path}\\n\")\n    return final_path\n\n\n# ============================================================\n# 7. Main: run Scratch & IN1K pretrain\n# ============================================================\n\nif __name__ == \"__main__\":\n    scratch_ckpt = pretrain_variant(\n        root_path=PRETRAIN_ROOT,\n        checkpoint_dir=\"/kaggle/working/simsiam_cbam_scratch\",\n        use_imagenet_init=False,\n        epochs=150,         \n        batch_size=64,\n        num_workers=2,\n        accumulation_steps=1,\n        fix_backbone_bn=True,\n        seed=42,\n    )\n\n    # 2) IN1K-initialized SimSiam+CBAM (for fairness comparison)\n    in1k_ckpt = pretrain_variant(\n        root_path=PRETRAIN_ROOT,\n        checkpoint_dir=\"/kaggle/working/simsiam_cbam_in1k\",\n        use_imagenet_init=True,\n        epochs=150,       \n        batch_size=64,\n        num_workers=2,\n        accumulation_steps=1,\n        fix_backbone_bn=True,\n        seed=42,\n    )\n\n    print(\"Scratch checkpoint:\", scratch_ckpt)\n    print(\"IN1K-init checkpoint:\", in1k_ckpt)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile cbam_resnet.py\nimport torch\nimport torch.nn as nn\nfrom typing import Optional, Callable\nfrom torchvision.models.resnet import ResNet, Bottleneck\n\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes: int, ratio: int = 16):\n        super().__init__()\n        hidden = max(in_planes // ratio, 1)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size: int = 7):\n        super().__init__()\n        padding = kernel_size // 2\n        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x_cat = torch.cat([avg_out, max_out], dim=1)\n        out = self.conv(x_cat)\n        return self.sigmoid(out)\n\n\nclass CBAMBottleneck(Bottleneck):\n    \"\"\"\n    ResNet Bottleneck with CBAM after the third BN and before the residual add.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        planes_out = self.conv3.out_channels\n        self.ca = ChannelAttention(planes_out)\n        self.sa = SpatialAttention()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        identity = x\n\n        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n        out = self.conv3(out); out = self.bn3(out)\n\n        out = self.ca(out) * out\n        out = self.sa(out) * out\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n        return out\n\n\ndef cbam_resnet50(*, norm_layer: Optional[Callable] = None, **kwargs) -> ResNet:\n    \"\"\"\n    Return a ResNet-50 that uses CBAMBottleneck blocks.\n    kwargs forwarded to torchvision.models.resnet.ResNet (e.g., num_classes).\n    \"\"\"\n    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], norm_layer=norm_layer, **kwargs)\n    # Zero-init last BN in each residual branch (improves training stability).\n    for m in model.modules():\n        if isinstance(m, Bottleneck):\n            nn.init.constant_(m.bn3.weight, 0)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T14:32:49.629413Z","iopub.execute_input":"2025-11-18T14:32:49.629778Z","iopub.status.idle":"2025-11-18T14:32:49.636450Z","shell.execute_reply.started":"2025-11-18T14:32:49.629754Z","shell.execute_reply":"2025-11-18T14:32:49.635855Z"}},"outputs":[{"name":"stdout","text":"Writing cbam_resnet.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nfrom typing import Iterable, List, Tuple\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nfrom torchvision import transforms\nfrom torchvision.transforms import RandAugment\nfrom torchvision.datasets import ImageFolder\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom cbam_resnet import cbam_resnet50\n\n# ============================================================\n# 0. Paths & global config\n# ============================================================\n\nDATA_ROOT   = \"/kaggle/input/minida/mini_output1\"\nSCRATCH_CKPT = \"/kaggle/working/simsiam_cbam_scratch/simsiam_cbam_pretrained_final.pth\"\nIN1K_CKPT    = \"/kaggle/working/simsiam_cbam_in1k/simsiam_cbam_pretrained_final.pth\"\n\nIMG_SIZE    = 256\nNUM_EPOCHS  = 50\nPATIENCE_CE = 10\nPATIENCE_HY = 8\nBATCH_SIZE  = 24\nNUM_WORKERS = 2\n\nBASE_LR_CE_BACKBONE = 3e-5\nBASE_LR_CE_HEAD     = 1e-4\nBASE_LR_HY          = 1e-4\n\nMIXUP_ALPHA_CE = 0.3\nMIXUP_ALPHA_HY = 0.3\nSUPCON_WEIGHT  = 0.5    # lambda for hybrid loss\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ============================================================\n# 1. Reproducibility & model stats\n# ============================================================\n\ndef seed_all(seed: int = 42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef count_parameters(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef compute_model_size_mb(model: nn.Module) -> float:\n    total_bytes = 0\n    for p in model.parameters():\n        total_bytes += p.numel() * p.element_size()\n    return total_bytes / (1024 ** 2)\n\n\nseed_all(42)\n\n\n# ============================================================\n# 2. Common helpers (mixup, weight decay groups)\n# ============================================================\n\ndef mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.3, device=None):\n    if alpha and alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.0\n    idx = torch.randperm(x.size(0), device=device)\n    mixed_x = lam * x + (1.0 - lam) * x[idx]\n    y_a, y_b = y, y[idx]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam: float):\n    return lam * criterion(pred, y_a) + (1.0 - lam) * criterion(pred, y_b)\n\n\ndef exclude_from_wd(named_params: Iterable, wd: float = 1e-4):\n    wd_params, no_wd_params = [], []\n    for n, p in named_params:\n        if not p.requires_grad:\n            continue\n        if p.ndim == 1 or n.endswith(\".bias\") or \"bn\" in n.lower():\n            no_wd_params.append(p)\n        else:\n            wd_params.append(p)\n    return [\n        {\"params\": wd_params,   \"weight_decay\": wd},\n        {\"params\": no_wd_params,\"weight_decay\": 0.0},\n    ]\n\n\n# ============================================================\n# 3. SupCon Loss (same as hybrid script)\n# ============================================================\n\nclass SupConLoss(nn.Module):\n    def __init__(self, temperature=0.07):\n        super().__init__()\n        self.temperature = temperature\n        self.eps = 1e-8\n\n    def forward(self, features, labels):\n        device = features.device\n        batch_size = features.size(0)\n        labels = labels.contiguous().view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n\n        anchor_dot_contrast = torch.div(\n            torch.matmul(features, features.T), self.temperature\n        )\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n\n        exp_logits = torch.exp(logits) * (1 - torch.eye(batch_size, device=device))\n        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + self.eps)\n\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + self.eps)\n        loss = -mean_log_prob_pos.mean()\n        return loss\n\n\n# ============================================================\n# 4. Dataloaders (CE-style & Hybrid-style, matching your old code)\n# ============================================================\n\ndef get_loaders_ce(\n    data_root: str,\n    batch_size: int = 32,\n    num_workers: int = 2,\n    img_size: int = 256,\n):\n    pin = torch.cuda.is_available()\n\n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(img_size),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(),\n        transforms.ToTensor(),\n        transforms.RandomErasing(\n            p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3), value='random'\n        ),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    resize_size = int(round(img_size * 1.14))\n    eval_tf = transforms.Compose([\n        transforms.Resize(resize_size),\n        transforms.CenterCrop(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    train_ds = ImageFolder(os.path.join(data_root, \"train\"), transform=train_tf)\n    val_ds   = ImageFolder(os.path.join(data_root, \"val\"),   transform=eval_tf)\n    test_ds  = ImageFolder(os.path.join(data_root, \"test\"),  transform=eval_tf)\n\n    class_names = train_ds.classes\n\n    targets_np = np.array(train_ds.targets, dtype=np.int64)\n    classes = np.unique(targets_np)\n    class_counts = np.array([(targets_np == c).sum() for c in classes], dtype=np.float64)\n    class_counts[class_counts == 0] = 1.0\n    weights_per_class = 1.0 / class_counts\n    sample_weights = weights_per_class[targets_np]\n    sampler = WeightedRandomSampler(\n        torch.as_tensor(sample_weights, dtype=torch.double),\n        num_samples=len(sample_weights),\n        replacement=True,\n    )\n\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, sampler=sampler,\n        num_workers=num_workers, pin_memory=pin, drop_last=True\n    )\n    val_loader   = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=pin\n    )\n    test_loader  = DataLoader(\n        test_ds, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=pin\n    )\n\n    return train_loader, val_loader, test_loader, test_ds, class_names\n\n\ndef get_loaders_hybrid(\n    data_root: str,\n    batch_size: int = 32,\n    num_workers: int = 2,\n    img_size: int = 224,\n):\n    # This follows your hybrid script more closely (224 input)\n    pin = torch.cuda.is_available()\n\n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    val_tf = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225]),\n    ])\n\n    train_ds = ImageFolder(os.path.join(data_root, \"train\"), transform=train_tf)\n    val_ds   = ImageFolder(os.path.join(data_root, \"val\"),   transform=val_tf)\n    test_ds  = ImageFolder(os.path.join(data_root, \"test\"),  transform=val_tf)\n\n    class_names = train_ds.classes\n\n    class_counts = np.bincount(train_ds.targets)\n    weights = 1. / class_counts[train_ds.targets]\n    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, sampler=sampler,\n        num_workers=num_workers, pin_memory=pin\n    )\n    val_loader   = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=pin\n    )\n    test_loader  = DataLoader(\n        test_ds, batch_size=batch_size, shuffle=False,\n        num_workers=num_workers, pin_memory=pin\n    )\n\n    return train_loader, val_loader, test_loader, test_ds, class_names\n\n\n# ============================================================\n# 5. Models (CE-only & Hybrid) matching your old scripts\n# ============================================================\n\nclass FineTuneCBAM_CE(nn.Module):\n    \"\"\"\n    CE-only: backbone + 512 FC + dropout + final classifier.\n    Matches your finetune_cbam.py\n    \"\"\"\n    def __init__(self, pretrained_path: str, num_classes: int):\n        super().__init__()\n        backbone = cbam_resnet50(num_classes=1000)\n        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # up to avgpool\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes),\n        )\n        ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n        sd = ckpt.get(\"backbone\", ckpt)\n        self.backbone.load_state_dict(sd, strict=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.backbone(x).flatten(1)\n        return self.classifier(x)\n\n\nclass FineTuneCBAM_Hybrid(nn.Module):\n    \"\"\"\n    Hybrid SupCon+CE: backbone + feature_layer(2048->128) + classifier(2048->512->3).\n    Matches your hybrid script.\n    \"\"\"\n    def __init__(self, pretrained_path: str, num_classes: int):\n        super().__init__()\n        backbone = cbam_resnet50(num_classes=1000)\n        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n        ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n        sd = ckpt.get(\"backbone\", ckpt)\n        self.backbone.load_state_dict(sd, strict=False)\n\n        self.feature_layer = nn.Linear(2048, 128)  # for SupCon\n        self.classifier = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x, return_features=False):\n        feats = self.backbone(x).flatten(1)\n        features = F.normalize(self.feature_layer(feats), dim=1)\n        logits = self.classifier(feats)\n        if return_features:\n            return logits, features\n        return logits\n\n\n# ============================================================\n# 6. Schedulers (same as your code)\n# ============================================================\n\nfrom torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n\ndef get_scheduler(optimizer, total_epochs: int, warmup_epochs: int = 5):\n    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n    cosine = CosineAnnealingLR(optimizer, T_max=max(total_epochs - warmup_epochs, 1))\n    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n\n\n# ============================================================\n# 7. Training loops (CE-only)\n# ============================================================\n\ndef train_epoch_ce(\n    model: nn.Module,\n    loader: DataLoader,\n    criterion,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    scaler: torch.amp.GradScaler,\n    use_mixup: bool = True,\n    mixup_alpha: float = 0.3,\n):\n    model.train()\n    total_loss, correct = 0.0, 0\n    for imgs, labels in tqdm(loader, desc=\"Train (CE)\", leave=False):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n            if use_mixup and mixup_alpha > 0:\n                imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha, device=device)\n                logits = model(imgs)\n                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n                preds = logits.argmax(1)\n                correct += int(\n                    lam * preds.eq(y_a).sum().item()\n                    + (1.0 - lam) * preds.eq(y_b).sum().item()\n                )\n            else:\n                logits = model(imgs)\n                loss = criterion(logits, labels)\n                preds = logits.argmax(1)\n                correct += int(preds.eq(labels).sum().item())\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += float(loss.item()) * imgs.size(0)\n\n    n = len(loader.dataset)\n    return total_loss / n, correct / n\n\n\n@torch.no_grad()\ndef eval_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    criterion,\n    device: torch.device,\n    desc: str = \"Eval\",\n):\n    model.eval()\n    total_loss, correct = 0.0, 0\n    all_labels: List[int] = []\n    all_preds:  List[int] = []\n\n    for imgs, labels in tqdm(loader, desc=desc, leave=False):\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits = model(imgs)\n        loss = criterion(logits, labels)\n        total_loss += float(loss.item()) * imgs.size(0)\n        preds = logits.argmax(1)\n        correct += int(preds.eq(labels).sum().item())\n        all_labels.extend(labels.cpu().numpy().tolist())\n        all_preds.extend(preds.cpu().numpy().tolist())\n\n    n = len(loader.dataset)\n    return total_loss / n, correct / n, np.array(all_labels), np.array(all_preds)\n\n\n# ============================================================\n# 8. Training loops (Hybrid CE+SupCon)\n# ============================================================\n\ndef train_epoch_hybrid(\n    model: nn.Module,\n    loader: DataLoader,\n    ce_loss_fn,\n    supcon_loss_fn,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    mixup_alpha: float = 0.3,\n    supcon_weight: float = 0.5,\n):\n    model.train()\n    total_loss, total_ce, total_sup, correct = 0, 0, 0, 0\n\n    for imgs, labels in tqdm(loader, desc=\"Train (Hybrid)\", leave=False):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha, device=device)\n        logits, features = model(imgs, return_features=True)\n        loss_ce = mixup_criterion(ce_loss_fn, logits, y_a, y_b, lam)\n        loss_sup = supcon_loss_fn(features, labels)\n        loss = (1 - supcon_weight) * loss_ce + supcon_weight * loss_sup\n\n        preds = logits.argmax(1)\n        correct += int(\n            lam * preds.eq(y_a).sum().item()\n            + (1.0 - lam) * preds.eq(y_b).sum().item()\n        )\n\n        loss.backward()\n        optimizer.step()\n\n        bs = imgs.size(0)\n        total_loss += loss.item() * bs\n        total_ce   += loss_ce.item() * bs\n        total_sup  += loss_sup.item() * bs\n\n    n = len(loader.dataset)\n    return (\n        total_loss / n,\n        correct / n,\n        total_ce / n,\n        total_sup / n,\n    )\n\n\n# ============================================================\n# 9. Early stopping\n# ============================================================\n\nclass EarlyStopping:\n    def __init__(self, patience: int = 7, verbose: bool = True):\n        self.patience = patience\n        self.counter = 0\n        self.best_acc = None\n        self.best_state = None\n        self.best_epoch = -1\n        self.verbose = verbose\n\n    def __call__(self, val_acc: float, model: nn.Module, epoch: int):\n        if self.best_acc is None or val_acc > self.best_acc:\n            self.best_acc = val_acc\n            self.counter = 0\n            self.best_epoch = epoch\n            self.best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            if self.verbose:\n                print(\"Validation accuracy improved, saving best state.\")\n            return False\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n            return self.counter >= self.patience\n\n\n# ============================================================\n# 10. TTA (CE-style and Hybrid-style, simplified)\n# ============================================================\n\n@torch.no_grad()\ndef tta_ce(model: nn.Module, test_ds: ImageFolder, batch_size: int, device: torch.device, img_size: int):\n    resize_base = int(round(img_size * 1.14))\n    resize_up   = int(round(img_size * 1.25))\n\n    tta_transforms = [\n        transforms.Compose([\n            transforms.Resize(resize_base),\n            transforms.CenterCrop(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(resize_base),\n            transforms.CenterCrop(img_size),\n            transforms.RandomHorizontalFlip(1.0),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(resize_up),\n            transforms.CenterCrop(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(resize_base),\n            transforms.CenterCrop(img_size),\n            transforms.GaussianBlur(3),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n    ]\n\n    pin = torch.cuda.is_available()\n    model.eval()\n    probs_all = []\n\n    for t in tta_transforms:\n        test_ds.transform = t\n        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n                            num_workers=2, pin_memory=pin)\n        probs = []\n        for imgs, _ in loader:\n            imgs = imgs.to(device)\n            logits = model(imgs)\n            probs.append(F.softmax(logits, dim=1).cpu().numpy())\n        probs_all.append(np.concatenate(probs, axis=0))\n\n    probs_all = np.stack(probs_all, axis=0)\n    mean_probs = probs_all.mean(axis=0)\n    preds = mean_probs.argmax(axis=1)\n    return preds\n\n\n@torch.no_grad()\ndef tta_hybrid(model: nn.Module, test_ds: ImageFolder, batch_size: int, device: torch.device):\n    tta_transforms = [\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.RandomHorizontalFlip(1.0),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.RandomVerticalFlip(1.0),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.RandomRotation(15),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(280),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.GaussianBlur(3),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],\n                                 [0.229, 0.224, 0.225]),\n        ]),\n    ]\n    model.eval()\n    probs_all = []\n    for t in tta_transforms:\n        test_ds.transform = t\n        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n        preds_list = []\n        for imgs, _ in loader:\n            imgs = imgs.to(device)\n            outputs = model(imgs)\n            preds_list.append(F.softmax(outputs, dim=1).cpu().numpy())\n        probs_all.append(np.concatenate(preds_list, axis=0))\n    probs_all = np.array(probs_all)\n    mean_probs = probs_all.mean(axis=0)\n    final_preds = np.argmax(mean_probs, axis=1)\n    return final_preds\n\n\n# ============================================================\n# 11. Variant runners\n# ============================================================\n\ndef run_ce_variant(variant_name: str, ckpt_path: str):\n    print(\"\\n\" + \"#\" * 60)\n    print(f\"[CE] Variant: {variant_name}\")\n    print(f\"Checkpoint: {ckpt_path}\")\n    print(\"#\" * 60)\n\n    seed_all(42)\n    train_loader, val_loader, test_loader, test_ds, class_names = get_loaders_ce(\n        DATA_ROOT, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, img_size=IMG_SIZE\n    )\n    num_classes = len(class_names)\n\n    model = FineTuneCBAM_CE(pretrained_path=ckpt_path, num_classes=num_classes).to(DEVICE)\n\n    # Ensure BN uses batch stats during finetune\n    for m in model.backbone.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.train()\n            m.requires_grad_(True)\n\n    # param groups: smaller LR for backbone\n    backbone_named = [(n, p) for n, p in model.named_parameters() if \"classifier\" not in n]\n    head_named     = [(n, p) for n, p in model.named_parameters() if \"classifier\" in n]\n    groups_backbone = exclude_from_wd(backbone_named, wd=1e-4)\n    groups_head     = exclude_from_wd(head_named, wd=1e-4)\n    for g in groups_backbone:\n        g[\"lr\"] = BASE_LR_CE_BACKBONE\n    for g in groups_head:\n        g[\"lr\"] = BASE_LR_CE_HEAD\n\n    optimizer = torch.optim.AdamW(groups_backbone + groups_head, betas=(0.9, 0.999))\n    scheduler = get_scheduler(optimizer, total_epochs=NUM_EPOCHS, warmup_epochs=5)\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.02)\n    scaler = torch.amp.GradScaler('cuda', enabled=(DEVICE.type == \"cuda\"))\n\n    # head-only warmup\n    for p in model.backbone.parameters():\n        p.requires_grad = False\n\n    early = EarlyStopping(patience=PATIENCE_CE, verbose=True)\n\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n\n    for epoch in range(NUM_EPOCHS):\n        if epoch == 5:\n            for p in model.backbone.parameters():\n                p.requires_grad = True\n            print(f\"Unfroze backbone at epoch {epoch}\")\n\n        # mixup decay (as in your script)\n        decay_start = int(NUM_EPOCHS * 2 / 3)\n        if epoch < decay_start:\n            cur_alpha = MIXUP_ALPHA_CE\n        else:\n            remaining = max(NUM_EPOCHS - epoch - 1, 0)\n            span = max(NUM_EPOCHS - decay_start, 1)\n            cur_alpha = MIXUP_ALPHA_CE * (remaining / span)\n\n        print(f\"\\n[CE] Epoch {epoch+1}/{NUM_EPOCHS} | mixup_alpha={cur_alpha:.4f}\")\n        t_loss, t_acc = train_epoch_ce(\n            model, train_loader, criterion, optimizer, DEVICE,\n            scaler, use_mixup=(cur_alpha > 1e-6), mixup_alpha=cur_alpha\n        )\n        v_loss, v_acc, v_labels, v_preds = eval_epoch(\n            model, val_loader, criterion, DEVICE, desc=\"Val (CE)\"\n        )\n        scheduler.step()\n\n        print(f\"Train Loss: {t_loss:.4f}, Acc: {t_acc:.4f} | \"\n              f\"Val Loss: {v_loss:.4f}, Acc: {v_acc:.4f}\")\n\n        train_losses.append(t_loss); train_accs.append(t_acc)\n        val_losses.append(v_loss);   val_accs.append(v_acc)\n\n        if early(v_acc, model, epoch):\n            print(\"Early stopping triggered (CE).\")\n            break\n\n    # load best\n    model.load_state_dict(early.best_state)\n    model.to(DEVICE).eval()\n    best_val = early.best_acc\n\n    # final test\n    te_loss, te_acc, te_labels, te_preds = eval_epoch(\n        model, test_loader, criterion, DEVICE, desc=\"Test (CE)\"\n    )\n    print(\"\\n[CE] Test report:\")\n    print(classification_report(te_labels, te_preds, target_names=class_names))\n\n    # TTA\n    tta_preds = tta_ce(model, test_ds, BATCH_SIZE, DEVICE, IMG_SIZE)\n    tta_acc = (tta_preds == np.array(test_ds.targets)).mean()\n\n    print(f\"\\n[{variant_name}] CE Best Val Acc: {best_val*100:.2f}%\")\n    print(f\"[{variant_name}] CE Test Acc:     {te_acc*100:.2f}%\")\n    print(f\"[{variant_name}] CE TTA  Acc:     {tta_acc*100:.2f}%\")\n\n    n_params = count_parameters(model)\n    size_mb  = compute_model_size_mb(model)\n\n    return {\n        \"variant\": variant_name,\n        \"type\": \"CE\",\n        \"best_val\": best_val,\n        \"test_acc\": te_acc,\n        \"tta_acc\": tta_acc,\n        \"n_params\": n_params,\n        \"size_mb\": size_mb,\n    }\n\n\ndef run_hybrid_variant(variant_name: str, ckpt_path: str):\n    print(\"\\n\" + \"#\" * 60)\n    print(f\"[HYBRID] Variant: {variant_name}\")\n    print(f\"Checkpoint: {ckpt_path}\")\n    print(\"#\" * 60)\n\n    seed_all(42)\n    train_loader, val_loader, test_loader, test_ds, class_names = get_loaders_hybrid(\n        DATA_ROOT, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n    )\n    num_classes = len(class_names)\n\n    model = FineTuneCBAM_Hybrid(pretrained_path=ckpt_path, num_classes=num_classes).to(DEVICE)\n\n    ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n    supcon_loss_fn = SupConLoss(temperature=0.07)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR_HY, weight_decay=1e-4)\n    scheduler = get_scheduler(optimizer, total_epochs=NUM_EPOCHS, warmup_epochs=5)\n    early = EarlyStopping(patience=PATIENCE_HY, verbose=True)\n\n    # freeze backbone for first 5 epochs (as in your hybrid script)\n    for p in model.backbone.parameters():\n        p.requires_grad = False\n    for p in model.classifier.parameters():\n        p.requires_grad = True\n    for p in model.feature_layer.parameters():\n        p.requires_grad = True\n\n    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n\n    for epoch in range(NUM_EPOCHS):\n        if epoch == 5:\n            for p in model.backbone.parameters():\n                p.requires_grad = True\n            print(\"Unfroze backbone at epoch 5 (Hybrid).\")\n\n        print(f\"\\n[HYBRID] Epoch {epoch+1}/{NUM_EPOCHS}\")\n        t_loss, t_acc, t_ce, t_sup = train_epoch_hybrid(\n            model, train_loader, ce_loss_fn, supcon_loss_fn,\n            optimizer, DEVICE, mixup_alpha=MIXUP_ALPHA_HY,\n            supcon_weight=SUPCON_WEIGHT\n        )\n        v_loss, v_acc, v_labels, v_preds = eval_epoch(\n            model, val_loader, ce_loss_fn, DEVICE, desc=\"Val (Hybrid)\"\n        )\n        scheduler.step()\n\n        print(f\"Train Loss: {t_loss:.4f}, Acc: {t_acc:.4f} | \"\n              f\"Val Loss: {v_loss:.4f}, Acc: {v_acc:.4f}\")\n\n        train_losses.append(t_loss); train_accs.append(t_acc)\n        val_losses.append(v_loss);   val_accs.append(v_acc)\n\n        if early(v_acc, model, epoch):\n            print(\"Early stopping triggered (Hybrid).\")\n            break\n\n    model.load_state_dict(early.best_state)\n    model.to(DEVICE).eval()\n    best_val = early.best_acc\n\n    te_loss, te_acc, te_labels, te_preds = eval_epoch(\n        model, test_loader, ce_loss_fn, DEVICE, desc=\"Test (Hybrid)\"\n    )\n    print(\"\\n[HYBRID] Test report:\")\n    print(classification_report(te_labels, te_preds, target_names=class_names))\n\n    tta_preds = tta_hybrid(model, test_ds, BATCH_SIZE, DEVICE)\n    tta_acc = (tta_preds == np.array(test_ds.targets)).mean()\n\n    print(f\"\\n[{variant_name}] Hybrid Best Val Acc: {best_val*100:.2f}%\")\n    print(f\"[{variant_name}] Hybrid Test Acc:     {te_acc*100:.2f}%\")\n    print(f\"[{variant_name}] Hybrid TTA  Acc:     {tta_acc*100:.2f}%\")\n\n    n_params = count_parameters(model)\n    size_mb  = compute_model_size_mb(model)\n\n    return {\n        \"variant\": variant_name,\n        \"type\": \"Hybrid\",\n        \"best_val\": best_val,\n        \"test_acc\": te_acc,\n        \"tta_acc\": tta_acc,\n        \"n_params\": n_params,\n        \"size_mb\": size_mb,\n    }\n\n\n# ============================================================\n# 12. Run all four variants & summary\n# ============================================================\n\nif __name__ == \"__main__\":\n    results = []\n\n    # 1) Scratch + CE-only\n    results.append(run_ce_variant(\n        variant_name=\"scratch_ce\",\n        ckpt_path=SCRATCH_CKPT,\n    ))\n\n    # 2) Scratch + Hybrid\n    results.append(run_hybrid_variant(\n        variant_name=\"scratch_hybrid\",\n        ckpt_path=SCRATCH_CKPT,\n    ))\n\n    # 3) IN1K-init + CE-only\n    results.append(run_ce_variant(\n        variant_name=\"in1k_ce\",\n        ckpt_path=IN1K_CKPT,\n    ))\n\n    # 4) IN1K-init + Hybrid\n    results.append(run_hybrid_variant(\n        variant_name=\"in1k_hybrid\",\n        ckpt_path=IN1K_CKPT,\n    ))\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Summary over CBAM SimSiam fine-tune variants\")\n    print(\"=\" * 60)\n    print(f\"{'Variant':15} | {'Type':7} | {'Params':12} | {'Size(MB)':9} | \"\n          f\"{'Best Val':8} | {'Test':8} | {'TTA':8}\")\n    print(\"-\" * 60)\n    for r in results:\n        print(\n            f\"{r['variant']:15} | \"\n            f\"{r['type']:7} | \"\n            f\"{r['n_params']:12,d} | \"\n            f\"{r['size_mb']:9.2f} | \"\n            f\"{r['best_val']*100:8.2f}% | \"\n            f\"{r['test_acc']*100:8.2f}% | \"\n            f\"{r['tta_acc']*100:8.2f}%\"\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:17:31.040340Z","iopub.execute_input":"2025-11-18T16:17:31.040881Z","iopub.status.idle":"2025-11-18T17:31:39.706471Z","shell.execute_reply.started":"2025-11-18T16:17:31.040851Z","shell.execute_reply":"2025-11-18T17:31:39.705492Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n[CE] Variant: scratch_ce\nCheckpoint: /kaggle/working/simsiam_cbam_scratch/simsiam_cbam_pretrained_final.pth\n############################################################\n\n[CE] Epoch 1/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0529, Acc: 0.3340 | Val Loss: 1.0782, Acc: 0.6061\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 2/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0378, Acc: 0.3467 | Val Loss: 1.0071, Acc: 0.4343\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 3/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0229, Acc: 0.3784 | Val Loss: 0.9730, Acc: 0.6162\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 4/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9935, Acc: 0.4313 | Val Loss: 0.9024, Acc: 0.5657\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 5/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9712, Acc: 0.4461 | Val Loss: 0.8622, Acc: 0.7172\nValidation accuracy improved, saving best state.\nUnfroze backbone at epoch 5\n\n[CE] Epoch 6/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9357, Acc: 0.5264 | Val Loss: 0.7599, Acc: 0.7576\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 7/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9364, Acc: 0.4672 | Val Loss: 0.7097, Acc: 0.7778\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 8/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8999, Acc: 0.4968 | Val Loss: 0.6762, Acc: 0.7071\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 9/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8934, Acc: 0.5011 | Val Loss: 0.6301, Acc: 0.7576\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 10/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8753, Acc: 0.5264 | Val Loss: 0.5882, Acc: 0.8182\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 11/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7769, Acc: 0.6237 | Val Loss: 0.4903, Acc: 0.8485\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 12/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8235, Acc: 0.5835 | Val Loss: 0.4893, Acc: 0.8283\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 13/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8043, Acc: 0.5920 | Val Loss: 0.4713, Acc: 0.8687\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 14/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8199, Acc: 0.5899 | Val Loss: 0.4746, Acc: 0.8889\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 15/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7902, Acc: 0.5941 | Val Loss: 0.4962, Acc: 0.8182\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 16/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7454, Acc: 0.6279 | Val Loss: 0.4056, Acc: 0.8586\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 17/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7591, Acc: 0.6004 | Val Loss: 0.4362, Acc: 0.8586\nEarlyStopping counter: 3 / 10\n\n[CE] Epoch 18/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7840, Acc: 0.6089 | Val Loss: 0.4046, Acc: 0.8889\nEarlyStopping counter: 4 / 10\n\n[CE] Epoch 19/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7656, Acc: 0.5983 | Val Loss: 0.4019, Acc: 0.8788\nEarlyStopping counter: 5 / 10\n\n[CE] Epoch 20/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8340, Acc: 0.5645 | Val Loss: 0.4323, Acc: 0.8485\nEarlyStopping counter: 6 / 10\n\n[CE] Epoch 21/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7475, Acc: 0.6089 | Val Loss: 0.4158, Acc: 0.9091\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 22/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7814, Acc: 0.6258 | Val Loss: 0.3974, Acc: 0.8788\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 23/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7855, Acc: 0.6131 | Val Loss: 0.4382, Acc: 0.8182\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 24/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7780, Acc: 0.6004 | Val Loss: 0.3885, Acc: 0.8687\nEarlyStopping counter: 3 / 10\n\n[CE] Epoch 25/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6749, Acc: 0.6850 | Val Loss: 0.3745, Acc: 0.8889\nEarlyStopping counter: 4 / 10\n\n[CE] Epoch 26/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7454, Acc: 0.6237 | Val Loss: 0.3934, Acc: 0.8485\nEarlyStopping counter: 5 / 10\n\n[CE] Epoch 27/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7450, Acc: 0.6321 | Val Loss: 0.3777, Acc: 0.8384\nEarlyStopping counter: 6 / 10\n\n[CE] Epoch 28/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7658, Acc: 0.6004 | Val Loss: 0.3783, Acc: 0.8384\nEarlyStopping counter: 7 / 10\n\n[CE] Epoch 29/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6941, Acc: 0.6702 | Val Loss: 0.3753, Acc: 0.8384\nEarlyStopping counter: 8 / 10\n\n[CE] Epoch 30/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7441, Acc: 0.6364 | Val Loss: 0.3543, Acc: 0.9091\nEarlyStopping counter: 9 / 10\n\n[CE] Epoch 31/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7383, Acc: 0.6321 | Val Loss: 0.3782, Acc: 0.8485\nEarlyStopping counter: 10 / 10\nEarly stopping triggered (CE).\n","output_type":"stream"},{"name":"stderr","text":"                                                        ","output_type":"stream"},{"name":"stdout","text":"\n[CE] Test report:\n              precision    recall  f1-score   support\n\n  Alternaria       1.00      0.81      0.90        37\nHealthy Leaf       0.82      1.00      0.90        31\n  straw_mite       1.00      1.00      1.00        31\n\n    accuracy                           0.93        99\n   macro avg       0.94      0.94      0.93        99\nweighted avg       0.94      0.93      0.93        99\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"\n[scratch_ce] CE Best Val Acc: 90.91%\n[scratch_ce] CE Test Acc:     92.93%\n[scratch_ce] CE TTA  Acc:     92.93%\n\n############################################################\n[HYBRID] Variant: scratch_hybrid\nCheckpoint: /kaggle/working/simsiam_cbam_scratch/simsiam_cbam_pretrained_final.pth\n############################################################\n\n[HYBRID] Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0654, Acc: 0.3890 | Val Loss: 1.0735, Acc: 0.5455\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0239, Acc: 0.5137 | Val Loss: 0.9903, Acc: 0.5253\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0084, Acc: 0.4355 | Val Loss: 0.9114, Acc: 0.7172\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9669, Acc: 0.5074 | Val Loss: 0.9130, Acc: 0.5253\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9461, Acc: 0.5011 | Val Loss: 0.8198, Acc: 0.7172\nEarlyStopping counter: 2 / 8\nUnfroze backbone at epoch 5 (Hybrid).\n\n[HYBRID] Epoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9627, Acc: 0.5180 | Val Loss: 0.7536, Acc: 0.7475\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9644, Acc: 0.5032 | Val Loss: 0.7065, Acc: 0.7980\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9801, Acc: 0.5032 | Val Loss: 0.7052, Acc: 0.7475\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9317, Acc: 0.5856 | Val Loss: 0.5972, Acc: 0.7879\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9087, Acc: 0.6364 | Val Loss: 0.5213, Acc: 0.8990\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8880, Acc: 0.6512 | Val Loss: 0.4768, Acc: 0.8687\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8719, Acc: 0.6195 | Val Loss: 0.4695, Acc: 0.8586\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9247, Acc: 0.5772 | Val Loss: 0.4606, Acc: 0.9091\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9003, Acc: 0.6068 | Val Loss: 0.4617, Acc: 0.9192\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9174, Acc: 0.5920 | Val Loss: 0.4644, Acc: 0.8687\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8661, Acc: 0.6279 | Val Loss: 0.5092, Acc: 0.8182\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8688, Acc: 0.6448 | Val Loss: 0.4159, Acc: 0.8788\nEarlyStopping counter: 3 / 8\n\n[HYBRID] Epoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8845, Acc: 0.6342 | Val Loss: 0.4601, Acc: 0.8384\nEarlyStopping counter: 4 / 8\n\n[HYBRID] Epoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8961, Acc: 0.6321 | Val Loss: 0.4336, Acc: 0.8485\nEarlyStopping counter: 5 / 8\n\n[HYBRID] Epoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8873, Acc: 0.6173 | Val Loss: 0.4218, Acc: 0.9091\nEarlyStopping counter: 6 / 8\n\n[HYBRID] Epoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8328, Acc: 0.6660 | Val Loss: 0.4652, Acc: 0.8283\nEarlyStopping counter: 7 / 8\n\n[HYBRID] Epoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8714, Acc: 0.6575 | Val Loss: 0.4034, Acc: 0.8889\nEarlyStopping counter: 8 / 8\nEarly stopping triggered (Hybrid).\n","output_type":"stream"},{"name":"stderr","text":"                                                            ","output_type":"stream"},{"name":"stdout","text":"\n[HYBRID] Test report:\n              precision    recall  f1-score   support\n\n  Alternaria       0.94      0.92      0.93        37\nHealthy Leaf       0.91      0.97      0.94        31\n  straw_mite       1.00      0.97      0.98        31\n\n    accuracy                           0.95        99\n   macro avg       0.95      0.95      0.95        99\nweighted avg       0.95      0.95      0.95        99\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"\n[scratch_hybrid] Hybrid Best Val Acc: 91.92%\n[scratch_hybrid] Hybrid Test Acc:     94.95%\n[scratch_hybrid] Hybrid TTA  Acc:     95.96%\n\n############################################################\n[CE] Variant: in1k_ce\nCheckpoint: /kaggle/working/simsiam_cbam_in1k/simsiam_cbam_pretrained_final.pth\n############################################################\n\n[CE] Epoch 1/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0386, Acc: 0.4249 | Val Loss: 0.9950, Acc: 0.4848\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 2/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9968, Acc: 0.4820 | Val Loss: 0.8984, Acc: 0.7475\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 3/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9696, Acc: 0.4461 | Val Loss: 0.8014, Acc: 0.8081\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 4/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8937, Acc: 0.5751 | Val Loss: 0.7025, Acc: 0.8283\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 5/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8222, Acc: 0.6406 | Val Loss: 0.6047, Acc: 0.8485\nValidation accuracy improved, saving best state.\nUnfroze backbone at epoch 5\n\n[CE] Epoch 6/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7597, Acc: 0.6490 | Val Loss: 0.3628, Acc: 0.9091\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 7/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6916, Acc: 0.6575 | Val Loss: 0.2476, Acc: 0.9596\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 8/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6312, Acc: 0.6998 | Val Loss: 0.2129, Acc: 0.9596\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 9/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4901, Acc: 0.7674 | Val Loss: 0.1518, Acc: 0.9697\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 10/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5288, Acc: 0.7611 | Val Loss: 0.1331, Acc: 0.9798\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 11/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3556, Acc: 0.8330 | Val Loss: 0.1319, Acc: 0.9596\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 12/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4957, Acc: 0.7780 | Val Loss: 0.1386, Acc: 0.9798\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 13/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4727, Acc: 0.7801 | Val Loss: 0.1436, Acc: 0.9697\nEarlyStopping counter: 3 / 10\n\n[CE] Epoch 14/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5226, Acc: 0.7611 | Val Loss: 0.1462, Acc: 0.9798\nEarlyStopping counter: 4 / 10\n\n[CE] Epoch 15/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4984, Acc: 0.7780 | Val Loss: 0.1271, Acc: 0.9899\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 16/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4336, Acc: 0.8140 | Val Loss: 0.1462, Acc: 0.9596\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 17/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3928, Acc: 0.8414 | Val Loss: 0.1123, Acc: 0.9899\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 18/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4746, Acc: 0.7970 | Val Loss: 0.1301, Acc: 0.9798\nEarlyStopping counter: 3 / 10\n\n[CE] Epoch 19/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4434, Acc: 0.8013 | Val Loss: 0.1154, Acc: 0.9899\nEarlyStopping counter: 4 / 10\n\n[CE] Epoch 20/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5205, Acc: 0.7653 | Val Loss: 0.1363, Acc: 0.9899\nEarlyStopping counter: 5 / 10\n\n[CE] Epoch 21/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4174, Acc: 0.8097 | Val Loss: 0.1219, Acc: 0.9899\nEarlyStopping counter: 6 / 10\n\n[CE] Epoch 22/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3923, Acc: 0.8224 | Val Loss: 0.1830, Acc: 0.9596\nEarlyStopping counter: 7 / 10\n\n[CE] Epoch 23/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4356, Acc: 0.8161 | Val Loss: 0.1173, Acc: 1.0000\nValidation accuracy improved, saving best state.\n\n[CE] Epoch 24/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4400, Acc: 0.8182 | Val Loss: 0.1358, Acc: 0.9899\nEarlyStopping counter: 1 / 10\n\n[CE] Epoch 25/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3038, Acc: 0.8732 | Val Loss: 0.1242, Acc: 0.9899\nEarlyStopping counter: 2 / 10\n\n[CE] Epoch 26/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4600, Acc: 0.7992 | Val Loss: 0.1235, Acc: 0.9899\nEarlyStopping counter: 3 / 10\n\n[CE] Epoch 27/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4117, Acc: 0.8288 | Val Loss: 0.1135, Acc: 0.9899\nEarlyStopping counter: 4 / 10\n\n[CE] Epoch 28/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3819, Acc: 0.8203 | Val Loss: 0.1095, Acc: 1.0000\nEarlyStopping counter: 5 / 10\n\n[CE] Epoch 29/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3294, Acc: 0.8478 | Val Loss: 0.1031, Acc: 1.0000\nEarlyStopping counter: 6 / 10\n\n[CE] Epoch 30/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4411, Acc: 0.8034 | Val Loss: 0.1087, Acc: 1.0000\nEarlyStopping counter: 7 / 10\n\n[CE] Epoch 31/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3901, Acc: 0.8161 | Val Loss: 0.1161, Acc: 1.0000\nEarlyStopping counter: 8 / 10\n\n[CE] Epoch 32/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4402, Acc: 0.8055 | Val Loss: 0.1153, Acc: 1.0000\nEarlyStopping counter: 9 / 10\n\n[CE] Epoch 33/50 | mixup_alpha=0.3000\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4144, Acc: 0.8203 | Val Loss: 0.1172, Acc: 0.9899\nEarlyStopping counter: 10 / 10\nEarly stopping triggered (CE).\n","output_type":"stream"},{"name":"stderr","text":"                                                        ","output_type":"stream"},{"name":"stdout","text":"\n[CE] Test report:\n              precision    recall  f1-score   support\n\n  Alternaria       1.00      0.97      0.99        37\nHealthy Leaf       0.97      1.00      0.98        31\n  straw_mite       1.00      1.00      1.00        31\n\n    accuracy                           0.99        99\n   macro avg       0.99      0.99      0.99        99\nweighted avg       0.99      0.99      0.99        99\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"\n[in1k_ce] CE Best Val Acc: 100.00%\n[in1k_ce] CE Test Acc:     98.99%\n[in1k_ce] CE TTA  Acc:     98.99%\n\n############################################################\n[HYBRID] Variant: in1k_hybrid\nCheckpoint: /kaggle/working/simsiam_cbam_in1k/simsiam_cbam_pretrained_final.pth\n############################################################\n\n[HYBRID] Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9716, Acc: 0.4186 | Val Loss: 1.0337, Acc: 0.6465\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9244, Acc: 0.5814 | Val Loss: 0.9286, Acc: 0.6465\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 3/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8030, Acc: 0.5708 | Val Loss: 0.8046, Acc: 0.8384\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 4/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7585, Acc: 0.6469 | Val Loss: 0.7420, Acc: 0.7374\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 5/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6655, Acc: 0.6258 | Val Loss: 0.6089, Acc: 0.8687\nValidation accuracy improved, saving best state.\nUnfroze backbone at epoch 5 (Hybrid).\n\n[HYBRID] Epoch 6/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7672, Acc: 0.6575 | Val Loss: 0.4761, Acc: 0.9192\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 7/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6916, Acc: 0.6998 | Val Loss: 0.3569, Acc: 0.9495\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 8/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7109, Acc: 0.7273 | Val Loss: 0.4039, Acc: 0.8586\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 9/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6399, Acc: 0.7653 | Val Loss: 0.3025, Acc: 0.9495\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 10/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6661, Acc: 0.7970 | Val Loss: 0.2571, Acc: 0.9495\nEarlyStopping counter: 3 / 8\n\n[HYBRID] Epoch 11/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6518, Acc: 0.8013 | Val Loss: 0.2392, Acc: 0.9798\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 12/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5429, Acc: 0.8140 | Val Loss: 0.2760, Acc: 0.9697\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 13/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6715, Acc: 0.7674 | Val Loss: 0.2457, Acc: 0.9899\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 14/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6420, Acc: 0.8013 | Val Loss: 0.2273, Acc: 0.9798\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 15/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6122, Acc: 0.8055 | Val Loss: 0.2200, Acc: 0.9798\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 16/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5808, Acc: 0.8414 | Val Loss: 0.2265, Acc: 0.9899\nEarlyStopping counter: 3 / 8\n\n[HYBRID] Epoch 17/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5999, Acc: 0.8245 | Val Loss: 0.2432, Acc: 0.9899\nEarlyStopping counter: 4 / 8\n\n[HYBRID] Epoch 18/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6432, Acc: 0.8140 | Val Loss: 0.2467, Acc: 0.9798\nEarlyStopping counter: 5 / 8\n\n[HYBRID] Epoch 19/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5920, Acc: 0.8161 | Val Loss: 0.2331, Acc: 0.9899\nEarlyStopping counter: 6 / 8\n\n[HYBRID] Epoch 20/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6033, Acc: 0.8161 | Val Loss: 0.2049, Acc: 1.0000\nValidation accuracy improved, saving best state.\n\n[HYBRID] Epoch 21/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4985, Acc: 0.8605 | Val Loss: 0.2057, Acc: 0.9899\nEarlyStopping counter: 1 / 8\n\n[HYBRID] Epoch 22/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5945, Acc: 0.8330 | Val Loss: 0.2123, Acc: 0.9899\nEarlyStopping counter: 2 / 8\n\n[HYBRID] Epoch 23/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5679, Acc: 0.8140 | Val Loss: 0.2520, Acc: 0.9798\nEarlyStopping counter: 3 / 8\n\n[HYBRID] Epoch 24/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4846, Acc: 0.8689 | Val Loss: 0.2061, Acc: 0.9899\nEarlyStopping counter: 4 / 8\n\n[HYBRID] Epoch 25/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4883, Acc: 0.8414 | Val Loss: 0.2509, Acc: 0.9697\nEarlyStopping counter: 5 / 8\n\n[HYBRID] Epoch 26/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5511, Acc: 0.7886 | Val Loss: 0.2243, Acc: 0.9899\nEarlyStopping counter: 6 / 8\n\n[HYBRID] Epoch 27/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4620, Acc: 0.8626 | Val Loss: 0.2069, Acc: 1.0000\nEarlyStopping counter: 7 / 8\n\n[HYBRID] Epoch 28/50\n","output_type":"stream"},{"name":"stderr","text":"                                                               \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4030, Acc: 0.8541 | Val Loss: 0.2072, Acc: 1.0000\nEarlyStopping counter: 8 / 8\nEarly stopping triggered (Hybrid).\n","output_type":"stream"},{"name":"stderr","text":"                                                            ","output_type":"stream"},{"name":"stdout","text":"\n[HYBRID] Test report:\n              precision    recall  f1-score   support\n\n  Alternaria       1.00      0.95      0.97        37\nHealthy Leaf       0.94      1.00      0.97        31\n  straw_mite       1.00      1.00      1.00        31\n\n    accuracy                           0.98        99\n   macro avg       0.98      0.98      0.98        99\nweighted avg       0.98      0.98      0.98        99\n\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"name":"stdout","text":"\n[in1k_hybrid] Hybrid Best Val Acc: 100.00%\n[in1k_hybrid] Hybrid Test Acc:     97.98%\n[in1k_hybrid] Hybrid TTA  Acc:     100.00%\n\n============================================================\nSummary over CBAM SimSiam fine-tune variants\n============================================================\nVariant         | Type    | Params       | Size(MB)  | Best Val | Test     | TTA     \n------------------------------------------------------------\nscratch_ce      | CE      |   27,075,171 |    103.28 |    90.91% |    92.93% |    92.93%\nscratch_hybrid  | Hybrid  |   27,337,443 |    104.28 |    91.92% |    94.95% |    95.96%\nin1k_ce         | CE      |   27,075,171 |    103.28 |   100.00% |    98.99% |    98.99%\nin1k_hybrid     | Hybrid  |   27,337,443 |    104.28 |   100.00% |    97.98% |   100.00%\n","output_type":"stream"}],"execution_count":6}]}