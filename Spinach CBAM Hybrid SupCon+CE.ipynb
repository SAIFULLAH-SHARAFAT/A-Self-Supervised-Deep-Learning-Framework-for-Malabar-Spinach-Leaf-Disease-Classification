{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hybrid SupCon+CE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T23:14:23.080483Z",
     "iopub.status.busy": "2025-07-06T23:14:23.079690Z",
     "iopub.status.idle": "2025-07-06T23:43:45.456002Z",
     "shell.execute_reply": "2025-07-06T23:43:45.455110Z",
     "shell.execute_reply.started": "2025-07-06T23:14:23.080455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.2487, Acc 0.3898 | Val Loss 1.1088, Acc 0.3131\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 2.2073, Acc 0.4732 | Val Loss 1.1009, Acc 0.3131\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 2.1854, Acc 0.4149 | Val Loss 1.0241, Acc 0.4646\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 2.1509, Acc 0.3909 | Val Loss 0.9630, Acc 0.5556\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 2.1198, Acc 0.4880 | Val Loss 0.9547, Acc 0.5051\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 2.2027, Acc 0.4493 | Val Loss 0.9060, Acc 0.5859\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 2.1879, Acc 0.4928 | Val Loss 0.8403, Acc 0.6465\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 2.1510, Acc 0.5059 | Val Loss 0.7801, Acc 0.7677\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 2.1323, Acc 0.5324 | Val Loss 0.7574, Acc 0.7273\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 2.1112, Acc 0.5755 | Val Loss 0.7450, Acc 0.6970\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 2.1601, Acc 0.5789 | Val Loss 0.6835, Acc 0.7273\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 2.1311, Acc 0.5866 | Val Loss 0.5978, Acc 0.8384\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 2.1179, Acc 0.6101 | Val Loss 0.5575, Acc 0.8081\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 2.0786, Acc 0.6883 | Val Loss 0.4622, Acc 0.8889\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 2.0899, Acc 0.6630 | Val Loss 0.4747, Acc 0.8283\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 2.0270, Acc 0.6823 | Val Loss 0.4400, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 2.0749, Acc 0.6422 | Val Loss 0.4603, Acc 0.8283\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 2.0704, Acc 0.6573 | Val Loss 0.4909, Acc 0.8586\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 2.0773, Acc 0.6651 | Val Loss 0.4470, Acc 0.8788\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 2.0559, Acc 0.6503 | Val Loss 0.4231, Acc 0.9192\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 2.0119, Acc 0.7370 | Val Loss 0.4326, Acc 0.8990\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 2.0867, Acc 0.6377 | Val Loss 0.4136, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 2.0443, Acc 0.6928 | Val Loss 0.4408, Acc 0.9293\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 2.0399, Acc 0.6767 | Val Loss 0.4642, Acc 0.8485\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 1.9983, Acc 0.6738 | Val Loss 0.4130, Acc 0.9192\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 2.0241, Acc 0.6965 | Val Loss 0.4057, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 1.9852, Acc 0.6989 | Val Loss 0.4227, Acc 0.9091\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 1.9733, Acc 0.7095 | Val Loss 0.3969, Acc 0.9091\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 2.0262, Acc 0.6961 | Val Loss 0.3916, Acc 0.9192\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss 1.9875, Acc 0.7056 | Val Loss 0.3999, Acc 0.8889\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss 1.9295, Acc 0.7617 | Val Loss 0.3832, Acc 0.9394\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss 1.9527, Acc 0.7137 | Val Loss 0.3812, Acc 0.9293\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss 1.9037, Acc 0.7407 | Val Loss 0.3880, Acc 0.9293\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss 1.9424, Acc 0.7320 | Val Loss 0.3910, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss 1.9574, Acc 0.7187 | Val Loss 0.3765, Acc 0.9394\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss 1.8872, Acc 0.7782 | Val Loss 0.3792, Acc 0.9394\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss 1.8645, Acc 0.7565 | Val Loss 0.3805, Acc 0.9192\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss 1.9698, Acc 0.7004 | Val Loss 0.3763, Acc 0.9293\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss 1.9289, Acc 0.7224 | Val Loss 0.3786, Acc 0.9293\n",
      "EarlyStopping counter: 8 / 8\n",
      "Early stopping at epoch 39\n",
      "\n",
      "Test set results (CBAM Hybrid):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3125, Test Acc: 0.9596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.89      0.94        37\n",
      "Healthy Leaf       0.89      1.00      0.94        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.96        99\n",
      "   macro avg       0.96      0.96      0.96        99\n",
      "weighted avg       0.96      0.96      0.96        99\n",
      "\n",
      "Test ROC-AUC (macro): 0.9988\n",
      "\n",
      "Test-Time Augmentation (TTA) Evaluation (CBAM Hybrid):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       0.97      0.89      0.93        37\n",
      "Healthy Leaf       0.88      0.97      0.92        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.95        99\n",
      "   macro avg       0.95      0.95      0.95        99\n",
      "weighted avg       0.95      0.95      0.95        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "# ---------- SEED & DEVICE ----------\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- PATHS & CLASSES ----------\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- MIXUP ----------\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ---------- SUPERVISED CONTRASTIVE LOSS ----------\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.eps = 1e-8\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        batch_size = features.size(0)\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(batch_size, device=device))\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + self.eps)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + self.eps)\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "# ---------- DATALOADERS ----------\n",
    "def get_loaders(batch_size=32):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, test_ds\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.feature_layer = nn.Linear(2048, 128)  # For SupCon\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        feats = self.backbone(x).flatten(1)\n",
    "        features = F.normalize(self.feature_layer(feats), dim=1)\n",
    "        logits = self.classifier(feats)\n",
    "        if return_features:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "# ---------- LR SCHEDULER ----------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------- TRAINING & EVAL ----------\n",
    "def train_epoch(model, loader, ce_loss_fn, supcon_loss_fn, optimizer,\n",
    "                use_mixup=True, mixup_alpha=0.3, supcon_weight=0.5):\n",
    "    model.train()\n",
    "    total_loss, total_ce, total_supcon, correct = 0, 0, 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "            logits, features = model(imgs, return_features=True)\n",
    "            loss_ce = mixup_criterion(ce_loss_fn, logits, y_a, y_b, lam)\n",
    "            loss_supcon = supcon_loss_fn(features, labels)\n",
    "            loss = (1 - supcon_weight) * loss_ce + supcon_weight * loss_supcon\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            logits, features = model(imgs, return_features=True)\n",
    "            loss_ce = ce_loss_fn(logits, labels)\n",
    "            loss_supcon = supcon_loss_fn(features, labels)\n",
    "            loss = (1 - supcon_weight) * loss_ce + supcon_weight * loss_supcon\n",
    "            correct += logits.argmax(1).eq(labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_ce += loss_ce.item() * imgs.size(0)\n",
    "        total_supcon += loss_supcon.item() * imgs.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, correct / n, total_ce / n, total_supcon / n\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ---------- EARLY STOPPING ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.verbose = verbose\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            if self.verbose: print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ---------- PLOTTING ----------\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix (CBAM)')\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_per_class(y_true, y_score, n_classes, class_names, save_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(n_classes):\n",
    "        if np.sum(y_true==i) == 0: continue\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve((y_true==i).astype(int), y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"ROC error for class {class_names[i]}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Per-class ROC Curves (CBAM)')\n",
    "    plt.legend()\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_reliability(y_true, y_prob, n_bins=10):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i, name in enumerate(class_names):\n",
    "        try:\n",
    "            prob_true, prob_pred = calibration_curve((y_true==i).astype(int), y_prob[:,i], n_bins=n_bins, strategy='uniform')\n",
    "            plt.plot(prob_pred, prob_true, marker='o', label=f\"{name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Reliability curve failed for {name}: {e}\")\n",
    "    plt.plot([0,1],[0,1],'--', color='gray')\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.title(\"Reliability Diagram (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reliability_diagram_cbam.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs, train_ce_losses, train_supcon_losses):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Total')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.plot(train_ce_losses, label='Train CE')\n",
    "    plt.plot(train_supcon_losses, label='Train SupCon')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves (CBAM Hybrid)\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curves (CBAM Hybrid)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_acc_curves_cbam_hybrid.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ---------- TTA ----------\n",
    "def tta_eval(model, test_ds, batch_size, class_names):\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomVerticalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomRotation(15),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(280), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.GaussianBlur(3),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    ]\n",
    "    tta_probs = []\n",
    "    model.eval()\n",
    "    for t in tta_transforms:\n",
    "        test_ds.transform = t\n",
    "        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "        tta_probs.append(np.concatenate(preds))\n",
    "    tta_probs = np.array(tta_probs)\n",
    "    mean_probs = np.mean(tta_probs, axis=0)\n",
    "    final_preds = np.argmax(mean_probs, axis=1)\n",
    "    return mean_probs, final_preds\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3, supcon_weight=0.5):\n",
    "    train_loader, val_loader, test_loader, test_ds = get_loaders(batch_size)\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # (OPTIONAL) Freeze backbone for warmup\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.feature_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "    train_ce_losses, train_supcon_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 5:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        t_loss, t_acc, t_ce, t_sup = train_epoch(\n",
    "            model, train_loader, ce_loss_fn, supcon_loss_fn, optimizer,\n",
    "            use_mixup=True, mixup_alpha=mixup_alpha, supcon_weight=supcon_weight)\n",
    "        v_loss, v_acc, _, _ = eval_epoch(model, val_loader, ce_loss_fn)\n",
    "        scheduler.step()\n",
    "        train_losses.append(t_loss)\n",
    "        train_accs.append(t_acc)\n",
    "        train_ce_losses.append(t_ce)\n",
    "        train_supcon_losses.append(t_sup)\n",
    "        val_losses.append(v_loss)\n",
    "        val_accs.append(v_acc)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {t_loss:.4f}, Acc {t_acc:.4f} | Val Loss {v_loss:.4f}, Acc {v_acc:.4f}\")\n",
    "        if early_stopper(v_acc, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs, train_ce_losses, train_supcon_losses)\n",
    "    model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    print(\"\\nTest set results (CBAM Hybrid):\")\n",
    "    test_loss, test_acc, test_labels, test_probs = eval_epoch(model, test_loader, ce_loss_fn)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_labels, test_preds, class_names, save_path=\"cbam_norm_confmat_hybrid.png\")\n",
    "\n",
    "    try:\n",
    "        test_labels_onehot = np.eye(num_classes)[test_labels]\n",
    "        roc_macro = roc_auc_score(test_labels_onehot, test_probs, average='macro', multi_class='ovr')\n",
    "        print(f\"Test ROC-AUC (macro): {roc_macro:.4f}\")\n",
    "        plot_roc_per_class(test_labels, test_probs, num_classes, class_names, save_path=\"cbam_perclass_roc_hybrid.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "\n",
    "    plot_reliability(test_labels, test_probs)\n",
    "\n",
    "    # -------- TTA --------\n",
    "    print(\"\\nTest-Time Augmentation (TTA) Evaluation (CBAM Hybrid):\")\n",
    "    mean_probs, final_preds = tta_eval(model, test_ds, batch_size, class_names)\n",
    "    print(classification_report(test_ds.targets, final_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_ds.targets, final_preds, class_names, save_path=\"cbam_tta_confmat_hybrid.png\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3, supcon_weight=0.5)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
