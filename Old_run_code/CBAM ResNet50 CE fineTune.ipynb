{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBAM-ResNet fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T18:32:55.697084Z",
     "iopub.status.busy": "2025-07-12T18:32:55.695517Z",
     "iopub.status.idle": "2025-07-12T18:58:56.416937Z",
     "shell.execute_reply": "2025-07-12T18:58:56.415853Z",
     "shell.execute_reply.started": "2025-07-12T18:32:55.697039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 1.0797, Acc 0.4428 | Val Loss 1.1021, Acc 0.3131\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 1.0514, Acc 0.4315 | Val Loss 1.1091, Acc 0.3131\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 1.0452, Acc 0.4233 | Val Loss 1.0132, Acc 0.4747\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 1.0478, Acc 0.4563 | Val Loss 0.9460, Acc 0.6869\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 1.0646, Acc 0.4371 | Val Loss 0.9650, Acc 0.6162\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 1.0260, Acc 0.4961 | Val Loss 0.8190, Acc 0.6869\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.9878, Acc 0.5293 | Val Loss 0.7635, Acc 0.7273\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.9395, Acc 0.5544 | Val Loss 0.5978, Acc 0.8384\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.9318, Acc 0.5686 | Val Loss 0.5133, Acc 0.8788\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.8408, Acc 0.6399 | Val Loss 0.4725, Acc 0.8384\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.8235, Acc 0.6594 | Val Loss 0.5732, Acc 0.7778\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.8280, Acc 0.6499 | Val Loss 0.4551, Acc 0.8485\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.8032, Acc 0.6788 | Val Loss 0.4415, Acc 0.8889\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.7189, Acc 0.7159 | Val Loss 0.4325, Acc 0.8889\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 0.8371, Acc 0.6578 | Val Loss 0.4500, Acc 0.8990\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 0.7392, Acc 0.7146 | Val Loss 0.4594, Acc 0.8182\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 0.8212, Acc 0.6590 | Val Loss 0.4256, Acc 0.8990\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 0.7327, Acc 0.7387 | Val Loss 0.4640, Acc 0.8283\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 0.7776, Acc 0.6827 | Val Loss 0.3873, Acc 0.9091\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 0.7947, Acc 0.7034 | Val Loss 0.3819, Acc 0.9293\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 0.6738, Acc 0.7706 | Val Loss 0.4699, Acc 0.8485\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 0.7487, Acc 0.7139 | Val Loss 0.3828, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 0.7398, Acc 0.7155 | Val Loss 0.4218, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 0.7984, Acc 0.6800 | Val Loss 0.3936, Acc 0.9091\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 0.7769, Acc 0.6762 | Val Loss 0.4109, Acc 0.9091\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 0.7512, Acc 0.7150 | Val Loss 0.4405, Acc 0.8788\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 0.7297, Acc 0.7033 | Val Loss 0.3844, Acc 0.8788\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 0.6994, Acc 0.7555 | Val Loss 0.3401, Acc 0.9495\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 0.7122, Acc 0.7373 | Val Loss 0.3407, Acc 0.9394\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss 0.7144, Acc 0.7273 | Val Loss 0.3748, Acc 0.9091\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss 0.6796, Acc 0.7512 | Val Loss 0.3562, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss 0.7132, Acc 0.7086 | Val Loss 0.3366, Acc 0.9293\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss 0.6956, Acc 0.7399 | Val Loss 0.3485, Acc 0.9192\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss 0.6700, Acc 0.7500 | Val Loss 0.3634, Acc 0.9091\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss 0.7068, Acc 0.7411 | Val Loss 0.3515, Acc 0.9192\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss 0.6281, Acc 0.7872 | Val Loss 0.3346, Acc 0.9293\n",
      "EarlyStopping counter: 8 / 8\n",
      "Early stopping at epoch 36\n",
      "Model saved as best_finetuned_cbam.pth\n",
      "\n",
      "Test set results (CBAM):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2851, Test Acc: 0.9697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.92      0.96        37\n",
      "Healthy Leaf       0.91      1.00      0.95        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.97      0.97      0.97        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from cbam_resnet import cbam_resnet50  # Ensure this file is in your working directory\n",
    "\n",
    "# ---------- SEED & DEVICE ----------\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- PATHS & CLASSES ----------\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- DATALOADERS ----------\n",
    "def get_loaders(batch_size=32):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, test_ds\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ---------- LR SCHEDULER ----------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------- MIXUP ----------\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ---------- TRAINING & EVAL ----------\n",
    "def train_epoch(model, loader, criterion, optimizer, use_mixup=True, mixup_alpha=0.3):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "            outputs = model(imgs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ---------- EARLY STOPPING ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.verbose = verbose\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            if self.verbose: print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3):\n",
    "    train_loader, val_loader, test_loader, test_ds = get_loaders(batch_size)\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Freeze backbone for warmup\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 5:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        t_loss, t_acc = train_epoch(model, train_loader, criterion, optimizer, use_mixup=True, mixup_alpha=mixup_alpha)\n",
    "        v_loss, v_acc, _, _ = eval_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        train_losses.append(t_loss)\n",
    "        train_accs.append(t_acc)\n",
    "        val_losses.append(v_loss)\n",
    "        val_accs.append(v_acc)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {t_loss:.4f}, Acc {t_acc:.4f} | Val Loss {v_loss:.4f}, Acc {v_acc:.4f}\")\n",
    "        if early_stopper(v_acc, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # --- Always restore best state and eval before testing ---\n",
    "    model.load_state_dict(early_stopper.best_state)\n",
    "    model.eval()\n",
    "    # --- Save model at its best state ---\n",
    "    torch.save(model.state_dict(), \"best_finetuned_cbam.pth\")\n",
    "    print(\"Model saved as best_finetuned_cbam.pth\")\n",
    "\n",
    "    # --- Test evaluation ---\n",
    "    print(\"\\nTest set results (CBAM):\")\n",
    "    test_loss, test_acc, test_labels, test_probs = eval_epoch(model, test_loader, criterion)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "    # (Add your plotting here as needed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
