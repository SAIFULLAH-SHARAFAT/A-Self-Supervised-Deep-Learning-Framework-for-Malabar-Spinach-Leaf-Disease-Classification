{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T04:02:07.887610Z",
     "iopub.status.busy": "2025-12-04T04:02:07.886880Z",
     "iopub.status.idle": "2025-12-04T04:34:59.313148Z",
     "shell.execute_reply": "2025-12-04T04:34:59.312375Z",
     "shell.execute_reply.started": "2025-12-04T04:02:07.887578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 04:02:07,921 - INFO - Classes (3): ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
      "2025-12-04 04:02:07,922 - INFO - timm=1.0.19 torch=2.6.0+cu124 device=cuda\n",
      "2025-12-04 04:02:08,368 - INFO - Dataset sizes | train=473 val=99 test=99\n",
      "2025-12-04 04:02:08,369 - INFO - Eval protocol: STRICT resize to (IMG_SIZE, IMG_SIZE) with NO CenterCrop (applied to ALL models).\n",
      "2025-12-04 04:02:08,372 - INFO - Experiments:\n",
      "2025-12-04 04:02:08,372 - INFO -  - BASE_scratch | swinv2_base_window12to16_192to256 | pretrained=False\n",
      "2025-12-04 04:02:08,373 - INFO -  - BASE_in22k_ft_in1k | swinv2_base_window12to16_192to256.ms_in22k_ft_in1k | pretrained=True\n",
      "2025-12-04 04:02:08,374 - INFO -  - SMALL_in1k_baseline | swinv2_small_window16_256.ms_in1k | pretrained=True\n",
      "2025-12-04 04:02:08,376 - INFO - ====================================================================================================\n",
      "2025-12-04 04:02:08,376 - INFO - Running: BASE_scratch\n",
      "2025-12-04 04:02:09,842 - INFO - Model: swinv2_base_window12to16_192to256 | pretrained=False | init=random initialization\n",
      "2025-12-04 04:02:09,843 - INFO - pretrained_cfg.tag=None dataset=None\n",
      "2025-12-04 04:02:09,844 - INFO - Params: 86.90M | est fp32 weights ~ 331.5 MB\n",
      "2025-12-04 04:02:49,344 - INFO - [VAL] ep=1 loss=1.1076 val_f1=0.6218 val_acc=0.6364\n",
      "2025-12-04 04:03:29,195 - INFO - [VAL] ep=2 loss=0.9815 val_f1=0.5468 val_acc=0.6061\n",
      "2025-12-04 04:04:08,657 - INFO - [VAL] ep=3 loss=0.9320 val_f1=0.5294 val_acc=0.5556\n",
      "2025-12-04 04:04:47,323 - INFO - [VAL] ep=4 loss=0.9242 val_f1=0.6777 val_acc=0.6768\n",
      "2025-12-04 04:05:27,765 - INFO - [VAL] ep=5 loss=0.8650 val_f1=0.7018 val_acc=0.7273\n",
      "2025-12-04 04:06:08,241 - INFO - [VAL] ep=6 loss=0.7544 val_f1=0.7288 val_acc=0.7374\n",
      "2025-12-04 04:06:48,466 - INFO - [VAL] ep=7 loss=0.7667 val_f1=0.6978 val_acc=0.7273\n",
      "2025-12-04 04:07:27,846 - INFO - [VAL] ep=8 loss=0.6914 val_f1=0.8029 val_acc=0.8081\n",
      "2025-12-04 04:08:07,939 - INFO - [VAL] ep=9 loss=0.6429 val_f1=0.7354 val_acc=0.7374\n",
      "2025-12-04 04:08:47,099 - INFO - [VAL] ep=10 loss=0.6389 val_f1=0.8324 val_acc=0.8283\n",
      "2025-12-04 04:09:27,385 - INFO - [VAL] ep=11 loss=0.6666 val_f1=0.7116 val_acc=0.7172\n",
      "2025-12-04 04:10:06,492 - INFO - [VAL] ep=12 loss=0.6209 val_f1=0.8783 val_acc=0.8788\n",
      "2025-12-04 04:10:46,503 - INFO - [VAL] ep=13 loss=0.5608 val_f1=0.7500 val_acc=0.7677\n",
      "2025-12-04 04:11:26,051 - INFO - [VAL] ep=14 loss=0.5696 val_f1=0.8909 val_acc=0.8889\n",
      "2025-12-04 04:12:05,393 - INFO - [VAL] ep=15 loss=0.5495 val_f1=0.8813 val_acc=0.8788\n",
      "2025-12-04 04:12:45,347 - INFO - [VAL] ep=16 loss=0.5536 val_f1=0.9099 val_acc=0.9091\n",
      "2025-12-04 04:13:25,187 - INFO - [VAL] ep=17 loss=0.5213 val_f1=0.9021 val_acc=0.8990\n",
      "2025-12-04 04:14:04,047 - INFO - [VAL] ep=18 loss=0.5167 val_f1=0.8992 val_acc=0.8990\n",
      "2025-12-04 04:14:43,515 - INFO - [VAL] ep=19 loss=0.5178 val_f1=0.8983 val_acc=0.8990\n",
      "2025-12-04 04:15:22,450 - INFO - [VAL] ep=20 loss=0.4979 val_f1=0.8681 val_acc=0.8687\n",
      "2025-12-04 04:16:02,540 - INFO - [VAL] ep=21 loss=0.4931 val_f1=0.9099 val_acc=0.9091\n",
      "2025-12-04 04:16:41,266 - INFO - [VAL] ep=22 loss=0.4954 val_f1=0.8911 val_acc=0.8889\n",
      "2025-12-04 04:17:20,717 - INFO - [VAL] ep=23 loss=0.5107 val_f1=0.9304 val_acc=0.9293\n",
      "2025-12-04 04:17:59,775 - INFO - [VAL] ep=24 loss=0.5037 val_f1=0.8416 val_acc=0.8384\n",
      "2025-12-04 04:18:38,618 - INFO - [VAL] ep=25 loss=0.4797 val_f1=0.9008 val_acc=0.8990\n",
      "2025-12-04 04:19:17,824 - INFO - [VAL] ep=26 loss=0.4564 val_f1=0.9092 val_acc=0.9091\n",
      "2025-12-04 04:19:56,616 - INFO - [VAL] ep=27 loss=0.4678 val_f1=0.9008 val_acc=0.8990\n",
      "2025-12-04 04:20:35,054 - INFO - [VAL] ep=28 loss=0.4601 val_f1=0.9502 val_acc=0.9495\n",
      "2025-12-04 04:21:15,616 - INFO - [VAL] ep=29 loss=0.4637 val_f1=0.7625 val_acc=0.7778\n",
      "2025-12-04 04:21:54,673 - INFO - [VAL] ep=30 loss=0.4581 val_f1=0.9401 val_acc=0.9394\n",
      "2025-12-04 04:22:01,856 - INFO - [TEST] acc=0.9495 bal_acc=0.9532 f1=0.9504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria     0.9706    0.8919    0.9296        37\n",
      "Healthy Leaf     0.9688    1.0000    0.9841        31\n",
      "  straw_mite     0.9091    0.9677    0.9375        31\n",
      "\n",
      "    accuracy                         0.9495        99\n",
      "   macro avg     0.9495    0.9532    0.9504        99\n",
      "weighted avg     0.9508    0.9495    0.9491        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 04:22:02,693 - INFO - ====================================================================================================\n",
      "2025-12-04 04:22:02,694 - INFO - Running: BASE_in22k_ft_in1k\n",
      "2025-12-04 04:22:04,588 - INFO - Model: swinv2_base_window12to16_192to256.ms_in22k_ft_in1k | pretrained=True | init=pretrained weights\n",
      "2025-12-04 04:22:04,589 - INFO - pretrained_cfg.tag=ms_in22k_ft_in1k dataset=None\n",
      "2025-12-04 04:22:04,589 - INFO - Params: 86.90M | est fp32 weights ~ 331.5 MB\n",
      "2025-12-04 04:22:43,638 - INFO - [VAL] ep=1 loss=1.0336 val_f1=0.9215 val_acc=0.9192\n",
      "2025-12-04 04:23:30,265 - INFO - [VAL] ep=2 loss=0.4072 val_f1=0.9703 val_acc=0.9697\n",
      "2025-12-04 04:24:15,889 - INFO - [VAL] ep=3 loss=0.3417 val_f1=0.9901 val_acc=0.9899\n",
      "2025-12-04 04:24:57,685 - INFO - [VAL] ep=4 loss=0.3280 val_f1=0.9802 val_acc=0.9798\n",
      "2025-12-04 04:25:37,206 - INFO - [VAL] ep=5 loss=0.3332 val_f1=0.9802 val_acc=0.9798\n",
      "2025-12-04 04:26:15,384 - INFO - [VAL] ep=6 loss=0.3238 val_f1=0.9705 val_acc=0.9697\n",
      "2025-12-04 04:26:55,254 - INFO - [VAL] ep=7 loss=0.3439 val_f1=0.9602 val_acc=0.9596\n",
      "2025-12-04 04:27:35,442 - INFO - [VAL] ep=8 loss=0.3100 val_f1=0.9701 val_acc=0.9697\n",
      "2025-12-04 04:28:15,987 - INFO - [VAL] ep=9 loss=0.3149 val_f1=0.9802 val_acc=0.9798\n",
      "2025-12-04 04:28:55,111 - INFO - [VAL] ep=10 loss=0.3040 val_f1=0.9901 val_acc=0.9899\n",
      "2025-12-04 04:28:55,113 - INFO - Early stopping at epoch 10\n",
      "2025-12-04 04:29:02,512 - INFO - [TEST] acc=0.9798 bal_acc=0.9820 f1=0.9803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria     1.0000    0.9459    0.9722        37\n",
      "Healthy Leaf     0.9394    1.0000    0.9688        31\n",
      "  straw_mite     1.0000    1.0000    1.0000        31\n",
      "\n",
      "    accuracy                         0.9798        99\n",
      "   macro avg     0.9798    0.9820    0.9803        99\n",
      "weighted avg     0.9810    0.9798    0.9798        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 04:29:03,407 - INFO - ====================================================================================================\n",
      "2025-12-04 04:29:03,408 - INFO - Running: SMALL_in1k_baseline\n",
      "2025-12-04 04:29:04,709 - INFO - Model: swinv2_small_window16_256.ms_in1k | pretrained=True | init=pretrained weights\n",
      "2025-12-04 04:29:04,709 - INFO - pretrained_cfg.tag=ms_in1k dataset=None\n",
      "2025-12-04 04:29:04,711 - INFO - Params: 48.96M | est fp32 weights ~ 186.8 MB\n",
      "2025-12-04 04:29:42,968 - INFO - [VAL] ep=1 loss=0.6146 val_f1=0.9608 val_acc=0.9596\n",
      "2025-12-04 04:30:21,804 - INFO - [VAL] ep=2 loss=0.3475 val_f1=0.9901 val_acc=0.9899\n",
      "2025-12-04 04:31:01,193 - INFO - [VAL] ep=3 loss=0.3243 val_f1=0.9901 val_acc=0.9899\n",
      "2025-12-04 04:31:39,422 - INFO - [VAL] ep=4 loss=0.3025 val_f1=0.9705 val_acc=0.9697\n",
      "2025-12-04 04:32:16,619 - INFO - [VAL] ep=5 loss=0.3052 val_f1=0.9802 val_acc=0.9798\n",
      "2025-12-04 04:32:54,278 - INFO - [VAL] ep=6 loss=0.3071 val_f1=0.9702 val_acc=0.9697\n",
      "2025-12-04 04:33:33,658 - INFO - [VAL] ep=7 loss=0.3010 val_f1=0.9803 val_acc=0.9798\n",
      "2025-12-04 04:34:11,784 - INFO - [VAL] ep=8 loss=0.2936 val_f1=0.9801 val_acc=0.9798\n",
      "2025-12-04 04:34:51,503 - INFO - [VAL] ep=9 loss=0.3260 val_f1=0.9801 val_acc=0.9798\n",
      "2025-12-04 04:34:51,504 - INFO - Early stopping at epoch 9\n",
      "2025-12-04 04:34:58,615 - INFO - [TEST] acc=1.0000 bal_acc=1.0000 f1=1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria     1.0000    1.0000    1.0000        37\n",
      "Healthy Leaf     1.0000    1.0000    1.0000        31\n",
      "  straw_mite     1.0000    1.0000    1.0000        31\n",
      "\n",
      "    accuracy                         1.0000        99\n",
      "   macro avg     1.0000    1.0000    1.0000        99\n",
      "weighted avg     1.0000    1.0000    1.0000        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 04:34:59,308 - INFO - Done! Results saved: ./swin_controlled_final_strict_eval/results_20251204_040207.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTS CSV: ./swin_controlled_final_strict_eval/results_20251204_040207.csv\n",
      "\n",
      "=== SUMMARY ===\n",
      "BASE_scratch         | f1=0.9504 acc=0.9495 | params=86.90M | init=random initialization | tag=None\n",
      "BASE_in22k_ft_in1k   | f1=0.9803 acc=0.9798 | params=86.90M | init=pretrained weights | tag=ms_in22k_ft_in1k\n",
      "SMALL_in1k_baseline  | f1=1.0000 acc=1.0000 | params=48.96M | init=pretrained weights | tag=ms_in1k\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, random, datetime, logging, csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score, balanced_accuracy_score, f1_score\n",
    "import timm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# ---------------------------- CONFIG ----------------------------\n",
    "class CFG:\n",
    "    DATA_PATHS = {\n",
    "        \"train\": \"/kaggle/input/minida/mini_output1/train\",\n",
    "        \"val\":   \"/kaggle/input/minida/mini_output1/val\",\n",
    "        \"test\":  \"/kaggle/input/minida/mini_output1/test\",\n",
    "    }\n",
    "\n",
    "    IMG_SIZE = 256\n",
    "    BATCH = 32\n",
    "    NUM_WORKERS = 2\n",
    "    AMP = True\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # CE-only fine-tuning (safe/simple)\n",
    "    EPOCHS = 30\n",
    "    LR = 1e-4\n",
    "    WD = 0.05\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    PATIENCE = 7\n",
    "\n",
    "    SEED = 42\n",
    "\n",
    "    OUTPUT_DIR = \"./swin_controlled_final_strict_eval\"\n",
    "    RUN_TS = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Controlled architecture: SwinV2-Base 192->256 window12->16\n",
    "    BASE_SCRATCH = \"swinv2_base_window12to16_192to256\"\n",
    "    BASE_IN22K_FT_IN1K = \"swinv2_base_window12to16_192to256.ms_in22k_ft_in1k\"\n",
    "\n",
    "    # Optional IN1K baseline (different capacity, but helpful)\n",
    "    RUN_SMALL_IN1K_BASELINE = True\n",
    "    SMALL_IN1K = \"swinv2_small_window16_256.ms_in1k\"\n",
    "\n",
    "cfg = CFG()\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------- LOGGING ----------------------------\n",
    "logger = logging.getLogger(\"study\")\n",
    "logger.setLevel(logging.INFO)\n",
    "fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "if not logger.handlers:\n",
    "    fh = logging.FileHandler(os.path.join(cfg.OUTPUT_DIR, f\"run_{cfg.RUN_TS}.log\"))\n",
    "    fh.setFormatter(fmt); logger.addHandler(fh)\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(fmt); logger.addHandler(sh)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(cfg.SEED)\n",
    "\n",
    "CLASS_NAMES = sorted(os.listdir(cfg.DATA_PATHS[\"train\"]))\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "logger.info(f\"Classes ({NUM_CLASSES}): {CLASS_NAMES}\")\n",
    "logger.info(f\"timm={timm.__version__} torch={torch.__version__} device={cfg.DEVICE}\")\n",
    "\n",
    "# ---------------------------- TRANSFORMS ----------------------------\n",
    "tf_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(cfg.IMG_SIZE, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# STRICT eval for ALL models: Resize directly to (H,W), no CenterCrop\n",
    "tf_eval_strict = transforms.Compose([\n",
    "    transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ---------------------------- DATA ----------------------------\n",
    "train_ds = datasets.ImageFolder(cfg.DATA_PATHS[\"train\"], transform=tf_train)\n",
    "val_ds   = datasets.ImageFolder(cfg.DATA_PATHS[\"val\"], transform=tf_eval_strict)\n",
    "test_ds  = datasets.ImageFolder(cfg.DATA_PATHS[\"test\"], transform=tf_eval_strict)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=cfg.BATCH, shuffle=True,\n",
    "    num_workers=cfg.NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=cfg.BATCH, shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=cfg.BATCH, shuffle=False,\n",
    "    num_workers=cfg.NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "logger.info(f\"Dataset sizes | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)}\")\n",
    "logger.info(\"Eval protocol: STRICT resize to (IMG_SIZE, IMG_SIZE) with NO CenterCrop (applied to ALL models).\")\n",
    "\n",
    "# ---------------------------- HELPERS ----------------------------\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def estimate_state_dict_size_mb(model):\n",
    "    total_params, _ = count_params(model)\n",
    "    return (total_params * 4) / (1024**2)  # fp32 bytes\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds_all, tgts_all = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(cfg.DEVICE)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        preds_all.append(preds)\n",
    "        tgts_all.append(y.numpy())\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    tgts_all = np.concatenate(tgts_all)\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(accuracy_score(tgts_all, preds_all)),\n",
    "        \"bal_acc\": float(balanced_accuracy_score(tgts_all, preds_all)),\n",
    "        \"f1_macro\": float(f1_score(tgts_all, preds_all, average=\"macro\")),\n",
    "        \"report\": classification_report(tgts_all, preds_all, target_names=CLASS_NAMES, digits=4),\n",
    "    }\n",
    "\n",
    "def finetune_ce(model, run_dir):\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WD)\n",
    "    crit = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)\n",
    "    scaler = GradScaler(device=\"cuda\", enabled=(cfg.AMP and cfg.DEVICE == \"cuda\"))\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    for ep in range(cfg.EPOCHS):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {ep+1}/{cfg.EPOCHS}\", leave=False):\n",
    "            x, y = x.to(cfg.DEVICE), y.to(cfg.DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\" if cfg.DEVICE == \"cuda\" else \"cpu\", enabled=scaler.is_enabled()):\n",
    "                logits = model(x)\n",
    "                loss = crit(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        valm = evaluate(model, val_loader)\n",
    "        logger.info(f\"[VAL] ep={ep+1} loss={np.mean(losses):.4f} val_f1={valm['f1_macro']:.4f} val_acc={valm['acc']:.4f}\")\n",
    "\n",
    "        if valm[\"f1_macro\"] > best_f1 + 1e-6:\n",
    "            best_f1 = valm[\"f1_macro\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            torch.save(best_state, os.path.join(run_dir, \"best_ft.pth\"))\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= cfg.PATIENCE:\n",
    "                logger.info(f\"Early stopping at epoch {ep+1}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    testm = evaluate(model, test_loader)\n",
    "    logger.info(f\"[TEST] acc={testm['acc']:.4f} bal_acc={testm['bal_acc']:.4f} f1={testm['f1_macro']:.4f}\")\n",
    "    print(\"\\nTEST REPORT:\\n\", testm[\"report\"])\n",
    "    return testm\n",
    "\n",
    "# ---------------------------- FIXED build_model ----------------------------\n",
    "def build_model(model_id, pretrained, run_dir):\n",
    "    model = timm.create_model(\n",
    "        model_id,\n",
    "        pretrained=pretrained,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        img_size=cfg.IMG_SIZE,\n",
    "    ).to(cfg.DEVICE)\n",
    "\n",
    "    pcfg = getattr(model, \"pretrained_cfg\", {}) or {}\n",
    "\n",
    "    # Only report pretrained_cfg when pretrained=True\n",
    "    tag = pcfg.get(\"tag\", None) if pretrained else None\n",
    "    dataset = pcfg.get(\"dataset\", None) if pretrained else None\n",
    "    url = pcfg.get(\"url\", None) if pretrained else None\n",
    "    pcfg_num_classes = pcfg.get(\"num_classes\", None) if pretrained else None\n",
    "    init_desc = \"pretrained weights\" if pretrained else \"random initialization\"\n",
    "\n",
    "    total_params, trainable_params = count_params(model)\n",
    "    est_mb = estimate_state_dict_size_mb(model)\n",
    "\n",
    "    meta = {\n",
    "        \"model_id\": model_id,\n",
    "        \"pretrained\": pretrained,\n",
    "        \"init_desc\": init_desc,\n",
    "        \"img_size\": cfg.IMG_SIZE,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"timm_version\": timm.__version__,\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"device\": cfg.DEVICE,\n",
    "        \"eval_transform\": \"STRICT Resize((IMG_SIZE, IMG_SIZE)) no CenterCrop\",\n",
    "        \"pretrained_cfg\": {\n",
    "            \"dataset\": dataset,\n",
    "            \"tag\": tag,\n",
    "            \"url\": url,\n",
    "            \"num_classes\": pcfg_num_classes,\n",
    "        },\n",
    "        \"params_total\": int(total_params),\n",
    "        \"params_trainable\": int(trainable_params),\n",
    "        \"estimated_fp32_state_dict_mb\": float(est_mb),\n",
    "        \"timestamp\": cfg.RUN_TS\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(run_dir, \"model_metadata.json\"), \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Model: {model_id} | pretrained={pretrained} | init={init_desc}\")\n",
    "    logger.info(f\"pretrained_cfg.tag={tag} dataset={dataset}\")\n",
    "    logger.info(f\"Params: {total_params/1e6:.2f}M | est fp32 weights ~ {est_mb:.1f} MB\")\n",
    "\n",
    "    return model, meta\n",
    "\n",
    "# ---------------------------- EXPERIMENT LIST ----------------------------\n",
    "EXPS = [\n",
    "    {\"name\": \"BASE_scratch\", \"model_id\": cfg.BASE_SCRATCH, \"pretrained\": False},\n",
    "    {\"name\": \"BASE_in22k_ft_in1k\", \"model_id\": cfg.BASE_IN22K_FT_IN1K, \"pretrained\": True},\n",
    "]\n",
    "\n",
    "if cfg.RUN_SMALL_IN1K_BASELINE:\n",
    "    EXPS.append({\"name\": \"SMALL_in1k_baseline\", \"model_id\": cfg.SMALL_IN1K, \"pretrained\": True})\n",
    "\n",
    "results_csv = os.path.join(cfg.OUTPUT_DIR, f\"results_{cfg.RUN_TS}.csv\")\n",
    "rows = []\n",
    "\n",
    "logger.info(\"Experiments:\")\n",
    "for e in EXPS:\n",
    "    logger.info(f\" - {e['name']} | {e['model_id']} | pretrained={e['pretrained']}\")\n",
    "\n",
    "# ---------------------------- RUN ----------------------------\n",
    "for exp in EXPS:\n",
    "    run_dir = os.path.join(cfg.OUTPUT_DIR, exp[\"name\"])\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    logger.info(\"=\"*100)\n",
    "    logger.info(f\"Running: {exp['name']}\")\n",
    "\n",
    "    model, meta = build_model(exp[\"model_id\"], exp[\"pretrained\"], run_dir)\n",
    "    testm = finetune_ce(model, run_dir)\n",
    "\n",
    "    row = {\n",
    "        \"exp_name\": exp[\"name\"],\n",
    "        \"model_id\": exp[\"model_id\"],\n",
    "        \"pretrained\": exp[\"pretrained\"],\n",
    "        \"init_desc\": meta.get(\"init_desc\"),\n",
    "        \"eval_transform\": meta.get(\"eval_transform\"),\n",
    "        \"pretrained_cfg.dataset\": meta[\"pretrained_cfg\"][\"dataset\"],\n",
    "        \"pretrained_cfg.tag\": meta[\"pretrained_cfg\"][\"tag\"],\n",
    "        \"params_total_M\": round(meta[\"params_total\"] / 1e6, 3),\n",
    "        \"estimated_fp32_state_dict_mb\": round(meta[\"estimated_fp32_state_dict_mb\"], 2),\n",
    "        \"acc\": testm[\"acc\"],\n",
    "        \"bal_acc\": testm[\"bal_acc\"],\n",
    "        \"f1_macro\": testm[\"f1_macro\"],\n",
    "        \"img_size\": cfg.IMG_SIZE,\n",
    "        \"epochs_max\": cfg.EPOCHS,\n",
    "        \"label_smoothing\": cfg.LABEL_SMOOTHING,\n",
    "        \"lr\": cfg.LR,\n",
    "        \"wd\": cfg.WD,\n",
    "        \"timm_version\": timm.__version__,\n",
    "        \"torch_version\": torch.__version__,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "    write_header = not os.path.exists(results_csv)\n",
    "    with open(results_csv, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "logger.info(f\"Done! Results saved: {results_csv}\")\n",
    "print(\"\\nRESULTS CSV:\", results_csv)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "for r in rows:\n",
    "    print(f\"{r['exp_name']:20s} | f1={r['f1_macro']:.4f} acc={r['acc']:.4f} \"\n",
    "          f\"| params={r['params_total_M']:.2f}M | init={r['init_desc']} | tag={r['pretrained_cfg.tag']}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
