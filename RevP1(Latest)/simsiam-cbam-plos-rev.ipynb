{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%writefile pretrain_simsiam_cbam.py\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Iterable, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class PretrainConfig:\n",
    "    root_path: str = \"/kaggle/input/minida/mini_output1/pretrain\"\n",
    "    out_dir: str = \"/kaggle/working/simsiam_cbam\"\n",
    "    epochs: int = 200\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 2\n",
    "    accumulation_steps: int = 1\n",
    "\n",
    "    # optimizer\n",
    "    wd: float = 1e-4\n",
    "    momentum: float = 0.9\n",
    "    base_lr_ref: float = 0.05  # SimSiam style\n",
    "    lr_ref_bs: int = 256\n",
    "\n",
    "    # reproducibility\n",
    "    seed: int = 42\n",
    "    deterministic: bool = True\n",
    "\n",
    "    # backbone BN behavior in pretrain\n",
    "    # Keep default as your working run (True) to avoid changing accuracy behavior unexpectedly.\n",
    "    fix_backbone_bn: bool = True\n",
    "\n",
    "    # torch.compile safety\n",
    "    enable_compile: bool = True\n",
    "\n",
    "    # checkpoint every N epochs\n",
    "    save_every: int = 10\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "def seed_all(seed: int = 42, deterministic: bool = True):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CBAM-ResNet50 backbone\n",
    "# ----------------------------\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(in_planes // ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    \"\"\"Bottleneck + CBAM after bn3 and before residual add.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        c_out = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(c_out)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def cbam_resnet50(*, norm_layer=None, **kwargs) -> ResNet:\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], norm_layer=norm_layer, **kwargs)\n",
    "    # Zero-init last BN in residual branch for stability (makes block initially identity-like)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, Bottleneck) and hasattr(m, \"bn3\") and m.bn3.weight is not None:\n",
    "            nn.init.constant_(m.bn3.weight, 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optim helper: param-wise WD\n",
    "# ----------------------------\n",
    "def exclude_from_wd(named_params: Iterable[Tuple[str, torch.nn.Parameter]], wd: float):\n",
    "    wd_params, no_wd_params = [], []\n",
    "    for n, p in named_params:\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim == 1 or n.endswith(\".bias\") or \"bn\" in n.lower():\n",
    "            no_wd_params.append(p)\n",
    "        else:\n",
    "            wd_params.append(p)\n",
    "    return [{\"params\": wd_params, \"weight_decay\": wd},\n",
    "            {\"params\": no_wd_params, \"weight_decay\": 0.0}]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# SimSiam heads\n",
    "# ----------------------------\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=2048, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim, bias=False),\n",
    "            nn.BatchNorm1d(out_dim, affine=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(self, fix_backbone_bn: bool = True):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # to avgpool\n",
    "        self.projector = MLPHead(2048)\n",
    "        self.predictor = PredictionHead()\n",
    "        self.fix_backbone_bn = fix_backbone_bn\n",
    "\n",
    "        self._frozen_bn_modules = []\n",
    "        if self.fix_backbone_bn:\n",
    "            for m in self.backbone.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.requires_grad_(False)\n",
    "                    self._frozen_bn_modules.append(m)\n",
    "\n",
    "    def _forward_backbone(self, x):\n",
    "        x = self.backbone(x)      # (B, 2048, 1, 1)\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.projector(self._forward_backbone(x1))\n",
    "        z2 = self.projector(self._forward_backbone(x2))\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        if self.fix_backbone_bn:\n",
    "            for m in self._frozen_bn_modules:\n",
    "                m.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "def neg_cos_sim(p, z):\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset\n",
    "# ----------------------------\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform):\n",
    "        self.transform = transform\n",
    "        exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\")\n",
    "        self.files = []\n",
    "        for dp, _, fns in os.walk(root_dir):\n",
    "            for fn in fns:\n",
    "                if fn.lower().endswith(exts):\n",
    "                    self.files.append(os.path.join(dp, fn))\n",
    "        self.files.sort()\n",
    "        if not self.files:\n",
    "            raise RuntimeError(f\"No images found under: {root_dir}\")\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = Image.open(self.files[idx]).convert(\"RGB\")\n",
    "        return self.transform(img), self.transform(img)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def maybe_compile(model: nn.Module, enable: bool = True) -> nn.Module:\n",
    "    if not enable: \n",
    "        return model\n",
    "    if not hasattr(torch, \"compile\") or not torch.cuda.is_available():\n",
    "        return model\n",
    "    try:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major < 7:\n",
    "            print(f\"[compile] Skipping torch.compile (CC {major}.x < 7.0)\")\n",
    "            return model\n",
    "        return torch.compile(model)\n",
    "    except Exception as e:\n",
    "        print(\"[compile] failed -> eager:\", e)\n",
    "        return model\n",
    "\n",
    "\n",
    "def pretrain(cfg: PretrainConfig):\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "    seed_all(cfg.seed, cfg.deterministic)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # augmentations (SimSiam strong augs)\n",
    "    crop_size = 224\n",
    "    blur_kernel = max(int(0.1 * crop_size) // 2 * 2 + 1, 3)\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(crop_size, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=blur_kernel, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    ds = UnlabeledDataset(cfg.root_path, tfm)\n",
    "    dl = DataLoader(\n",
    "        ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=(cfg.num_workers > 0),\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    model = SimSiam(fix_backbone_bn=cfg.fix_backbone_bn).to(device)\n",
    "    model = maybe_compile(model, cfg.enable_compile)\n",
    "\n",
    "    # LR scaling (keep your behavior)\n",
    "    global_bs = cfg.batch_size\n",
    "    lr = cfg.base_lr_ref * max(min(global_bs, 1024), 64) / cfg.lr_ref_bs\n",
    "\n",
    "    param_groups = exclude_from_wd(model.named_parameters(), wd=cfg.wd)\n",
    "    opt = torch.optim.SGD(param_groups, lr=lr, momentum=cfg.momentum)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    # save config\n",
    "    with open(os.path.join(cfg.out_dir, \"pretrain_config.json\"), \"w\") as f:\n",
    "        json.dump(asdict(cfg), f, indent=2)\n",
    "\n",
    "    ckpt_path = os.path.join(cfg.out_dir, \"simsiam_cbam_checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        target.load_state_dict(ckpt[\"model\"])\n",
    "        opt.load_state_dict(ckpt[\"optimizer\"])\n",
    "        sched.load_state_dict(ckpt[\"scheduler\"])\n",
    "        if \"scaler\" in ckpt:\n",
    "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "        start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
    "        print(\"Resumed from epoch:\", start_epoch)\n",
    "\n",
    "    print(f\"Pretraining {cfg.epochs} epochs (from {start_epoch})... | lr={lr:.6f}\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, cfg.epochs):\n",
    "        target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        target.train(True)\n",
    "\n",
    "        total = 0.0\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        micro = 0\n",
    "\n",
    "        for x1, x2 in dl:\n",
    "            x1 = x1.to(device, non_blocking=True)\n",
    "            x2 = x2.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "                p1, p2, z1, z2 = model(x1, x2)\n",
    "                loss_full = 0.5 * (neg_cos_sim(p1, z2) + neg_cos_sim(p2, z1))\n",
    "                loss = loss_full / max(cfg.accumulation_steps, 1)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro += 1\n",
    "\n",
    "            if micro == cfg.accumulation_steps:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                micro = 0\n",
    "\n",
    "            total += float(loss_full.item())\n",
    "\n",
    "        if micro > 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_loss = total / len(dl)\n",
    "        sched.step()\n",
    "\n",
    "        if (epoch + 1) % cfg.save_every == 0:\n",
    "            ckpt_target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": ckpt_target.state_dict(),\n",
    "                \"optimizer\": opt.state_dict(),\n",
    "                \"scheduler\": sched.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "            }, ckpt_path)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:03d}/{cfg.epochs} | loss={avg_loss:.4f}\")\n",
    "\n",
    "    final_path = os.path.join(cfg.out_dir, \"simsiam_cbam_pretrained_final.pth\")\n",
    "    ckpt_target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "    torch.save({\n",
    "        \"backbone\": ckpt_target.backbone.state_dict(),\n",
    "        \"projector\": ckpt_target.projector.state_dict(),\n",
    "        \"predictor\": ckpt_target.predictor.state_dict(),\n",
    "    }, final_path)\n",
    "\n",
    "    print(\"Saved:\", final_path)\n",
    "    print(\"Total time (min):\", (time.time() - t0) / 60.0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg = PretrainConfig()\n",
    "    pretrain(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T22:26:15.653575Z",
     "iopub.status.busy": "2025-12-05T22:26:15.653199Z",
     "iopub.status.idle": "2025-12-05T22:26:15.672370Z",
     "shell.execute_reply": "2025-12-05T22:26:15.671812Z",
     "shell.execute_reply.started": "2025-12-05T22:26:15.653544Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetune_cbam_eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune_cbam_eval.py\n",
    "import os\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"adaptive_max_pool2d_backward_cuda does not have a deterministic implementation.*\"\n",
    ")\n",
    "\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import RandAugment\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "# =========================================================\n",
    "# AMP compatibility wrapper\n",
    "# =========================================================\n",
    "def make_amp(device: torch.device):\n",
    "    use_cuda = (device.type == \"cuda\")\n",
    "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"autocast\") and hasattr(torch.amp, \"GradScaler\"):\n",
    "        autocast_fn = lambda: torch.amp.autocast(device_type=\"cuda\", enabled=use_cuda)\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=use_cuda)\n",
    "        return autocast_fn, scaler\n",
    "    autocast_fn = lambda: torch.cuda.amp.autocast(enabled=use_cuda)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_cuda)\n",
    "    return autocast_fn, scaler\n",
    "\n",
    "# =========================================================\n",
    "# Determinism (CBAM-safe): warn_only avoids crash from AdaptiveMaxPool2d backward\n",
    "# =========================================================\n",
    "def seed_all(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # TF32 off for more repeatability\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # IMPORTANT: CBAM uses AdaptiveMaxPool2d -> non-deterministic CUDA backward\n",
    "    # So we enable deterministic algorithms but warn_only to avoid runtime error.\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except TypeError:\n",
    "        # older torch\n",
    "        try:\n",
    "            torch.use_deterministic_algorithms(True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def seed_worker(worker_id: int):\n",
    "    wseed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(wseed)\n",
    "    random.seed(wseed)\n",
    "\n",
    "# =========================================================\n",
    "# Mixup\n",
    "# =========================================================\n",
    "def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float, device: torch.device):\n",
    "    if alpha and alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    bs = x.size(0)\n",
    "    idx = torch.randperm(bs, device=device)\n",
    "    mixed = lam * x + (1.0 - lam) * x[idx]\n",
    "    return mixed, y, y[idx], lam\n",
    "\n",
    "def mixup_ce_loss(ce, logits, y_a, y_b, lam: float):\n",
    "    return lam * ce(logits, y_a) + (1.0 - lam) * ce(logits, y_b)\n",
    "\n",
    "# =========================================================\n",
    "# SupCon (multi-view, correct labels/mask)\n",
    "# =========================================================\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-view supervised contrastive loss.\n",
    "    Expects feats of shape (B, V, D) where V>=2.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.07, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.t = float(temperature)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def forward(self, feats: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        device = feats.device\n",
    "        B, V, D = feats.shape\n",
    "\n",
    "        feats = F.normalize(feats, dim=2).float()      # (B,V,D)\n",
    "        feats = feats.view(B * V, D)                   # (BV,D)\n",
    "\n",
    "        labels = labels.contiguous().view(B, 1)        # (B,1)\n",
    "        labels = labels.repeat(1, V).view(B * V, 1)    # (BV,1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)  # (BV,BV)\n",
    "        mask.fill_diagonal_(0)\n",
    "\n",
    "        logits = (feats @ feats.T) / self.t\n",
    "        logits = logits - logits.max(dim=1, keepdim=True).values.detach()\n",
    "\n",
    "        eye = torch.eye(B * V, device=device)\n",
    "        exp_logits = torch.exp(logits) * (1.0 - eye)\n",
    "        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + self.eps)\n",
    "\n",
    "        pos_count = mask.sum(dim=1)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (pos_count + self.eps)\n",
    "        return -mean_log_prob_pos.mean()\n",
    "\n",
    "# =========================================================\n",
    "# Two-view wrapper for HYBRID training\n",
    "# =========================================================\n",
    "class TwoCropTransform:\n",
    "    def __init__(self, base_transform):\n",
    "        self.base = base_transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.base(img), self.base(img)\n",
    "\n",
    "class ImageFolderTwoView(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, y = self.samples[index]\n",
    "        img = self.loader(path)\n",
    "        x1, x2 = self.transform(img)\n",
    "        return (x1, x2), y\n",
    "\n",
    "# =========================================================\n",
    "# CBAM-ResNet50 definition (must match pretrain architecture)\n",
    "# =========================================================\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(in_planes // ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    \"\"\"Bottleneck + CBAM after bn3 and before residual add.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        c_out = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(c_out)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def cbam_resnet50(**kwargs) -> ResNet:\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    # zero-init last BN (stability)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, Bottleneck) and hasattr(m, \"bn3\") and m.bn3.weight is not None:\n",
    "            nn.init.constant_(m.bn3.weight, 0)\n",
    "    return model\n",
    "\n",
    "# =========================================================\n",
    "# Fine-tune model (CBAM backbone + classifier + optional SupCon head)\n",
    "# =========================================================\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path: str, num_classes: int, hybrid: bool):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # to avgpool\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.30),\n",
    "            nn.Linear(512, num_classes, bias=True),\n",
    "        )\n",
    "\n",
    "        self.hybrid = bool(hybrid)\n",
    "        self.supcon_proj = nn.Linear(2048, 128, bias=True) if self.hybrid else None\n",
    "\n",
    "        ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        # expected: {\"backbone\": ..., \"projector\": ..., \"predictor\": ...}\n",
    "        if \"backbone\" in ckpt:\n",
    "            missing, unexpected = self.backbone.load_state_dict(ckpt[\"backbone\"], strict=False)\n",
    "        else:\n",
    "            # in case user saved backbone-only\n",
    "            missing, unexpected = self.backbone.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "        if missing or unexpected:\n",
    "            print(f\"[state_dict notice] missing: {missing} | unexpected: {unexpected}\")\n",
    "\n",
    "    def forward_feats(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x).flatten(1)  # (B,2048)\n",
    "\n",
    "    def logits_from_feats(self, feats: torch.Tensor) -> torch.Tensor:\n",
    "        return self.classifier(feats)\n",
    "\n",
    "    def supcon_from_feats(self, feats: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.hybrid:\n",
    "            raise RuntimeError(\"supcon_from_feats called while hybrid=False\")\n",
    "        return F.normalize(self.supcon_proj(feats), dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.forward_feats(x)\n",
    "        return self.logits_from_feats(feats)\n",
    "\n",
    "def count_params_m(model: nn.Module) -> float:\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "# =========================================================\n",
    "# Data\n",
    "# =========================================================\n",
    "def build_datasets(data_root: str):\n",
    "    train_dir = os.path.join(data_root, \"train\")\n",
    "    val_dir   = os.path.join(data_root, \"val\")\n",
    "    test_dir  = os.path.join(data_root, \"test\")\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.7, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        RandAugment(num_ops=2, magnitude=7),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    train_ds_ce = ImageFolder(train_dir, transform=train_tf)\n",
    "    train_ds_hybrid = ImageFolderTwoView(train_dir, transform=TwoCropTransform(train_tf))\n",
    "\n",
    "    val_ds  = ImageFolder(val_dir,  transform=eval_tf)\n",
    "    test_ds = ImageFolder(test_dir, transform=eval_tf)\n",
    "\n",
    "    class_names = train_ds_ce.classes\n",
    "    assert val_ds.classes == class_names and test_ds.classes == class_names, \"Split class mismatch.\"\n",
    "    assert train_ds_hybrid.classes == class_names, \"HYBRID train classes mismatch.\"\n",
    "\n",
    "    return train_ds_ce, train_ds_hybrid, val_ds, test_ds, class_names, train_dir, val_dir, test_dir\n",
    "\n",
    "def build_loaders(train_ds, val_ds, test_ds, batch_size, num_workers, seed, use_weighted_sampler: bool):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    pin = torch.cuda.is_available()\n",
    "\n",
    "    if use_weighted_sampler:\n",
    "        targets = np.array(train_ds.targets, dtype=np.int64)\n",
    "        counts = np.bincount(targets)\n",
    "        counts[counts == 0] = 1\n",
    "        class_w = 1.0 / counts\n",
    "        sample_w = class_w[targets]\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights=torch.as_tensor(sample_w, dtype=torch.double),\n",
    "            num_samples=len(sample_w),\n",
    "            replacement=True\n",
    "        )\n",
    "        shuffle = False\n",
    "    else:\n",
    "        sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size,\n",
    "        shuffle=shuffle, sampler=sampler,\n",
    "        num_workers=num_workers, pin_memory=pin,\n",
    "        worker_init_fn=seed_worker, generator=g,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin,\n",
    "        worker_init_fn=seed_worker, generator=g,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin,\n",
    "        worker_init_fn=seed_worker, generator=g,\n",
    "        persistent_workers=True if num_workers > 0 else False\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# =========================================================\n",
    "# Eval helpers\n",
    "# =========================================================\n",
    "def macro_roc_auc(y_true: np.ndarray, probs: np.ndarray, n_classes: int) -> float:\n",
    "    onehot = np.eye(n_classes)[y_true]\n",
    "    return float(roc_auc_score(onehot, probs, average=\"macro\", multi_class=\"ovr\"))\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(model, loader, device) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    model.eval()\n",
    "    ce_plain = nn.CrossEntropyLoss()\n",
    "    ys, preds, probs = [], [], []\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        pr = F.softmax(logits, dim=1)\n",
    "        loss = ce_plain(logits, y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        total_loss += float(loss.item()) * bs\n",
    "        n += bs\n",
    "\n",
    "        ys.append(y.cpu().numpy())\n",
    "        preds.append(logits.argmax(1).cpu().numpy())\n",
    "        probs.append(pr.cpu().numpy())\n",
    "\n",
    "    y = np.concatenate(ys)\n",
    "    p = np.concatenate(preds)\n",
    "    pr = np.concatenate(probs)\n",
    "    return y, p, pr, total_loss / max(n, 1)\n",
    "\n",
    "# =========================================================\n",
    "# TTA (val selection only, then apply to test)\n",
    "# =========================================================\n",
    "def _tta_views_transforms(n_views: int):\n",
    "    norm = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    v1 = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), norm]),\n",
    "    ]\n",
    "    v2 = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), norm]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(), norm]),\n",
    "    ]\n",
    "    v4 = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), norm]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(), norm]),\n",
    "        transforms.Compose([transforms.Resize(288), transforms.CenterCrop(224), transforms.ToTensor(), norm]),\n",
    "        transforms.Compose([transforms.Resize(288), transforms.CenterCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(p=1.0), transforms.ToTensor(), norm]),\n",
    "    ]\n",
    "    if n_views == 1: return v1, \"V1: center@256\"\n",
    "    if n_views == 2: return v2, \"V2: center@256 + hflip@256\"\n",
    "    if n_views == 4: return v4, \"V4: center/hflip @256 and @288\"\n",
    "    raise ValueError(\"n_views must be one of {1,2,4}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def tta_probs(model, dataset_dir: str, class_names: List[str], device, batch_size, num_workers, seed, n_views: int):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    base_ds = ImageFolder(dataset_dir)\n",
    "    assert base_ds.classes == class_names, \"TTA dataset class order mismatch.\"\n",
    "\n",
    "    tfms, policy = _tta_views_transforms(n_views)\n",
    "    y_true = np.array(base_ds.targets, dtype=np.int64)\n",
    "\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "    for tfm in tfms:\n",
    "        base_ds.transform = tfm\n",
    "        loader = DataLoader(\n",
    "            base_ds, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=num_workers, pin_memory=torch.cuda.is_available(),\n",
    "            worker_init_fn=seed_worker, generator=g,\n",
    "            persistent_workers=True if num_workers > 0 else False\n",
    "        )\n",
    "        chunks = []\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            chunks.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "        probs_all.append(np.concatenate(chunks, axis=0))\n",
    "\n",
    "    mean_probs = np.mean(probs_all, axis=0)\n",
    "    return y_true, mean_probs, policy\n",
    "\n",
    "def pick_tta_policy_on_val(model, val_dir: str, class_names: List[str], device, batch_size, num_workers, seed,\n",
    "                           candidate_views=(1, 2, 4)):\n",
    "    best = {\"views\": 1, \"acc\": -1.0, \"policy\": \"\"}\n",
    "    per_policy = {}\n",
    "\n",
    "    for k in candidate_views:\n",
    "        y, probs, policy = tta_probs(\n",
    "            model=model, dataset_dir=val_dir, class_names=class_names,\n",
    "            device=device, batch_size=batch_size, num_workers=num_workers, seed=seed, n_views=k\n",
    "        )\n",
    "        pred = probs.argmax(axis=1)\n",
    "        acc = float(accuracy_score(y, pred))\n",
    "        per_policy[str(k)] = {\"val_acc\": acc, \"policy\": policy}\n",
    "        if acc > best[\"acc\"]:\n",
    "            best = {\"views\": int(k), \"acc\": acc, \"policy\": policy}\n",
    "\n",
    "    return best, per_policy\n",
    "\n",
    "# =========================================================\n",
    "# Train one seed\n",
    "# =========================================================\n",
    "def train_one_seed(\n",
    "    mode: str,\n",
    "    seed: int,\n",
    "    pretrained_path: str,\n",
    "    train_ds, val_ds, test_ds,\n",
    "    class_names: List[str],\n",
    "    val_dir: str,\n",
    "    test_dir: str,\n",
    "    device: torch.device,\n",
    "    out_dir: str,\n",
    "    epochs: int,\n",
    "    patience: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    use_weighted_sampler: bool,\n",
    "    lr_backbone: float,\n",
    "    lr_head: float,\n",
    "    weight_decay: float,\n",
    "    label_smoothing: float,\n",
    "    mixup_alpha: float,\n",
    "    supcon_weight: float,\n",
    "    temperature: float,\n",
    "    tta_candidates: List[int],\n",
    ") -> Dict:\n",
    "    seed_all(seed)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    hybrid = (mode.upper() == \"HYBRID\")\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=len(class_names), hybrid=hybrid).to(device)\n",
    "\n",
    "    # BN trainable\n",
    "    model.train()\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "            m.train()\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "    train_loader, val_loader, test_loader = build_loaders(\n",
    "        train_ds, val_ds, test_ds,\n",
    "        batch_size=batch_size, num_workers=num_workers, seed=seed,\n",
    "        use_weighted_sampler=use_weighted_sampler\n",
    "    )\n",
    "\n",
    "    # sampler => unweighted CE (avoid double-compensation)\n",
    "    if use_weighted_sampler:\n",
    "        ce = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    else:\n",
    "        targets = np.array(train_ds.targets, dtype=np.int64)\n",
    "        counts = np.bincount(targets)\n",
    "        counts[counts == 0] = 1\n",
    "        w = (1.0 / counts)\n",
    "        w = w / w.mean()\n",
    "        ce = nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor(w, dtype=torch.float32, device=device),\n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "\n",
    "    supcon = SupConLoss(temperature=temperature) if hybrid else None\n",
    "\n",
    "    # param groups: backbone vs head (+ supcon head is included in head group)\n",
    "    backbone_params, head_params = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if n.startswith(\"backbone\"):\n",
    "            backbone_params.append(p)\n",
    "        else:\n",
    "            head_params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [{\"params\": backbone_params, \"lr\": lr_backbone},\n",
    "         {\"params\": head_params, \"lr\": lr_head}],\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    autocast_ctx, scaler = make_amp(device)\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    bad = 0\n",
    "    best_path = os.path.join(out_dir, f\"best_{mode.lower()}_seed{seed}.pth\")\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0.0\n",
    "        nseen = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if hybrid:\n",
    "                (x1, x2), y = batch\n",
    "                x1 = x1.to(device, non_blocking=True)\n",
    "                x2 = x2.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast_ctx():\n",
    "                    # CE on view-1 (optional mixup on view-1 only)\n",
    "                    if mixup_alpha and mixup_alpha > 0:\n",
    "                        xmix, ya, yb, lam = mixup_data(x1, y, mixup_alpha, device)\n",
    "                        feats_mix = model.forward_feats(xmix)\n",
    "                        logits_mix = model.logits_from_feats(feats_mix)\n",
    "                        loss_ce = mixup_ce_loss(ce, logits_mix, ya, yb, lam)\n",
    "\n",
    "                        pred = logits_mix.argmax(1)\n",
    "                        correct += lam * pred.eq(ya).sum().item() + (1.0 - lam) * pred.eq(yb).sum().item()\n",
    "                    else:\n",
    "                        feats1 = model.forward_feats(x1)\n",
    "                        logits1 = model.logits_from_feats(feats1)\n",
    "                        loss_ce = ce(logits1, y)\n",
    "\n",
    "                        pred = logits1.argmax(1)\n",
    "                        correct += pred.eq(y).sum().item()\n",
    "\n",
    "                    # SupCon on clean views\n",
    "                    feats1_clean = model.forward_feats(x1)\n",
    "                    feats2_clean = model.forward_feats(x2)\n",
    "                    z1 = model.supcon_from_feats(feats1_clean)\n",
    "                    z2 = model.supcon_from_feats(feats2_clean)\n",
    "\n",
    "                    feats = torch.stack([z1, z2], dim=1)  # (B,2,128)\n",
    "                    loss_sup = supcon(feats, y)\n",
    "\n",
    "                    loss = loss_ce + supcon_weight * loss_sup\n",
    "\n",
    "            else:\n",
    "                x, y = batch\n",
    "                x = x.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast_ctx():\n",
    "                    if mixup_alpha and mixup_alpha > 0:\n",
    "                        xmix, ya, yb, lam = mixup_data(x, y, mixup_alpha, device)\n",
    "                        feats_mix = model.forward_feats(xmix)\n",
    "                        logits = model.logits_from_feats(feats_mix)\n",
    "                        loss = mixup_ce_loss(ce, logits, ya, yb, lam)\n",
    "\n",
    "                        pred = logits.argmax(1)\n",
    "                        correct += lam * pred.eq(ya).sum().item() + (1.0 - lam) * pred.eq(yb).sum().item()\n",
    "                    else:\n",
    "                        feats = model.forward_feats(x)\n",
    "                        logits = model.logits_from_feats(feats)\n",
    "                        loss = ce(logits, y)\n",
    "\n",
    "                        pred = logits.argmax(1)\n",
    "                        correct += pred.eq(y).sum().item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            bs = y.size(0)\n",
    "            total_loss += float(loss.item()) * bs\n",
    "            nseen += bs\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss = total_loss / max(nseen, 1)\n",
    "        train_acc = float(correct) / max(nseen, 1)\n",
    "\n",
    "        yv, pv, _, vloss = eval_loader(model, val_loader, device)\n",
    "        val_acc = float(accuracy_score(yv, pv))\n",
    "        val_f1 = float(f1_score(yv, pv, average=\"macro\"))\n",
    "\n",
    "        saved = \"\"\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            saved = \"(saved)\"\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        print(f\"[{mode}][seed={seed}] Epoch {ep:02d}/{epochs} | \"\n",
    "              f\"train loss {train_loss:.4f} acc {train_acc:.4f} || \"\n",
    "              f\"val loss {vloss:.4f} acc {val_acc:.4f} macroF1 {val_f1:.4f} {saved}\")\n",
    "\n",
    "        if bad >= patience:\n",
    "            print(f\"[{mode}][seed={seed}] Early stopping.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    yt, pt, prt, tloss = eval_loader(model, test_loader, device)\n",
    "    test_acc = float(accuracy_score(yt, pt))\n",
    "    auc = float(macro_roc_auc(yt, prt, len(class_names)))\n",
    "\n",
    "    best_tta, per_policy = pick_tta_policy_on_val(\n",
    "        model=model,\n",
    "        val_dir=val_dir,\n",
    "        class_names=class_names,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        seed=seed,\n",
    "        candidate_views=tuple(tta_candidates)\n",
    "    )\n",
    "\n",
    "    ytta, prob_tta, chosen_policy = tta_probs(\n",
    "        model=model,\n",
    "        dataset_dir=test_dir,\n",
    "        class_names=class_names,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        seed=seed,\n",
    "        n_views=best_tta[\"views\"]\n",
    "    )\n",
    "    pred_tta = prob_tta.argmax(axis=1)\n",
    "    tta_acc = float(accuracy_score(ytta, pred_tta))\n",
    "    auc_tta = float(macro_roc_auc(ytta, prob_tta, len(class_names)))\n",
    "\n",
    "    rep = classification_report(yt, pt, target_names=class_names, digits=4)\n",
    "    cm = confusion_matrix(yt, pt).tolist()\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"seed\": int(seed),\n",
    "        \"epochs_ran\": int(ep),\n",
    "        \"best_val_acc\": float(best_val_acc),\n",
    "        \"test_loss\": float(tloss),\n",
    "        \"test_acc\": float(test_acc),\n",
    "        \"tta_acc\": float(tta_acc),\n",
    "        \"macro_auc\": float(auc),\n",
    "        \"macro_auc_tta\": float(auc_tta),\n",
    "        \"params_m\": float(round(count_params_m(model), 4)),\n",
    "        \"tta_candidates\": list(map(int, tta_candidates)),\n",
    "        \"tta_val_selection\": {\n",
    "            \"picked_views\": int(best_tta[\"views\"]),\n",
    "            \"picked_val_acc\": float(best_tta[\"acc\"]),\n",
    "            \"picked_policy\": best_tta[\"policy\"],\n",
    "            \"all_candidates\": per_policy\n",
    "        },\n",
    "        \"tta_policy_applied_to_test\": chosen_policy,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": rep,\n",
    "        \"checkpoint_path\": best_path\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# Summary\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class SummaryStats:\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "def summarize(values: List[float]) -> SummaryStats:\n",
    "    a = np.array(values, dtype=np.float64)\n",
    "    if len(a) <= 1:\n",
    "        return SummaryStats(mean=float(a.mean()) if len(a) else 0.0, std=0.0)\n",
    "    return SummaryStats(mean=float(a.mean()), std=float(a.std(ddof=1)))\n",
    "\n",
    "# =========================================================\n",
    "# Main\n",
    "# =========================================================\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data_root\", type=str, required=True)\n",
    "    ap.add_argument(\"--pretrained_path\", type=str, required=True)\n",
    "    ap.add_argument(\"--out_json\", type=str, default=\"cbam_results_3seeds.json\")\n",
    "    ap.add_argument(\"--out_dir\", type=str, default=\"./runs_cbam_reviewproof\")\n",
    "\n",
    "    ap.add_argument(\"--seeds\", type=int, nargs=\"+\", default=[0, 1, 2])\n",
    "    ap.add_argument(\"--epochs\", type=int, default=60)\n",
    "    ap.add_argument(\"--patience\", type=int, default=10)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    ap.add_argument(\"--num_workers\", type=int, default=2)\n",
    "\n",
    "    ap.add_argument(\"--use_weighted_sampler\", action=\"store_true\")\n",
    "\n",
    "    ap.add_argument(\"--label_smoothing\", type=float, default=0.0)\n",
    "    ap.add_argument(\"--weight_decay\", type=float, default=1e-4)\n",
    "\n",
    "    ap.add_argument(\"--ce_lr_backbone\", type=float, default=3e-5)\n",
    "    ap.add_argument(\"--ce_lr_head\", type=float, default=2e-4)\n",
    "    ap.add_argument(\"--ce_mixup_alpha\", type=float, default=0.0)\n",
    "\n",
    "    ap.add_argument(\"--hy_lr_backbone\", type=float, default=3e-5)\n",
    "    ap.add_argument(\"--hy_lr_head\", type=float, default=1e-4)\n",
    "    ap.add_argument(\"--hy_mixup_alpha\", type=float, default=0.05)\n",
    "    ap.add_argument(\"--supcon_weight\", type=float, default=0.04)\n",
    "    ap.add_argument(\"--temperature\", type=float, default=0.07)\n",
    "\n",
    "    ap.add_argument(\"--tta_candidates\", type=int, nargs=\"+\", default=[1, 2, 4],\n",
    "                    help=\"TTA candidates on VAL (subset of {1,2,4}).\")\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    allowed = {1, 2, 4}\n",
    "    cand = [int(x) for x in args.tta_candidates]\n",
    "    if any(x not in allowed for x in cand) or len(cand) == 0:\n",
    "        raise ValueError(\"--tta_candidates must be a non-empty subset of {1,2,4} (e.g., 1 2 4).\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    if not os.path.exists(args.pretrained_path):\n",
    "        raise FileNotFoundError(f\"Pretrained weights not found: {args.pretrained_path}\")\n",
    "\n",
    "    train_ds_ce, train_ds_hybrid, val_ds, test_ds, class_names, train_dir, val_dir, test_dir = build_datasets(args.data_root)\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(\"Split dirs:\", {\"train\": train_dir, \"val\": val_dir, \"test\": test_dir})\n",
    "\n",
    "    all_results = {\"config\": vars(args), \"class_names\": class_names, \"per_seed\": []}\n",
    "\n",
    "    for mode in [\"CE\", \"HYBRID\"]:\n",
    "        train_ds = train_ds_ce if mode == \"CE\" else train_ds_hybrid\n",
    "        for seed in args.seeds:\n",
    "            r = train_one_seed(\n",
    "                mode=mode,\n",
    "                seed=seed,\n",
    "                pretrained_path=args.pretrained_path,\n",
    "                train_ds=train_ds,\n",
    "                val_ds=val_ds,\n",
    "                test_ds=test_ds,\n",
    "                class_names=class_names,\n",
    "                val_dir=val_dir,\n",
    "                test_dir=test_dir,\n",
    "                device=device,\n",
    "                out_dir=args.out_dir,\n",
    "                epochs=args.epochs,\n",
    "                patience=args.patience,\n",
    "                batch_size=args.batch_size,\n",
    "                num_workers=args.num_workers,\n",
    "                use_weighted_sampler=args.use_weighted_sampler,\n",
    "                lr_backbone=(args.ce_lr_backbone if mode == \"CE\" else args.hy_lr_backbone),\n",
    "                lr_head=(args.ce_lr_head if mode == \"CE\" else args.hy_lr_head),\n",
    "                weight_decay=args.weight_decay,\n",
    "                label_smoothing=args.label_smoothing,\n",
    "                mixup_alpha=(args.ce_mixup_alpha if mode == \"CE\" else args.hy_mixup_alpha),\n",
    "                supcon_weight=args.supcon_weight,\n",
    "                temperature=args.temperature,\n",
    "                tta_candidates=cand\n",
    "            )\n",
    "            all_results[\"per_seed\"].append(r)\n",
    "\n",
    "    def collect(mode: str, key: str) -> List[float]:\n",
    "        return [x[key] for x in all_results[\"per_seed\"] if x[\"mode\"] == mode]\n",
    "\n",
    "    summary = {}\n",
    "    for mode in [\"CE\", \"HYBRID\"]:\n",
    "        s = {\n",
    "            \"test_acc_%\": asdict(summarize([v * 100.0 for v in collect(mode, \"test_acc\")])),\n",
    "            \"tta_acc_%\": asdict(summarize([v * 100.0 for v in collect(mode, \"tta_acc\")])),\n",
    "            \"macro_auc\": asdict(summarize(collect(mode, \"macro_auc\"))),\n",
    "            \"macro_auc_tta\": asdict(summarize(collect(mode, \"macro_auc_tta\"))),\n",
    "            \"params_m\": asdict(summarize(collect(mode, \"params_m\"))),\n",
    "        }\n",
    "        summary[mode] = s\n",
    "\n",
    "    all_results[\"summary\"] = summary\n",
    "\n",
    "    print(\"\\n================ SUMMARY (mean  std over seeds) ================\\n\")\n",
    "    for mode in [\"CE\", \"HYBRID\"]:\n",
    "        s = summary[mode]\n",
    "        print(f\"MODE: {mode}\")\n",
    "        print(f\"Test Acc (%): {s['test_acc_%']['mean']:.2f}  {s['test_acc_%']['std']:.2f}\")\n",
    "        print(f\"TTA  Acc (%): {s['tta_acc_%']['mean']:.2f}  {s['tta_acc_%']['std']:.2f}\")\n",
    "        print(f\"Macro ROC-AUC (single): {s['macro_auc']['mean']:.4f}  {s['macro_auc']['std']:.4f}\")\n",
    "        print(f\"Macro ROC-AUC (TTA):    {s['macro_auc_tta']['mean']:.4f}  {s['macro_auc_tta']['std']:.4f}\")\n",
    "        print(f\"# Params (M): {s['params_m']['mean']:.2f}  {s['params_m']['std']:.2f}\")\n",
    "        print()\n",
    "\n",
    "    with open(args.out_json, \"w\") as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"Saved per-seed metrics: {args.out_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T22:26:20.027377Z",
     "iopub.status.busy": "2025-12-05T22:26:20.026783Z",
     "iopub.status.idle": "2025-12-05T23:53:58.541862Z",
     "shell.execute_reply": "2025-12-05T23:53:58.541113Z",
     "shell.execute_reply.started": "2025-12-05T22:26:20.027354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
      "Split dirs: {'train': '/kaggle/input/minida/mini_output1/train', 'val': '/kaggle/input/minida/mini_output1/val', 'test': '/kaggle/input/minida/mini_output1/test'}\n",
      "[CE][seed=0] Epoch 01/60 | train loss 0.6994 acc 0.6850 || val loss 0.9797 acc 0.3232 macroF1 0.1803 (saved)\n",
      "[CE][seed=0] Epoch 02/60 | train loss 0.4927 acc 0.7970 || val loss 0.4994 acc 0.8687 macroF1 0.8637 (saved)\n",
      "[CE][seed=0] Epoch 03/60 | train loss 0.4752 acc 0.8055 || val loss 0.2702 acc 0.9091 macroF1 0.9106 (saved)\n",
      "[CE][seed=0] Epoch 04/60 | train loss 0.4743 acc 0.8055 || val loss 0.3512 acc 0.8687 macroF1 0.8637 \n",
      "[CE][seed=0] Epoch 05/60 | train loss 0.4388 acc 0.8161 || val loss 0.2270 acc 0.9192 macroF1 0.9210 (saved)\n",
      "[CE][seed=0] Epoch 06/60 | train loss 0.3820 acc 0.8224 || val loss 0.2605 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=0] Epoch 07/60 | train loss 0.3761 acc 0.8266 || val loss 0.2641 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=0] Epoch 08/60 | train loss 0.3510 acc 0.8457 || val loss 0.2400 acc 0.9293 macroF1 0.9306 (saved)\n",
      "[CE][seed=0] Epoch 09/60 | train loss 0.3767 acc 0.8266 || val loss 0.2233 acc 0.9394 macroF1 0.9393 (saved)\n",
      "[CE][seed=0] Epoch 10/60 | train loss 0.3403 acc 0.8351 || val loss 0.1844 acc 0.9293 macroF1 0.9306 \n",
      "[CE][seed=0] Epoch 11/60 | train loss 0.3700 acc 0.8414 || val loss 0.1948 acc 0.9293 macroF1 0.9310 \n",
      "[CE][seed=0] Epoch 12/60 | train loss 0.3686 acc 0.8288 || val loss 0.2617 acc 0.9293 macroF1 0.9288 \n",
      "[CE][seed=0] Epoch 13/60 | train loss 0.3244 acc 0.8562 || val loss 0.1773 acc 0.9293 macroF1 0.9310 \n",
      "[CE][seed=0] Epoch 14/60 | train loss 0.3301 acc 0.8541 || val loss 0.2233 acc 0.9495 macroF1 0.9501 (saved)\n",
      "[CE][seed=0] Epoch 15/60 | train loss 0.3258 acc 0.8584 || val loss 0.2317 acc 0.9293 macroF1 0.9286 \n",
      "[CE][seed=0] Epoch 16/60 | train loss 0.3358 acc 0.8668 || val loss 0.2513 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=0] Epoch 17/60 | train loss 0.3133 acc 0.8710 || val loss 0.1909 acc 0.9394 macroF1 0.9404 \n",
      "[CE][seed=0] Epoch 18/60 | train loss 0.2738 acc 0.8837 || val loss 0.1830 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=0] Epoch 19/60 | train loss 0.2896 acc 0.8816 || val loss 0.1640 acc 0.9596 macroF1 0.9599 (saved)\n",
      "[CE][seed=0] Epoch 20/60 | train loss 0.2751 acc 0.8901 || val loss 0.1717 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=0] Epoch 21/60 | train loss 0.2846 acc 0.8964 || val loss 0.1733 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=0] Epoch 22/60 | train loss 0.3128 acc 0.8647 || val loss 0.2533 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=0] Epoch 23/60 | train loss 0.2735 acc 0.8732 || val loss 0.1872 acc 0.9495 macroF1 0.9501 \n",
      "[CE][seed=0] Epoch 24/60 | train loss 0.3156 acc 0.8605 || val loss 0.2409 acc 0.9091 macroF1 0.9073 \n",
      "[CE][seed=0] Epoch 25/60 | train loss 0.2567 acc 0.8879 || val loss 0.2118 acc 0.9394 macroF1 0.9390 \n",
      "[CE][seed=0] Epoch 26/60 | train loss 0.3069 acc 0.8774 || val loss 0.2525 acc 0.8889 macroF1 0.8850 \n",
      "[CE][seed=0] Epoch 27/60 | train loss 0.2796 acc 0.8922 || val loss 0.2342 acc 0.9192 macroF1 0.9181 \n",
      "[CE][seed=0] Epoch 28/60 | train loss 0.2605 acc 0.8816 || val loss 0.2076 acc 0.9293 macroF1 0.9286 \n",
      "[CE][seed=0] Epoch 29/60 | train loss 0.2718 acc 0.8901 || val loss 0.1723 acc 0.9596 macroF1 0.9599 \n",
      "[CE][seed=0] Early stopping.\n",
      "[CE][seed=1] Epoch 01/60 | train loss 0.6456 acc 0.7019 || val loss 0.9958 acc 0.4141 macroF1 0.3056 (saved)\n",
      "[CE][seed=1] Epoch 02/60 | train loss 0.4817 acc 0.8140 || val loss 0.4826 acc 0.8586 macroF1 0.8560 (saved)\n",
      "[CE][seed=1] Epoch 03/60 | train loss 0.4877 acc 0.7759 || val loss 0.2909 acc 0.8990 macroF1 0.8975 (saved)\n",
      "[CE][seed=1] Epoch 04/60 | train loss 0.3892 acc 0.8414 || val loss 0.2418 acc 0.9192 macroF1 0.9210 (saved)\n",
      "[CE][seed=1] Epoch 05/60 | train loss 0.3716 acc 0.8330 || val loss 0.3197 acc 0.8687 macroF1 0.8618 \n",
      "[CE][seed=1] Epoch 06/60 | train loss 0.3924 acc 0.8203 || val loss 0.2139 acc 0.9394 macroF1 0.9404 (saved)\n",
      "[CE][seed=1] Epoch 07/60 | train loss 0.3789 acc 0.8245 || val loss 0.2204 acc 0.9091 macroF1 0.9108 \n",
      "[CE][seed=1] Epoch 08/60 | train loss 0.3588 acc 0.8499 || val loss 0.1976 acc 0.9495 macroF1 0.9501 (saved)\n",
      "[CE][seed=1] Epoch 09/60 | train loss 0.3599 acc 0.8436 || val loss 0.1905 acc 0.9495 macroF1 0.9505 \n",
      "[CE][seed=1] Epoch 10/60 | train loss 0.3557 acc 0.8414 || val loss 0.2721 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=1] Epoch 11/60 | train loss 0.3558 acc 0.8457 || val loss 0.3110 acc 0.8586 macroF1 0.8497 \n",
      "[CE][seed=1] Epoch 12/60 | train loss 0.3003 acc 0.8689 || val loss 0.1838 acc 0.9293 macroF1 0.9310 \n",
      "[CE][seed=1] Epoch 13/60 | train loss 0.3660 acc 0.8562 || val loss 0.1947 acc 0.9596 macroF1 0.9599 (saved)\n",
      "[CE][seed=1] Epoch 14/60 | train loss 0.3085 acc 0.8774 || val loss 0.2058 acc 0.9394 macroF1 0.9396 \n",
      "[CE][seed=1] Epoch 15/60 | train loss 0.3231 acc 0.8837 || val loss 0.1983 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=1] Epoch 16/60 | train loss 0.3842 acc 0.8288 || val loss 0.1770 acc 0.9495 macroF1 0.9501 \n",
      "[CE][seed=1] Epoch 17/60 | train loss 0.2677 acc 0.9070 || val loss 0.2082 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=1] Epoch 18/60 | train loss 0.3204 acc 0.8922 || val loss 0.1745 acc 0.9596 macroF1 0.9605 \n",
      "[CE][seed=1] Epoch 19/60 | train loss 0.2903 acc 0.8774 || val loss 0.2022 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=1] Epoch 20/60 | train loss 0.2968 acc 0.8732 || val loss 0.1660 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=1] Epoch 21/60 | train loss 0.2648 acc 0.8837 || val loss 0.1800 acc 0.9596 macroF1 0.9599 \n",
      "[CE][seed=1] Epoch 22/60 | train loss 0.2913 acc 0.8753 || val loss 0.1630 acc 0.9596 macroF1 0.9599 \n",
      "[CE][seed=1] Epoch 23/60 | train loss 0.2847 acc 0.8943 || val loss 0.1688 acc 0.9596 macroF1 0.9599 \n",
      "[CE][seed=1] Early stopping.\n",
      "[CE][seed=2] Epoch 01/60 | train loss 0.6732 acc 0.6956 || val loss 0.9911 acc 0.3131 macroF1 0.1615 (saved)\n",
      "[CE][seed=2] Epoch 02/60 | train loss 0.4834 acc 0.8288 || val loss 0.4698 acc 0.8485 macroF1 0.8423 (saved)\n",
      "[CE][seed=2] Epoch 03/60 | train loss 0.4786 acc 0.7822 || val loss 0.2833 acc 0.9394 macroF1 0.9399 (saved)\n",
      "[CE][seed=2] Epoch 04/60 | train loss 0.4030 acc 0.8351 || val loss 0.3573 acc 0.8586 macroF1 0.8497 \n",
      "[CE][seed=2] Epoch 05/60 | train loss 0.3881 acc 0.8372 || val loss 0.2758 acc 0.8788 macroF1 0.8752 \n",
      "[CE][seed=2] Epoch 06/60 | train loss 0.3597 acc 0.8478 || val loss 0.2143 acc 0.9394 macroF1 0.9404 \n",
      "[CE][seed=2] Epoch 07/60 | train loss 0.3535 acc 0.8372 || val loss 0.2034 acc 0.9394 macroF1 0.9399 \n",
      "[CE][seed=2] Epoch 08/60 | train loss 0.3521 acc 0.8499 || val loss 0.2659 acc 0.8990 macroF1 0.8962 \n",
      "[CE][seed=2] Epoch 09/60 | train loss 0.3429 acc 0.8520 || val loss 0.2100 acc 0.9596 macroF1 0.9599 (saved)\n",
      "[CE][seed=2] Epoch 10/60 | train loss 0.3119 acc 0.8710 || val loss 0.1856 acc 0.9495 macroF1 0.9501 \n",
      "[CE][seed=2] Epoch 11/60 | train loss 0.3700 acc 0.8414 || val loss 0.2166 acc 0.9192 macroF1 0.9181 \n",
      "[CE][seed=2] Epoch 12/60 | train loss 0.3083 acc 0.8541 || val loss 0.2076 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=2] Epoch 13/60 | train loss 0.3394 acc 0.8436 || val loss 0.2273 acc 0.9394 macroF1 0.9393 \n",
      "[CE][seed=2] Epoch 14/60 | train loss 0.3115 acc 0.8795 || val loss 0.1968 acc 0.9495 macroF1 0.9501 \n",
      "[CE][seed=2] Epoch 15/60 | train loss 0.2900 acc 0.8605 || val loss 0.1893 acc 0.9495 macroF1 0.9492 \n",
      "[CE][seed=2] Epoch 16/60 | train loss 0.3173 acc 0.8710 || val loss 0.2023 acc 0.9495 macroF1 0.9497 \n",
      "[CE][seed=2] Epoch 17/60 | train loss 0.2935 acc 0.8710 || val loss 0.1715 acc 0.9192 macroF1 0.9205 \n",
      "[CE][seed=2] Epoch 18/60 | train loss 0.3043 acc 0.8647 || val loss 0.2315 acc 0.9293 macroF1 0.9288 \n",
      "[CE][seed=2] Epoch 19/60 | train loss 0.2945 acc 0.8774 || val loss 0.1790 acc 0.9293 macroF1 0.9306 \n",
      "[CE][seed=2] Early stopping.\n",
      "[HYBRID][seed=0] Epoch 01/60 | train loss 0.9506 acc 0.6408 || val loss 0.7732 acc 0.8889 macroF1 0.8907 (saved)\n",
      "[HYBRID][seed=0] Epoch 02/60 | train loss 0.7280 acc 0.7773 || val loss 0.4645 acc 0.8990 macroF1 0.9009 (saved)\n",
      "[HYBRID][seed=0] Epoch 03/60 | train loss 0.6468 acc 0.8117 || val loss 0.2922 acc 0.8788 macroF1 0.8810 \n",
      "[HYBRID][seed=0] Epoch 04/60 | train loss 0.6240 acc 0.7974 || val loss 0.3344 acc 0.8485 macroF1 0.8400 \n",
      "[HYBRID][seed=0] Epoch 05/60 | train loss 0.6380 acc 0.8025 || val loss 0.2767 acc 0.9293 macroF1 0.9288 (saved)\n",
      "[HYBRID][seed=0] Epoch 06/60 | train loss 0.5993 acc 0.8280 || val loss 0.2646 acc 0.9293 macroF1 0.9306 \n",
      "[HYBRID][seed=0] Epoch 07/60 | train loss 0.5587 acc 0.8256 || val loss 0.2129 acc 0.9495 macroF1 0.9497 (saved)\n",
      "[HYBRID][seed=0] Epoch 08/60 | train loss 0.5851 acc 0.8054 || val loss 0.2623 acc 0.9293 macroF1 0.9288 \n",
      "[HYBRID][seed=0] Epoch 09/60 | train loss 0.6218 acc 0.8067 || val loss 0.2046 acc 0.9394 macroF1 0.9404 \n",
      "[HYBRID][seed=0] Epoch 10/60 | train loss 0.5652 acc 0.8503 || val loss 0.2200 acc 0.8990 macroF1 0.9013 \n",
      "[HYBRID][seed=0] Epoch 11/60 | train loss 0.5859 acc 0.8117 || val loss 0.2104 acc 0.9495 macroF1 0.9497 \n",
      "[HYBRID][seed=0] Epoch 12/60 | train loss 0.5934 acc 0.8134 || val loss 0.2401 acc 0.9394 macroF1 0.9390 \n",
      "[HYBRID][seed=0] Epoch 13/60 | train loss 0.4875 acc 0.8661 || val loss 0.2179 acc 0.9394 macroF1 0.9393 \n",
      "[HYBRID][seed=0] Epoch 14/60 | train loss 0.5612 acc 0.8340 || val loss 0.2076 acc 0.9394 macroF1 0.9404 \n",
      "[HYBRID][seed=0] Epoch 15/60 | train loss 0.5047 acc 0.8498 || val loss 0.2118 acc 0.9495 macroF1 0.9497 \n",
      "[HYBRID][seed=0] Epoch 16/60 | train loss 0.5010 acc 0.8461 || val loss 0.2223 acc 0.9394 macroF1 0.9393 \n",
      "[HYBRID][seed=0] Epoch 17/60 | train loss 0.4910 acc 0.8577 || val loss 0.2077 acc 0.9495 macroF1 0.9497 \n",
      "[HYBRID][seed=0] Early stopping.\n",
      "[HYBRID][seed=1] Epoch 01/60 | train loss 0.9262 acc 0.6659 || val loss 0.7657 acc 0.8990 macroF1 0.9009 (saved)\n",
      "[HYBRID][seed=1] Epoch 02/60 | train loss 0.7008 acc 0.7487 || val loss 0.4794 acc 0.8990 macroF1 0.9013 \n",
      "[HYBRID][seed=1] Epoch 03/60 | train loss 0.6650 acc 0.7904 || val loss 0.3106 acc 0.8990 macroF1 0.8993 \n",
      "[HYBRID][seed=1] Epoch 04/60 | train loss 0.5936 acc 0.8190 || val loss 0.2698 acc 0.9293 macroF1 0.9295 (saved)\n",
      "[HYBRID][seed=1] Epoch 05/60 | train loss 0.5482 acc 0.8311 || val loss 0.2626 acc 0.9394 macroF1 0.9393 (saved)\n",
      "[HYBRID][seed=1] Epoch 06/60 | train loss 0.5670 acc 0.8238 || val loss 0.2150 acc 0.9293 macroF1 0.9305 \n",
      "[HYBRID][seed=1] Epoch 07/60 | train loss 0.6536 acc 0.7963 || val loss 0.2175 acc 0.9495 macroF1 0.9501 (saved)\n",
      "[HYBRID][seed=1] Epoch 08/60 | train loss 0.5211 acc 0.8565 || val loss 0.2266 acc 0.9394 macroF1 0.9404 \n",
      "[HYBRID][seed=1] Epoch 09/60 | train loss 0.5998 acc 0.8361 || val loss 0.2232 acc 0.9394 macroF1 0.9404 \n",
      "[HYBRID][seed=1] Epoch 10/60 | train loss 0.5686 acc 0.8435 || val loss 0.2365 acc 0.9394 macroF1 0.9396 \n",
      "[HYBRID][seed=1] Epoch 11/60 | train loss 0.6120 acc 0.8207 || val loss 0.2301 acc 0.9394 macroF1 0.9393 \n",
      "[HYBRID][seed=1] Epoch 12/60 | train loss 0.6437 acc 0.8078 || val loss 0.3478 acc 0.8586 macroF1 0.8497 \n",
      "[HYBRID][seed=1] Epoch 13/60 | train loss 0.5088 acc 0.8616 || val loss 0.2087 acc 0.9394 macroF1 0.9396 \n",
      "[HYBRID][seed=1] Epoch 14/60 | train loss 0.5695 acc 0.8312 || val loss 0.1878 acc 0.9394 macroF1 0.9404 \n",
      "[HYBRID][seed=1] Epoch 15/60 | train loss 0.6362 acc 0.7991 || val loss 0.2586 acc 0.9293 macroF1 0.9288 \n",
      "[HYBRID][seed=1] Epoch 16/60 | train loss 0.5344 acc 0.8554 || val loss 0.2493 acc 0.9293 macroF1 0.9286 \n",
      "[HYBRID][seed=1] Epoch 17/60 | train loss 0.5000 acc 0.8473 || val loss 0.2050 acc 0.9394 macroF1 0.9399 \n",
      "[HYBRID][seed=1] Early stopping.\n",
      "[HYBRID][seed=2] Epoch 01/60 | train loss 0.8361 acc 0.6969 || val loss 0.7813 acc 0.8384 macroF1 0.8415 (saved)\n",
      "[HYBRID][seed=2] Epoch 02/60 | train loss 0.6596 acc 0.7880 || val loss 0.4630 acc 0.8889 macroF1 0.8864 (saved)\n",
      "[HYBRID][seed=2] Epoch 03/60 | train loss 0.6251 acc 0.8096 || val loss 0.2892 acc 0.9192 macroF1 0.9206 (saved)\n",
      "[HYBRID][seed=2] Epoch 04/60 | train loss 0.6993 acc 0.7564 || val loss 0.2458 acc 0.9596 macroF1 0.9599 (saved)\n",
      "[HYBRID][seed=2] Epoch 05/60 | train loss 0.5707 acc 0.8268 || val loss 0.2283 acc 0.9192 macroF1 0.9210 \n",
      "[HYBRID][seed=2] Epoch 06/60 | train loss 0.6410 acc 0.8143 || val loss 0.2903 acc 0.8990 macroF1 0.8962 \n",
      "[HYBRID][seed=2] Epoch 07/60 | train loss 0.6131 acc 0.8021 || val loss 0.2589 acc 0.9394 macroF1 0.9403 \n",
      "[HYBRID][seed=2] Epoch 08/60 | train loss 0.5491 acc 0.8271 || val loss 0.2705 acc 0.8990 macroF1 0.8962 \n",
      "[HYBRID][seed=2] Epoch 09/60 | train loss 0.5278 acc 0.8338 || val loss 0.2127 acc 0.9394 macroF1 0.9401 \n",
      "[HYBRID][seed=2] Epoch 10/60 | train loss 0.6075 acc 0.8352 || val loss 0.2650 acc 0.9192 macroF1 0.9181 \n",
      "[HYBRID][seed=2] Epoch 11/60 | train loss 0.5020 acc 0.8588 || val loss 0.2290 acc 0.9192 macroF1 0.9215 \n",
      "[HYBRID][seed=2] Epoch 12/60 | train loss 0.5311 acc 0.8410 || val loss 0.2196 acc 0.9596 macroF1 0.9599 \n",
      "[HYBRID][seed=2] Epoch 13/60 | train loss 0.5011 acc 0.8668 || val loss 0.2271 acc 0.9394 macroF1 0.9393 \n",
      "[HYBRID][seed=2] Epoch 14/60 | train loss 0.5092 acc 0.8508 || val loss 0.1769 acc 0.9596 macroF1 0.9599 \n",
      "[HYBRID][seed=2] Early stopping.\n",
      "\n",
      "================ SUMMARY (mean  std over seeds) ================\n",
      "\n",
      "MODE: CE\n",
      "Test Acc (%): 97.31  1.17\n",
      "TTA  Acc (%): 97.31  1.17\n",
      "Macro ROC-AUC (single): 0.9983  0.0003\n",
      "Macro ROC-AUC (TTA):    0.9983  0.0003\n",
      "# Params (M): 27.08  0.00\n",
      "\n",
      "MODE: HYBRID\n",
      "Test Acc (%): 95.29  0.58\n",
      "TTA  Acc (%): 95.29  0.58\n",
      "Macro ROC-AUC (single): 0.9967  0.0006\n",
      "Macro ROC-AUC (TTA):    0.9967  0.0006\n",
      "# Params (M): 27.34  0.00\n",
      "\n",
      "Saved per-seed metrics: cbam_results_3seeds.json\n"
     ]
    }
   ],
   "source": [
    "!python finetune_cbam_eval.py \\\n",
    "  --data_root /kaggle/input/minida/mini_output1 \\\n",
    "  --pretrained_path /kaggle/working/simsiam_cbam/simsiam_cbam_pretrained_final.pth \\\n",
    "  --seeds 0 1 2 \\\n",
    "  --epochs 60 --patience 10 --batch_size 32 --num_workers 2 \\\n",
    "  --label_smoothing 0.0 --weight_decay 1e-4 \\\n",
    "  --ce_lr_backbone 3e-5 --ce_lr_head 2e-4 --ce_mixup_alpha 0.0 \\\n",
    "  --hy_lr_backbone 3e-5 --hy_lr_head 1e-4 --hy_mixup_alpha 0.05 \\\n",
    "  --supcon_weight 0.04 --temperature 0.07 \\\n",
    "  --tta_candidates 1 2 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T01:46:05.448701Z",
     "iopub.status.busy": "2025-12-06T01:46:05.447973Z",
     "iopub.status.idle": "2025-12-06T01:46:33.171483Z",
     "shell.execute_reply": "2025-12-06T01:46:33.170571Z",
     "shell.execute_reply.started": "2025-12-06T01:46:05.448677Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked CE checkpoint: ./runs_cbam_reviewproof/best_ce_seed0.pth | seed: 0 | best_val_acc: 0.9595959595959596\n",
      "Picked HYBRID checkpoint: ./runs_cbam_reviewproof/best_hybrid_seed2.pth | seed: 2 | best_val_acc: 0.9595959595959596\n",
      "Saved: /kaggle/working/CBAM_SSM_CM_CE.png\n",
      "Saved: /kaggle/working/CBAM_SSM_ROC_CE.png\n",
      "Saved: /kaggle/working/CBAM_SSM_CM_HYBRID.png\n",
      "Saved: /kaggle/working/CBAM_SSM_ROC_HYBRID.png\n"
     ]
    }
   ],
   "source": [
    "# --- Single cell: generate Confusion Matrix + ROC (CBAM CE & HYBRID) from saved checkpoints ---\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS (edit if needed)\n",
    "# -----------------------------\n",
    "DATA_ROOT = \"/kaggle/input/minida/mini_output1\"          # has train/val/test folders\n",
    "RESULTS_JSON = \"/kaggle/working/cbam_results_3seeds.json\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# CBAM-ResNet50 definition (MUST match finetune_cbam_eval.py)\n",
    "# -----------------------------\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(in_planes // ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    \"\"\"Bottleneck + CBAM after bn3 and before residual add.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        c_out = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(c_out)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def cbam_resnet50(**kwargs) -> ResNet:\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    # zero-init last BN (stability)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, Bottleneck) and hasattr(m, \"bn3\") and m.bn3.weight is not None:\n",
    "            nn.init.constant_(m.bn3.weight, 0)\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model definition (must match finetune_cbam_eval.py)\n",
    "# -----------------------------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, num_classes: int, hybrid: bool):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # to avgpool\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.30),\n",
    "            nn.Linear(512, num_classes, bias=True),\n",
    "        )\n",
    "\n",
    "        self.hybrid = bool(hybrid)\n",
    "        self.supcon_proj = nn.Linear(2048, 128, bias=True) if self.hybrid else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x).flatten(1)\n",
    "        return self.classifier(feats)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Data\n",
    "# -----------------------------\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "test_dir = os.path.join(DATA_ROOT, \"test\")\n",
    "test_ds = ImageFolder(test_dir, transform=eval_tf)\n",
    "class_names = test_ds.classes\n",
    "n_classes = len(class_names)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available(),\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: pick best checkpoint per mode from cbam_results_3seeds.json\n",
    "# -----------------------------\n",
    "with open(RESULTS_JSON, \"r\") as f:\n",
    "    R = json.load(f)\n",
    "\n",
    "def pick_best(mode: str):\n",
    "    candidates = [x for x in R[\"per_seed\"] if x[\"mode\"].upper() == mode.upper()]\n",
    "    if len(candidates) == 0:\n",
    "        raise RuntimeError(f\"No entries found for mode={mode} in {RESULTS_JSON}\")\n",
    "    # best is highest best_val_acc; tie-breaker: higher test_acc\n",
    "    candidates.sort(key=lambda x: (x.get(\"best_val_acc\", -1), x.get(\"test_acc\", -1)), reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "best_ce = pick_best(\"CE\")\n",
    "best_hy = pick_best(\"HYBRID\")\n",
    "\n",
    "print(\"Picked CE checkpoint:\", best_ce[\"checkpoint_path\"], \"| seed:\", best_ce[\"seed\"], \"| best_val_acc:\", best_ce[\"best_val_acc\"])\n",
    "print(\"Picked HYBRID checkpoint:\", best_hy[\"checkpoint_path\"], \"| seed:\", best_hy[\"seed\"], \"| best_val_acc:\", best_hy[\"best_val_acc\"])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inference: get y_true, probs, preds\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def infer(checkpoint_path: str, hybrid: bool):\n",
    "    model = FineTuneCBAM(num_classes=n_classes, hybrid=hybrid).to(DEVICE)\n",
    "    sd = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    probs = []\n",
    "    preds = []\n",
    "\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        pr = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        pd = np.argmax(pr, axis=1)\n",
    "\n",
    "        probs.append(pr)\n",
    "        preds.append(pd)\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    probs = np.concatenate(probs)\n",
    "    preds = np.concatenate(preds)\n",
    "    return y_true, probs, preds\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plot: Confusion Matrix\n",
    "# -----------------------------\n",
    "def plot_confusion(y_true, y_pred, title, out_path):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig = plt.figure(figsize=(6.2, 5.4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    fig.colorbar(im)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xticks(np.arange(n_classes))\n",
    "    ax.set_yticks(np.arange(n_classes))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, format(cm[i, j], \"d\"),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=11\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plot: ROC curves (per-class + micro + macro)\n",
    "# -----------------------------\n",
    "def plot_roc(y_true, probs, title, out_path):\n",
    "    y_bin = label_binarize(y_true, classes=list(range(n_classes)))  # (N,C)\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    fig = plt.figure(figsize=(6.2, 5.4))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(fpr[\"micro\"], tpr[\"micro\"], linewidth=2.5, label=f\"micro-average (AUC = {roc_auc['micro']:.4f})\")\n",
    "    ax.plot(fpr[\"macro\"], tpr[\"macro\"], linewidth=2.5, label=f\"macro-average (AUC = {roc_auc['macro']:.4f})\")\n",
    "\n",
    "    for i, name in enumerate(class_names):\n",
    "        ax.plot(fpr[i], tpr[i], linewidth=1.8, label=f\"{name} (AUC = {roc_auc[i]:.4f})\")\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.5)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\", fontsize=9)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Generate for CE\n",
    "# -----------------------------\n",
    "y_ce, pr_ce, pd_ce = infer(best_ce[\"checkpoint_path\"], hybrid=False)\n",
    "plot_confusion(\n",
    "    y_ce, pd_ce,\n",
    "    \"Confusion Matrix (SimSiam-CBAM-ResNet-50, CE)\",\n",
    "    os.path.join(OUT_DIR, \"CBAM_SSM_CM_CE.png\")\n",
    ")\n",
    "plot_roc(\n",
    "    y_ce, pr_ce,\n",
    "    \"ROC Curve (SimSiam-CBAM-ResNet-50, CE)\",\n",
    "    os.path.join(OUT_DIR, \"CBAM_SSM_ROC_CE.png\")\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Generate for HYBRID\n",
    "# -----------------------------\n",
    "y_hy, pr_hy, pd_hy = infer(best_hy[\"checkpoint_path\"], hybrid=True)\n",
    "plot_confusion(\n",
    "    y_hy, pd_hy,\n",
    "    \"Confusion Matrix (SimSiam-CBAM-ResNet-50, Hybrid)\",\n",
    "    os.path.join(OUT_DIR, \"CBAM_SSM_CM_HYBRID.png\")\n",
    ")\n",
    "plot_roc(\n",
    "    y_hy, pr_hy,\n",
    "    \"ROC Curve (SimSiam-CBAM-ResNet-50, Hybrid)\",\n",
    "    os.path.join(OUT_DIR, \"CBAM_SSM_ROC_HYBRID.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T03:44:36.527200Z",
     "iopub.status.busy": "2025-12-06T03:44:36.526638Z",
     "iopub.status.idle": "2025-12-06T03:44:36.536543Z",
     "shell.execute_reply": "2025-12-06T03:44:36.535713Z",
     "shell.execute_reply.started": "2025-12-06T03:44:36.527171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cbam_temp_scaling_eval_from_ablation_json.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cbam_temp_scaling_eval_from_ablation_json.py\n",
    "import os, json, argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"adaptive_max_pool2d_backward_cuda does not have a deterministic implementation*\",\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# -----------------------------\n",
    "# CBAM-ResNet50 (same as your finetune_cbam_eval.py)\n",
    "# -----------------------------\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(in_planes // ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        c_out = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(c_out)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)   # <-- FIXED HERE\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def cbam_resnet50(**kwargs) -> ResNet:\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, Bottleneck) and hasattr(m, \"bn3\") and m.bn3.weight is not None:\n",
    "            nn.init.constant_(m.bn3.weight, 0)\n",
    "    return model\n",
    "\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512, bias=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.30),\n",
    "            nn.Linear(512, num_classes, bias=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x).flatten(1)\n",
    "        return self.classifier(feats)\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics / helpers\n",
    "# -----------------------------\n",
    "def macro_roc_auc(y_true: np.ndarray, probs: np.ndarray, n_classes: int) -> float:\n",
    "    onehot = np.eye(n_classes)[y_true]\n",
    "    return float(roc_auc_score(onehot, probs, average=\"macro\", multi_class=\"ovr\"))\n",
    "\n",
    "def ece_from_probs(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
    "    conf = probs.max(axis=1)\n",
    "    pred = probs.argmax(axis=1)\n",
    "    acc = (pred == y_true).astype(np.float64)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    N = len(y_true)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        mask = (conf > lo) & (conf <= hi) if i > 0 else (conf >= lo) & (conf <= hi)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        bin_acc = acc[mask].mean()\n",
    "        bin_conf = conf[mask].mean()\n",
    "        ece += (mask.sum() / N) * abs(bin_acc - bin_conf)\n",
    "    return float(ece)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_y = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        all_logits.append(model(x).detach().cpu())\n",
    "        all_y.append(y.detach().cpu())\n",
    "    return torch.cat(all_logits, dim=0), torch.cat(all_y, dim=0)\n",
    "\n",
    "def fit_temperature(val_logits: torch.Tensor, val_y: torch.Tensor, device, max_iter: int = 200) -> float:\n",
    "    val_logits = val_logits.to(device)\n",
    "    val_y = val_y.to(device)\n",
    "\n",
    "    log_T = torch.zeros((), device=device, requires_grad=True)\n",
    "    optimizer = torch.optim.LBFGS([log_T], lr=0.5, max_iter=max_iter, line_search_fn=\"strong_wolfe\")\n",
    "    nll = nn.CrossEntropyLoss()\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        T = torch.exp(log_T).clamp_min(1e-3)\n",
    "        loss = nll(val_logits / T, val_y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return float(torch.exp(log_T).clamp_min(1e-3).detach().cpu().item())\n",
    "\n",
    "# -----------------------------\n",
    "# Extract checkpoints from your ablation JSON (rows[])\n",
    "# -----------------------------\n",
    "def extract_from_rows(ab_json: dict):\n",
    "    rows = ab_json.get(\"rows\", None)\n",
    "    if not isinstance(rows, list) or len(rows) == 0:\n",
    "        raise KeyError(\"JSON must contain a non-empty list at key 'rows'.\")\n",
    "\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        if not isinstance(r, dict):\n",
    "            continue\n",
    "        pretty = r.get(\"pretty_name\") or r.get(\"noise_condition\")\n",
    "        ckpt = r.get(\"checkpoint\") or r.get(\"checkpoint_path\") or r.get(\"ckpt\")\n",
    "        if not pretty or not ckpt:\n",
    "            continue\n",
    "        out.append((pretty, ckpt, r))\n",
    "    if not out:\n",
    "        raise KeyError(\"No usable entries found in rows[]. Expected each row to have 'pretty_name' and 'checkpoint'.\")\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data_root\", type=str, required=True)\n",
    "    ap.add_argument(\"--ablation_json\", type=str, required=True)\n",
    "    ap.add_argument(\"--out_json\", type=str, default=\"cbam_noise_bestseed_temp_scaled.json\")\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    ap.add_argument(\"--num_workers\", type=int, default=2)\n",
    "    ap.add_argument(\"--ece_bins\", type=int, default=15)\n",
    "    ap.add_argument(\"--max_iter\", type=int, default=200)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "\n",
    "    val_ds = ImageFolder(os.path.join(args.data_root, \"val\"), transform=eval_tf)\n",
    "    test_ds = ImageFolder(os.path.join(args.data_root, \"test\"), transform=eval_tf)\n",
    "    assert val_ds.classes == test_ds.classes, \"val/test class order mismatch\"\n",
    "    class_names = val_ds.classes\n",
    "    n_classes = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                            num_workers=args.num_workers, pin_memory=torch.cuda.is_available(),\n",
    "                            persistent_workers=(args.num_workers > 0))\n",
    "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False,\n",
    "                             num_workers=args.num_workers, pin_memory=torch.cuda.is_available(),\n",
    "                             persistent_workers=(args.num_workers > 0))\n",
    "\n",
    "    with open(args.ablation_json, \"r\") as f:\n",
    "        ab = json.load(f)\n",
    "\n",
    "    entries = extract_from_rows(ab)\n",
    "\n",
    "    out = {\n",
    "        \"source_ablation_json\": args.ablation_json,\n",
    "        \"class_names\": class_names,\n",
    "        \"ece_bins\": int(args.ece_bins),\n",
    "        \"rows\": []\n",
    "    }\n",
    "\n",
    "    for pretty_name, ckpt_path, original_row in entries:\n",
    "        if not os.path.exists(ckpt_path):\n",
    "            raise FileNotFoundError(f\"{pretty_name}: checkpoint not found: {ckpt_path}\")\n",
    "\n",
    "        model = FineTuneCBAM(num_classes=n_classes).to(device)\n",
    "        sd = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(sd, strict=True)\n",
    "\n",
    "        # fit T on VAL\n",
    "        val_logits, val_y = collect_logits(model, val_loader, device)\n",
    "        T = fit_temperature(val_logits, val_y, device=device, max_iter=args.max_iter)\n",
    "\n",
    "        # evaluate on TEST with temp-scaled probs\n",
    "        test_logits, test_y = collect_logits(model, test_loader, device)\n",
    "        probs = F.softmax((test_logits.to(device) / T), dim=1).detach().cpu().numpy()\n",
    "        y_true = test_y.numpy()\n",
    "        y_pred = probs.argmax(axis=1)\n",
    "\n",
    "        acc = float(accuracy_score(y_true, y_pred))\n",
    "        auc = float(macro_roc_auc(y_true, probs, n_classes))\n",
    "        ece = float(ece_from_probs(probs, y_true, n_bins=args.ece_bins))\n",
    "\n",
    "        row_out = dict(original_row)\n",
    "        row_out.update({\n",
    "            \"temperature_T\": float(T),\n",
    "            \"temp_scaled_test_acc_%\": float(acc * 100.0),\n",
    "            \"temp_scaled_macro_roc_auc\": float(auc),\n",
    "            \"ece_temp_scaled_%\": float(ece * 100.0),\n",
    "        })\n",
    "        out[\"rows\"].append(row_out)\n",
    "\n",
    "        print(f\"{pretty_name:<22} | T={T:.3f} | \"\n",
    "              f\"Acc={acc*100:.2f}% | AUC={auc:.4f} | ECE(TS)={ece*100:.2f}%\")\n",
    "\n",
    "    with open(args.out_json, \"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(\"Saved:\", args.out_json)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T03:44:53.837657Z",
     "iopub.status.busy": "2025-12-06T03:44:53.836799Z",
     "iopub.status.idle": "2025-12-06T03:46:01.229912Z",
     "shell.execute_reply": "2025-12-06T03:46:01.228970Z",
     "shell.execute_reply.started": "2025-12-06T03:44:53.837630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Classes: ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
      "No Noise               | T=0.702 | Acc=97.98% | AUC=0.9986 | ECE(TS)=5.02%\n",
      "Gaussian Only          | T=0.628 | Acc=95.96% | AUC=0.9948 | ECE(TS)=3.31%\n",
      "Salt-and-Pepper Only   | T=0.714 | Acc=95.96% | AUC=0.9977 | ECE(TS)=5.97%\n",
      "Both Noises            | T=0.382 | Acc=93.94% | AUC=0.9920 | ECE(TS)=6.36%\n",
      "Saved: /kaggle/working/cbam_noise_bestseed_temp_scaled.json\n"
     ]
    }
   ],
   "source": [
    "!python cbam_temp_scaling_eval_from_ablation_json.py \\\n",
    "  --data_root /kaggle/input/minida/mini_output1 \\\n",
    "  --ablation_json /kaggle/working/cbam_noise_bestseed_results.json \\\n",
    "  --out_json /kaggle/working/cbam_noise_bestseed_temp_scaled.json \\\n",
    "  --batch_size 64 --num_workers 2 --ece_bins 15"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
