{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBAM SIMSIAM RESNET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T09:05:06.299986Z",
     "iopub.status.busy": "2025-08-28T09:05:06.299676Z",
     "iopub.status.idle": "2025-08-28T09:05:06.307343Z",
     "shell.execute_reply": "2025-08-28T09:05:06.306712Z",
     "shell.execute_reply.started": "2025-08-28T09:05:06.299960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cbam_resnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cbam_resnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Callable\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes: int, ratio: int = 16):\n",
    "        super().__init__()\n",
    "        hidden = max(in_planes // ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, hidden, kernel_size=1, bias=False)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(hidden, in_planes, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 7):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(x_cat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet Bottleneck with CBAM after the third BN and before the residual add.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        planes_out = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(planes_out)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out); out = self.relu(out)\n",
    "        out = self.conv3(out); out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def cbam_resnet50(*, norm_layer: Optional[Callable] = None, **kwargs) -> ResNet:\n",
    "    \"\"\"\n",
    "    Return a ResNet-50 that uses CBAMBottleneck blocks.\n",
    "    kwargs forwarded to torchvision.models.resnet.ResNet (e.g., num_classes).\n",
    "    \"\"\"\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], norm_layer=norm_layer, **kwargs)\n",
    "    # Zero-init last BN in each residual branch (improves training stability).\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, Bottleneck):\n",
    "            nn.init.constant_(m.bn3.weight, 0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T15:20:50.403029Z",
     "iopub.status.busy": "2025-08-28T15:20:50.402707Z",
     "iopub.status.idle": "2025-08-28T15:20:50.414832Z",
     "shell.execute_reply": "2025-08-28T15:20:50.413964Z",
     "shell.execute_reply.started": "2025-08-28T15:20:50.402997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simsiam_cbam_pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simsiam_cbam_pretrain.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "def seed_all(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optim helper (param-wise WD)\n",
    "# ----------------------------\n",
    "def exclude_from_wd(named_params: Iterable[Tuple[str, torch.nn.Parameter]], wd: float = 1e-4):\n",
    "    \"\"\"Create two param groups: with and without weight decay (no wd for BN/bias/1D).\"\"\"\n",
    "    wd_params, no_wd_params = [], []\n",
    "    for n, p in named_params:\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim == 1 or n.endswith(\".bias\") or \"bn\" in n.lower():\n",
    "            no_wd_params.append(p)\n",
    "        else:\n",
    "            wd_params.append(p)\n",
    "    return [{\"params\": wd_params, \"weight_decay\": wd},\n",
    "            {\"params\": no_wd_params, \"weight_decay\": 0.0}]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# SimSiam heads\n",
    "# ----------------------------\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=2048, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim, bias=False),\n",
    "            nn.BatchNorm1d(out_dim, affine=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    \"\"\"\n",
    "    SimSiam with CBAM-ResNet50 backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, fix_backbone_bn: bool = True):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        # children: conv1, bn1, relu, maxpool, layer1..4, avgpool, fc\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # up to avgpool\n",
    "        self.projector = MLPHead(2048)\n",
    "        self.predictor = PredictionHead()\n",
    "        self.fix_backbone_bn = fix_backbone_bn\n",
    "\n",
    "        # Freeze BN params and set them eval; keep handles to re-freeze in .train()\n",
    "        self._frozen_bn_modules = []\n",
    "        if self.fix_backbone_bn:\n",
    "            for m in self.backbone.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.requires_grad_(False)\n",
    "                    self._frozen_bn_modules.append(m)\n",
    "\n",
    "    def _forward_backbone(self, x):\n",
    "        x = self.backbone(x)           # (B, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)        # (B, 2048)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.projector(self._forward_backbone(x1))\n",
    "        z2 = self.projector(self._forward_backbone(x2))\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        # stop-grad on targets\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super().train(mode)\n",
    "        if self.fix_backbone_bn:\n",
    "            for m in self._frozen_bn_modules:\n",
    "                m.eval()  # keep BN frozen regardless of mode\n",
    "        return self\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset (recursive)\n",
    "# ----------------------------\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\")\n",
    "        filepaths = []\n",
    "        for dp, _, fns in os.walk(root_dir):\n",
    "            for fn in fns:\n",
    "                if fn.lower().endswith(exts):\n",
    "                    filepaths.append(os.path.join(dp, fn))\n",
    "        self.filepaths = sorted(filepaths)\n",
    "        if len(self.filepaths) == 0:\n",
    "            raise RuntimeError(f\"No images found under {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img = Image.open(self.filepaths[idx]).convert(\"RGB\")\n",
    "        if self.transform is None:\n",
    "            raise RuntimeError(\"Transform must be provided for SimSiam pretraining.\")\n",
    "        v1 = self.transform(img)\n",
    "        v2 = self.transform(img)\n",
    "        return v1, v2\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Loss\n",
    "# ----------------------------\n",
    "def negative_cosine_similarity(p, z):\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training\n",
    "# ----------------------------\n",
    "def pretrain(\n",
    "    root_path: str = \"/kaggle/input/minida/mini_output1/pretrain\",\n",
    "    checkpoint_dir: str = \"/kaggle/working/simsiam_cbam\",\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = 64,\n",
    "    num_workers: int = 2,\n",
    "    accumulation_steps: int = 1,\n",
    "    fix_backbone_bn: bool = True,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    seed_all(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"logs_cbam\"))\n",
    "\n",
    "    # SimSiam-style strong augs (GaussianBlur is important)\n",
    "    crop_size = 224\n",
    "    blur_kernel = max(int(0.1 * crop_size) // 2 * 2 + 1, 3)  # odd, >=3\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(crop_size, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=blur_kernel, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    dataset = UnlabeledDataset(root_dir=root_path, transform=train_transform)\n",
    "    pin = torch.cuda.is_available()\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    model = SimSiam(fix_backbone_bn=fix_backbone_bn).to(device)\n",
    "\n",
    "    # --- safe torch.compile (skip on older GPUs like P100 CC 6.0) ---\n",
    "    use_compile = False\n",
    "    if hasattr(torch, \"compile\") and torch.cuda.is_available():\n",
    "        try:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 7:\n",
    "                use_compile = True\n",
    "            else:\n",
    "                print(f\"Skipping torch.compile: compute capability {major}.x < 7.0\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not query device capability, skipping compile: {e}\")\n",
    "\n",
    "    if use_compile:\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            print(f\"torch.compile failed, falling back to eager: {e}\")\n",
    "\n",
    "    # Linear LR scaling with batch size (clamped)\n",
    "    global_bs = batch_size  # adjust if using DDP\n",
    "    base_lr = 0.05 * max(min(global_bs, 1024), 64) / 256.0\n",
    "\n",
    "    # Param-wise weight decay\n",
    "    param_groups = exclude_from_wd(model.named_parameters(), wd=1e-4)\n",
    "    optimizer = torch.optim.SGD(param_groups, lr=base_lr, momentum=0.9)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # New AMP API\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    ckpt_path = os.path.join(checkpoint_dir, \"simsiam_cbam_checkpoint.pth\")\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(\"Resuming from checkpoint...\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        target.load_state_dict(ckpt[\"model\"])\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "        if \"scaler\" in ckpt:\n",
    "            scaler.load_state_dict(ckpt[\"scaler\"])\n",
    "        start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
    "        print(f\"Resumed at epoch {start_epoch}\")\n",
    "\n",
    "    print(f\"Starting SimSiam+CBAM pretraining for {epochs} epochs (from {start_epoch})...\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        target.train(True)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        micro = 0\n",
    "\n",
    "        for step, (x1, x2) in enumerate(dataloader, start=1):\n",
    "            x1 = x1.to(device, non_blocking=True)\n",
    "            x2 = x2.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
    "                p1, p2, z1, z2 = model(x1, x2)\n",
    "                loss_full = 0.5 * (negative_cosine_similarity(p1, z2) +\n",
    "                                   negative_cosine_similarity(p2, z1))\n",
    "                loss = loss_full / max(accumulation_steps, 1)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            micro += 1\n",
    "\n",
    "            if micro == accumulation_steps:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                micro = 0\n",
    "\n",
    "            total_loss += float(loss_full.item())\n",
    "\n",
    "        # Finalize residue grads if loop ended mid-accumulation\n",
    "        if micro > 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            ckpt_target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": ckpt_target.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"scaler\": scaler.state_dict(),\n",
    "            }, ckpt_path)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}] Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    final_path = os.path.join(checkpoint_dir, \"simsiam_cbam_pretrained_final.pth\")\n",
    "    ckpt_target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "    torch.save({\n",
    "        \"backbone\": ckpt_target.backbone.state_dict(),\n",
    "        \"projector\": ckpt_target.projector.state_dict(),\n",
    "        \"predictor\": ckpt_target.predictor.state_dict(),\n",
    "    }, final_path)\n",
    "    writer.close()\n",
    "    print(f\"Pretraining complete! Model saved to {final_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T03:23:30.276284Z",
     "iopub.status.busy": "2025-08-29T03:23:30.276022Z",
     "iopub.status.idle": "2025-08-29T03:40:02.221580Z",
     "shell.execute_reply": "2025-08-29T03:40:02.220916Z",
     "shell.execute_reply.started": "2025-08-29T03:23:30.276262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-29 03:23:40.088698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756437820.289227      74 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756437820.347267      74 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Using device: cuda\n",
      "Skipping torch.compile: compute capability 6.x < 7.0\n",
      "Resuming from checkpoint...\n",
      "Resumed at epoch 190\n",
      "Starting SimSiam+CBAM pretraining for 200 epochs (from 190)...\n",
      "Epoch [191/200] Avg Loss: -0.4395\n",
      "Epoch [192/200] Avg Loss: -0.4541\n",
      "Epoch [193/200] Avg Loss: -0.4229\n",
      "Epoch [194/200] Avg Loss: -0.4344\n",
      "Epoch [195/200] Avg Loss: -0.4363\n",
      "Epoch [196/200] Avg Loss: -0.4456\n",
      "Epoch [197/200] Avg Loss: -0.4710\n",
      "Epoch [198/200] Avg Loss: -0.4336\n",
      "Epoch [199/200] Avg Loss: -0.4351\n",
      "Epoch [200/200] Avg Loss: -0.4539\n",
      "Pretraining complete! Model saved to /kaggle/working/simsiam_cbam/simsiam_cbam_pretrained_final.pth\n"
     ]
    }
   ],
   "source": [
    "!python simsiam_cbam_pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBAM-ResNet fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T07:07:14.588461Z",
     "iopub.status.busy": "2025-08-29T07:07:14.588161Z",
     "iopub.status.idle": "2025-08-29T07:32:38.736027Z",
     "shell.execute_reply": "2025-08-29T07:32:38.735261Z",
     "shell.execute_reply.started": "2025-08-29T07:07:14.588435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda | IMG_SIZE=256\n",
      "Loading pretrained backbone from: /kaggle/working/simsiam_cbam/simsiam_cbam_pretrained_final.pth\n",
      "Skipping torch.compile: compute capability 6.x < 7.0\n",
      "\n",
      "Epoch 1/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0524, Acc: 0.3700 | Val Loss: 1.0759, Acc: 0.6263\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 2/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0348, Acc: 0.3932 | Val Loss: 1.0009, Acc: 0.4747\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 3/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0196, Acc: 0.3679 | Val Loss: 0.9624, Acc: 0.6162\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 4/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9877, Acc: 0.4440 | Val Loss: 0.8948, Acc: 0.6566\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 5/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9629, Acc: 0.4440 | Val Loss: 0.8513, Acc: 0.7475\n",
      "Validation accuracy improved, saving best state.\n",
      "Unfroze backbone at epoch 5\n",
      "\n",
      "Epoch 6/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9339, Acc: 0.5159 | Val Loss: 0.7557, Acc: 0.7475\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 7/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9267, Acc: 0.4820 | Val Loss: 0.7080, Acc: 0.8081\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 8/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8988, Acc: 0.4968 | Val Loss: 0.6711, Acc: 0.7071\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 9/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8959, Acc: 0.5032 | Val Loss: 0.6340, Acc: 0.7879\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 10/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8764, Acc: 0.5243 | Val Loss: 0.5914, Acc: 0.8283\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 11/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7812, Acc: 0.6321 | Val Loss: 0.4867, Acc: 0.8788\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 12/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8259, Acc: 0.5814 | Val Loss: 0.4969, Acc: 0.8283\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 13/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8112, Acc: 0.5708 | Val Loss: 0.4747, Acc: 0.8788\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 14/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8214, Acc: 0.5793 | Val Loss: 0.4739, Acc: 0.8687\n",
      "EarlyStopping counter: 3 / 10\n",
      "\n",
      "Epoch 15/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7960, Acc: 0.5835 | Val Loss: 0.5007, Acc: 0.8182\n",
      "EarlyStopping counter: 4 / 10\n",
      "\n",
      "Epoch 16/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7463, Acc: 0.6321 | Val Loss: 0.4040, Acc: 0.8889\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 17/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7655, Acc: 0.5920 | Val Loss: 0.4335, Acc: 0.8384\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 18/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7939, Acc: 0.5856 | Val Loss: 0.4085, Acc: 0.8687\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 19/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7677, Acc: 0.5983 | Val Loss: 0.4048, Acc: 0.8687\n",
      "EarlyStopping counter: 3 / 10\n",
      "\n",
      "Epoch 20/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8363, Acc: 0.5603 | Val Loss: 0.4403, Acc: 0.8586\n",
      "EarlyStopping counter: 4 / 10\n",
      "\n",
      "Epoch 21/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7535, Acc: 0.6025 | Val Loss: 0.4203, Acc: 0.8990\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 22/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7789, Acc: 0.6152 | Val Loss: 0.3978, Acc: 0.8889\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 23/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7862, Acc: 0.6216 | Val Loss: 0.4498, Acc: 0.8081\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 24/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7886, Acc: 0.5814 | Val Loss: 0.3914, Acc: 0.8586\n",
      "EarlyStopping counter: 3 / 10\n",
      "\n",
      "Epoch 25/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6754, Acc: 0.6744 | Val Loss: 0.3707, Acc: 0.8990\n",
      "EarlyStopping counter: 4 / 10\n",
      "\n",
      "Epoch 26/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7498, Acc: 0.6195 | Val Loss: 0.3908, Acc: 0.8384\n",
      "EarlyStopping counter: 5 / 10\n",
      "\n",
      "Epoch 27/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7536, Acc: 0.6321 | Val Loss: 0.3866, Acc: 0.8485\n",
      "EarlyStopping counter: 6 / 10\n",
      "\n",
      "Epoch 28/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7702, Acc: 0.5856 | Val Loss: 0.3676, Acc: 0.8687\n",
      "EarlyStopping counter: 7 / 10\n",
      "\n",
      "Epoch 29/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7065, Acc: 0.6575 | Val Loss: 0.3862, Acc: 0.8485\n",
      "EarlyStopping counter: 8 / 10\n",
      "\n",
      "Epoch 30/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7560, Acc: 0.6195 | Val Loss: 0.3580, Acc: 0.9091\n",
      "Validation accuracy improved, saving best state.\n",
      "\n",
      "Epoch 31/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7527, Acc: 0.5983 | Val Loss: 0.3863, Acc: 0.8586\n",
      "EarlyStopping counter: 1 / 10\n",
      "\n",
      "Epoch 32/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7455, Acc: 0.6216 | Val Loss: 0.3674, Acc: 0.8788\n",
      "EarlyStopping counter: 2 / 10\n",
      "\n",
      "Epoch 33/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7306, Acc: 0.6237 | Val Loss: 0.3599, Acc: 0.8889\n",
      "EarlyStopping counter: 3 / 10\n",
      "\n",
      "Epoch 34/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7185, Acc: 0.6575 | Val Loss: 0.3537, Acc: 0.8990\n",
      "EarlyStopping counter: 4 / 10\n",
      "\n",
      "Epoch 35/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7114, Acc: 0.6364 | Val Loss: 0.3881, Acc: 0.8384\n",
      "EarlyStopping counter: 5 / 10\n",
      "\n",
      "Epoch 36/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8030, Acc: 0.5708 | Val Loss: 0.3780, Acc: 0.8990\n",
      "EarlyStopping counter: 6 / 10\n",
      "\n",
      "Epoch 37/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6783, Acc: 0.6638 | Val Loss: 0.3497, Acc: 0.8990\n",
      "EarlyStopping counter: 7 / 10\n",
      "\n",
      "Epoch 38/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7579, Acc: 0.6131 | Val Loss: 0.3784, Acc: 0.8586\n",
      "EarlyStopping counter: 8 / 10\n",
      "\n",
      "Epoch 39/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6906, Acc: 0.6512 | Val Loss: 0.3494, Acc: 0.8990\n",
      "EarlyStopping counter: 9 / 10\n",
      "\n",
      "Epoch 40/60 | mixup_alpha=0.3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7295, Acc: 0.6406 | Val Loss: 0.3486, Acc: 0.8788\n",
      "EarlyStopping counter: 10 / 10\n",
      "Early stopping triggered.\n",
      "Best epoch (val acc): 30\n",
      "Best model weights saved to ./finetuned_cbam_best.pth\n",
      "Best model checkpoint saved to ./finetuned_cbam_checkpoint.pth\n",
      "\n",
      "Test set results (CBAM):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2868, Test Acc: 0.9596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.89      0.94        37\n",
      "Healthy Leaf       0.89      1.00      0.94        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.96        99\n",
      "   macro avg       0.96      0.96      0.96        99\n",
      "weighted avg       0.96      0.96      0.96        99\n",
      "\n",
      "Test ROC-AUC (macro): 0.9991\n",
      "\n",
      "Test-Time Augmentation (TTA) Evaluation (CBAM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.95      0.97        37\n",
      "Healthy Leaf       0.94      1.00      0.97        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.98        99\n",
      "   macro avg       0.98      0.98      0.98        99\n",
      "weighted avg       0.98      0.98      0.98        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finetune_cbam.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List, Iterable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cbam_resnet import cbam_resnet50  # ensure cbam_resnet.py is available\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "def seed_all(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Mixup helpers\n",
    "# ----------------------------\n",
    "def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.3, device=None):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha and alpha > 0 else 1.0\n",
    "    idx = torch.randperm(x.size(0), device=device)\n",
    "    mixed_x = lam * x + (1.0 - lam) * x[idx]\n",
    "    y_a, y_b = y, y[idx]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam: float):\n",
    "    return lam * criterion(pred, y_a) + (1.0 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Optim helper (param-wise WD)\n",
    "# ----------------------------\n",
    "def exclude_from_wd(named_params: Iterable, wd: float = 1e-4):\n",
    "    wd_params, no_wd_params = [], []\n",
    "    for n, p in named_params:\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if p.ndim == 1 or n.endswith(\".bias\") or \"bn\" in n.lower():\n",
    "            no_wd_params.append(p)\n",
    "        else:\n",
    "            wd_params.append(p)\n",
    "    return [{\"params\": wd_params, \"weight_decay\": wd},\n",
    "            {\"params\": no_wd_params, \"weight_decay\": 0.0}]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Data\n",
    "# ----------------------------\n",
    "def make_train_transform(img_size: int) -> transforms.Compose:\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15), ratio=(0.3, 3.3), value='random'),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_eval_transform(img_size: int) -> transforms.Compose:\n",
    "    resize_size = int(round(img_size * 1.14))  # common 224->256 style scaling\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir: str,\n",
    "    val_dir: str,\n",
    "    test_dir: str,\n",
    "    batch_size: int = 32,\n",
    "    num_workers: int = 2,\n",
    "    img_size: int = 224,\n",
    "):\n",
    "    pin = torch.cuda.is_available()\n",
    "\n",
    "    train_transform = make_train_transform(img_size)\n",
    "    eval_transform  = make_eval_transform(img_size)\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, transform=train_transform)\n",
    "    val_ds   = ImageFolder(val_dir,   transform=eval_transform)\n",
    "    test_ds  = ImageFolder(test_dir,  transform=eval_transform)\n",
    "\n",
    "    class_names = train_ds.classes\n",
    "\n",
    "    # Class-balanced sampling\n",
    "    targets_np = np.array(train_ds.targets, dtype=np.int64)\n",
    "    classes = np.unique(targets_np)\n",
    "    class_counts = np.array([(targets_np == c).sum() for c in classes], dtype=np.float64)\n",
    "    class_counts[class_counts == 0] = 1.0\n",
    "    weights_per_class = 1.0 / class_counts\n",
    "    sample_weights = weights_per_class[targets_np]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, sampler=sampler,\n",
    "        num_workers=num_workers, pin_memory=pin, drop_last=True  # stabilizes BN + MixUp\n",
    "    )\n",
    "    val_loader   = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin\n",
    "    )\n",
    "    test_loader  = DataLoader(\n",
    "        test_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=num_workers, pin_memory=pin\n",
    "    )\n",
    "    return train_loader, val_loader, test_loader, test_ds, class_names\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    \"\"\"\n",
    "    Loads a CBAM-ResNet50 backbone and attaches a small classifier head.\n",
    "    Expects SimSiam pretrain checkpoint with key 'backbone' (or raw state_dict).\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_path: str, num_classes: int):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # up to avgpool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        ckpt = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        sd = ckpt.get(\"backbone\", ckpt)\n",
    "        loaded = self.backbone.load_state_dict(sd, strict=False)\n",
    "        missing, unexpected = loaded.missing_keys, loaded.unexpected_keys\n",
    "        if missing and any(k.startswith((\"layer1\", \"layer2\", \"layer3\", \"layer4\")) for k in missing):\n",
    "            raise RuntimeError(f\"Backbone weights incompatible, missing critical keys: {missing[:10]}\")\n",
    "        if unexpected:\n",
    "            print(f\"[state_dict notice] unexpected: {unexpected[:10]}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Scheduler (warmup + cosine)\n",
    "# ----------------------------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "def get_scheduler(optimizer, total_epochs: int, warmup_epochs: int = 5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=max(total_epochs - warmup_epochs, 1))\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Train / Eval\n",
    "# ----------------------------\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    scaler: torch.amp.GradScaler,\n",
    "    use_mixup: bool = True,\n",
    "    mixup_alpha: float = 0.3\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss, correct = 0.0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast('cuda', enabled=(device.type == \"cuda\")):\n",
    "            if use_mixup and mixup_alpha > 0:\n",
    "                imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha, device=device)\n",
    "                logits = model(imgs)\n",
    "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
    "                preds = logits.argmax(1)\n",
    "                correct += int(lam * preds.eq(y_a).sum().item()\n",
    "                               + (1.0 - lam) * preds.eq(y_b).sum().item())\n",
    "            else:\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, labels)\n",
    "                preds = logits.argmax(1)\n",
    "                correct += int(preds.eq(labels).sum().item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += float(loss.item()) * imgs.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, correct / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float, np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    total_loss, correct = 0.0, 0\n",
    "    all_labels: List[int] = []\n",
    "    all_probs:  List[np.ndarray] = []\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += float(loss.item()) * imgs.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += int(preds.eq(labels).sum().item())\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, correct / n, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Early stopping\n",
    "# ----------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, verbose: bool = True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.best_epoch = -1\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_acc: float, model: nn.Module, epoch: int):\n",
    "        if self.best_acc is None or val_acc > self.best_acc:\n",
    "            self.best_acc = val_acc\n",
    "            self.counter = 0\n",
    "            self.best_epoch = epoch\n",
    "            self.best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            if self.verbose:\n",
    "                print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plots\n",
    "# ----------------------------\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix (CBAM)')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_roc_per_class(y_true, y_score, class_names, save_path=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, name in enumerate(class_names):\n",
    "        if np.sum(y_true == i) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve((y_true == i).astype(int), y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"ROC error for class {name}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Per-class ROC Curves (CBAM)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_reliability(y_true, y_prob, class_names, n_bins=10, save_path=\"reliability_diagram_cbam.png\"):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    y_prob = np.clip(y_prob, 1e-6, 1-1e-6)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i, name in enumerate(class_names):\n",
    "        try:\n",
    "            prob_true, prob_pred = calibration_curve((y_true == i).astype(int),\n",
    "                                                     y_prob[:, i],\n",
    "                                                     n_bins=n_bins,\n",
    "                                                     strategy='uniform')\n",
    "            plt.plot(prob_pred, prob_true, marker='o', label=name)\n",
    "        except Exception as e:\n",
    "            print(f\"Reliability curve failed for {name}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.title(\"Reliability Diagram (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs, save_path=\"loss_acc_curves_cbam.png\"):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curves (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# TTA\n",
    "# ----------------------------\n",
    "def tta_eval(\n",
    "    model: nn.Module,\n",
    "    test_ds: ImageFolder,\n",
    "    batch_size: int,\n",
    "    class_names: List[str],\n",
    "    device: torch.device,\n",
    "    img_size: int\n",
    "):\n",
    "    resize_base = int(round(img_size * 1.14))\n",
    "    resize_up   = int(round(img_size * 1.25))\n",
    "\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([transforms.Resize(resize_base), transforms.CenterCrop(img_size),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(resize_base), transforms.CenterCrop(img_size),\n",
    "                            transforms.RandomHorizontalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(resize_up), transforms.CenterCrop(img_size),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(resize_base), transforms.CenterCrop(img_size),\n",
    "                            transforms.GaussianBlur(3),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                        [0.229, 0.224, 0.225])]),\n",
    "    ]\n",
    "    pin = torch.cuda.is_available()\n",
    "    model.eval()\n",
    "    probs_all = []\n",
    "\n",
    "    for t in tta_transforms:\n",
    "        test_ds.transform = t\n",
    "        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=2, pin_memory=pin)\n",
    "        probs_list = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                logits = model(imgs)\n",
    "                probs_list.append(F.softmax(logits, dim=1).cpu().numpy())\n",
    "        probs_all.append(np.concatenate(probs_list, axis=0))\n",
    "\n",
    "    tta_probs = np.array(probs_all)\n",
    "    mean_probs = np.mean(tta_probs, axis=0)\n",
    "    final_preds = mean_probs.argmax(axis=1)\n",
    "    return mean_probs, final_preds\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main(\n",
    "    data_root: str = \"/kaggle/input/minida/mini_output1\",\n",
    "    pretrained_path: str = \"/kaggle/working/simsiam_cbam/simsiam_cbam_pretrained_final.pth\",\n",
    "    epochs: int = 60,\n",
    "    batch_size: int = 24,\n",
    "    num_workers: int = 2,\n",
    "    patience: int = 10,\n",
    "    mixup_alpha: float = 0.3,\n",
    "    head_only_warmup_epochs: int = 5,\n",
    "    img_size: int = 256,\n",
    "    save_dir: str = \".\"  # NEW: where to write model files\n",
    "):\n",
    "    seed_all(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device} | IMG_SIZE={img_size}\")\n",
    "    print(f\"Loading pretrained backbone from: {pretrained_path}\")\n",
    "\n",
    "    # NEW: ensure output directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_dir = os.path.join(data_root, \"train\")\n",
    "    val_dir   = os.path.join(data_root, \"val\")\n",
    "    test_dir  = os.path.join(data_root, \"test\")\n",
    "\n",
    "    train_loader, val_loader, test_loader, test_ds, class_names = get_loaders(\n",
    "        train_dir, val_dir, test_dir, batch_size=batch_size, num_workers=num_workers, img_size=img_size\n",
    "    )\n",
    "\n",
    "    # Build model\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=len(class_names)).to(device)\n",
    "\n",
    "    # Ensure BN uses batch stats during supervised finetune\n",
    "    for m in model.backbone.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.train()\n",
    "            m.requires_grad_(True)\n",
    "\n",
    "    # Parameter groups: smaller LR for backbone, param-wise WD\n",
    "    backbone_named = [(n, p) for n, p in model.named_parameters() if \"classifier\" not in n]\n",
    "    head_named     = [(n, p) for n, p in model.named_parameters() if \"classifier\" in n]\n",
    "    groups_backbone = exclude_from_wd(backbone_named, wd=1e-4)\n",
    "    groups_head     = exclude_from_wd(head_named, wd=1e-4)\n",
    "    for g in groups_backbone: g[\"lr\"] = 3e-5\n",
    "    for g in groups_head:     g[\"lr\"] = 1e-4\n",
    "\n",
    "    optimizer = torch.optim.AdamW(groups_backbone + groups_head, betas=(0.9, 0.999))\n",
    "    scheduler = get_scheduler(optimizer, total_epochs=epochs, warmup_epochs=5)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.02)\n",
    "\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n",
    "\n",
    "    # Optional: head-only warmup (freeze backbone for first few epochs)\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    # --- safe torch.compile (skip on CC < 7.0) ---\n",
    "    use_compile = False\n",
    "    if hasattr(torch, \"compile\") and torch.cuda.is_available():\n",
    "        try:\n",
    "            major, _ = torch.cuda.get_device_capability()\n",
    "            if major >= 7:\n",
    "                use_compile = True\n",
    "            else:\n",
    "                print(f\"Skipping torch.compile: compute capability {major}.x < 7.0\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not query device capability, skipping compile: {e}\")\n",
    "\n",
    "    if use_compile:\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            print(f\"torch.compile failed, falling back to eager: {e}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Unfreeze backbone after warmup\n",
    "        if epoch == head_only_warmup_epochs:\n",
    "            target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "            for p in target.backbone.parameters():\n",
    "                p.requires_grad = True\n",
    "            print(f\"Unfroze backbone at epoch {epoch}\")\n",
    "\n",
    "        # MixUp annealing\n",
    "        decay_start = int(epochs * 2 / 3)\n",
    "        if epoch < decay_start:\n",
    "            cur_alpha = mixup_alpha\n",
    "        else:\n",
    "            remaining = max(epochs - epoch - 1, 0)\n",
    "            span = max(epochs - decay_start, 1)\n",
    "            cur_alpha = mixup_alpha * (remaining / span)\n",
    "        use_mixup_flag = cur_alpha > 1e-6\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs} | mixup_alpha={cur_alpha:.4f}\")\n",
    "        tr_loss, tr_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, scaler,\n",
    "            use_mixup=use_mixup_flag, mixup_alpha=cur_alpha\n",
    "        )\n",
    "        va_loss, va_acc, va_labels, va_probs = eval_epoch(model, val_loader, criterion, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {tr_loss:.4f}, Acc: {tr_acc:.4f} | \"\n",
    "              f\"Val Loss: {va_loss:.4f}, Acc: {va_acc:.4f}\")\n",
    "\n",
    "        train_losses.append(tr_loss); train_accs.append(tr_acc)\n",
    "        val_losses.append(va_loss);   val_accs.append(va_acc)\n",
    "\n",
    "        # Early stopping\n",
    "        target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        if not hasattr(main, \"_early\"):\n",
    "            main._early = EarlyStopping(patience=patience, verbose=True)\n",
    "        if main._early(va_acc, target, epoch):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # === Load best state and SAVE to disk (NEW) ===\n",
    "    target = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "    target.load_state_dict(main._early.best_state)\n",
    "    target.to(device).eval()\n",
    "    print(f\"Best epoch (val acc): {main._early.best_epoch + 1}\")\n",
    "\n",
    "    # NEW: save-only-weights file (for inference)\n",
    "    best_weights_path = os.path.join(save_dir, \"finetuned_cbam_best.pth\")\n",
    "    torch.save(target.state_dict(), best_weights_path)\n",
    "    print(f\"Best model weights saved to {best_weights_path}\")\n",
    "\n",
    "    # NEW: full checkpoint (resume training capability)\n",
    "    best_ckpt_path = os.path.join(save_dir, \"finetuned_cbam_checkpoint.pth\")\n",
    "    torch.save({\n",
    "        \"epoch\": main._early.best_epoch,\n",
    "        \"model_state_dict\": target.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"best_val_acc\": main._early.best_acc,\n",
    "        \"class_names\": class_names,\n",
    "        \"img_size\": img_size,\n",
    "    }, best_ckpt_path)\n",
    "    print(f\"Best model checkpoint saved to {best_ckpt_path}\")\n",
    "\n",
    "    # Curves\n",
    "    plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs)\n",
    "\n",
    "    # Final Test Evaluation\n",
    "    print(\"\\nTest set results (CBAM):\")\n",
    "    te_loss, te_acc, te_labels, te_probs = eval_epoch(target, test_loader, criterion, device)\n",
    "    te_preds = te_probs.argmax(axis=1)\n",
    "    print(f\"Test Loss: {te_loss:.4f}, Test Acc: {te_acc:.4f}\")\n",
    "    print(classification_report(te_labels, te_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(te_labels, te_preds, class_names, save_path=\"cbam_norm_confmat.png\")\n",
    "\n",
    "    try:\n",
    "        te_onehot = np.eye(len(class_names))[te_labels]\n",
    "        roc_macro = roc_auc_score(te_onehot, te_probs, average='macro', multi_class='ovr')\n",
    "        print(f\"Test ROC-AUC (macro): {roc_macro:.4f}\")\n",
    "        plot_roc_per_class(te_labels, te_probs, class_names, save_path=\"cbam_perclass_roc.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "\n",
    "    plot_reliability(te_labels, te_probs, class_names)\n",
    "\n",
    "    # TTA Evaluation\n",
    "    print(\"\\nTest-Time Augmentation (TTA) Evaluation (CBAM):\")\n",
    "    mean_probs, final_preds = tta_eval(target, test_ds, batch_size, class_names, device, img_size)\n",
    "    print(classification_report(test_ds.targets, final_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_ds.targets, final_preds, class_names, save_path=\"cbam_tta_confmat.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
