{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T22:01:47.511517Z",
     "iopub.status.busy": "2025-08-27T22:01:47.511227Z",
     "iopub.status.idle": "2025-08-27T22:03:07.838858Z",
     "shell.execute_reply": "2025-08-27T22:03:07.838147Z",
     "shell.execute_reply.started": "2025-08-27T22:01:47.511491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grad-cam\n",
      "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam) (11.2.1)\n",
      "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.21.0+cu124)\n",
      "Collecting ttach (from grad-cam)\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.67.1)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam) (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.7.1->grad-cam)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->grad-cam) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->grad-cam) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->grad-cam) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->grad-cam) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->grad-cam) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Building wheels for collected packages: grad-cam\n",
      "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=3a0f787a2956c024f285506704a12131c4e75fd0645753f3d0c6ba8e7492cd13\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/52/78/893c3b94279ef238f43a9e89608af648de401b96415bebbd1f\n",
      "Successfully built grad-cam\n",
      "Installing collected packages: ttach, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, grad-cam\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed grad-cam-1.5.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ttach-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T04:12:09.350909Z",
     "iopub.status.busy": "2025-08-27T04:12:09.350639Z",
     "iopub.status.idle": "2025-08-27T04:20:30.005366Z",
     "shell.execute_reply": "2025-08-27T04:20:30.004597Z",
     "shell.execute_reply.started": "2025-08-27T04:12:09.350889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-27 04:12:09,428 - INFO - Starting Swin Transformer training at 20250827_041209\n",
      "2025-08-27 04:12:09,728 - INFO - Dataset sizes | train: 473 | val: 99 | test: 99\n",
      "2025-08-27 04:12:10,805 - INFO - Skipping torch.compile: CUDA capability < 7.0; falling back to eager.\n",
      "Epoch 1/40: 100%|██████████| 29/29 [00:40<00:00,  1.39s/it]\n",
      "2025-08-27 04:12:59,714 - INFO - Epoch 1 Train | Loss: 0.6463 | Acc: 0.4957 | AUC: 0.6961\n",
      "2025-08-27 04:12:59,714 - INFO - Epoch 1 Val   | Loss: 0.1719 | Acc: 1.0000 | AUC: 1.0000\n",
      "2025-08-27 04:13:00,871 - INFO - Saved best model with AUC: 1.0000 at epoch 1\n",
      "Epoch 2/40: 100%|██████████| 29/29 [00:41<00:00,  1.44s/it]\n",
      "2025-08-27 04:13:51,435 - INFO - Epoch 2 Train | Loss: 0.3261 | Acc: 0.6573 | AUC: 0.7866\n",
      "2025-08-27 04:13:51,436 - INFO - Epoch 2 Val   | Loss: 0.1508 | Acc: 0.9596 | AUC: 0.9973\n",
      "Epoch 3/40: 100%|██████████| 29/29 [00:38<00:00,  1.33s/it]\n",
      "2025-08-27 04:14:38,439 - INFO - Epoch 3 Train | Loss: 0.3140 | Acc: 0.6853 | AUC: 0.8274\n",
      "2025-08-27 04:14:38,440 - INFO - Epoch 3 Val   | Loss: 0.1151 | Acc: 0.9596 | AUC: 0.9912\n",
      "Epoch 4/40: 100%|██████████| 29/29 [00:39<00:00,  1.38s/it]\n",
      "2025-08-27 04:15:26,848 - INFO - Epoch 4 Train | Loss: 0.3445 | Acc: 0.6810 | AUC: 0.8480\n",
      "2025-08-27 04:15:26,849 - INFO - Epoch 4 Val   | Loss: 0.0936 | Acc: 0.9798 | AUC: 0.9969\n",
      "Epoch 5/40: 100%|██████████| 29/29 [00:41<00:00,  1.43s/it]\n",
      "2025-08-27 04:16:16,638 - INFO - Epoch 5 Train | Loss: 0.2776 | Acc: 0.7241 | AUC: 0.8606\n",
      "2025-08-27 04:16:16,639 - INFO - Epoch 5 Val   | Loss: 0.0493 | Acc: 0.9798 | AUC: 0.9997\n",
      "Epoch 6/40: 100%|██████████| 29/29 [00:39<00:00,  1.35s/it]\n",
      "2025-08-27 04:17:04,433 - INFO - Epoch 6 Train | Loss: 0.2687 | Acc: 0.7392 | AUC: 0.8710\n",
      "2025-08-27 04:17:04,433 - INFO - Epoch 6 Val   | Loss: 0.0976 | Acc: 0.9697 | AUC: 0.9965\n",
      "Epoch 7/40: 100%|██████████| 29/29 [00:40<00:00,  1.41s/it]\n",
      "2025-08-27 04:17:53,735 - INFO - Epoch 7 Train | Loss: 0.2477 | Acc: 0.7177 | AUC: 0.8490\n",
      "2025-08-27 04:17:53,735 - INFO - Epoch 7 Val   | Loss: 0.0753 | Acc: 0.9798 | AUC: 0.9987\n",
      "Epoch 8/40: 100%|██████████| 29/29 [00:41<00:00,  1.42s/it]\n",
      "2025-08-27 04:18:43,499 - INFO - Epoch 8 Train | Loss: 0.2219 | Acc: 0.7457 | AUC: 0.8901\n",
      "2025-08-27 04:18:43,499 - INFO - Epoch 8 Val   | Loss: 0.1295 | Acc: 0.9596 | AUC: 0.9941\n",
      "2025-08-27 04:18:43,500 - INFO - Early stopping at epoch 8\n",
      "2025-08-27 04:18:43,811 - INFO - Best Val AUC: 1.0000 (epoch 1) | checkpoint: ./output/best_swinv2.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test (TTA) F1 Macro: 0.9705 | AUC: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.92      0.96        37\n",
      "Healthy Leaf       0.91      1.00      0.95        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.97      0.97      0.97        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n",
      "\n",
      "Test_tta Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria     1.0000    0.9189    0.9577        37\n",
      "Healthy Leaf     0.9118    1.0000    0.9538        31\n",
      "  straw_mite     1.0000    1.0000    1.0000        31\n",
      "\n",
      "    accuracy                         0.9697        99\n",
      "   macro avg     0.9706    0.9730    0.9705        99\n",
      "weighted avg     0.9724    0.9697    0.9698        99\n",
      "\n",
      "No TTA flip examples found!\n",
      "Saved Grad-CAM for class Alternaria: ./output/gradcam_Alternaria_20250827_041209.png\n",
      "Saved Grad-CAM for class Healthy Leaf: ./output/gradcam_Healthy Leaf_20250827_041209.png\n",
      "Saved Grad-CAM for class straw_mite: ./output/gradcam_straw_mite_20250827_041209.png\n"
     ]
    }
   ],
   "source": [
    "# === First, in a separate cell if needed ===\n",
    "# !pip install pytorch-grad-cam timm\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, roc_auc_score,\n",
    "    accuracy_score, balanced_accuracy_score, roc_curve\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "# Use new torch.amp API to avoid deprecation warnings\n",
    "from torch.amp import autocast, GradScaler\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "import datetime\n",
    "import logging\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import cv2\n",
    "from PIL import Image  # <-- needed for Grad-CAM step\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "# Try to import Swin reshape; if unavailable, define a fallback\n",
    "try:\n",
    "    from pytorch_grad_cam.utils.reshape_transforms import swin_reshape_transform as _swin_reshape_transform\n",
    "    _HAVE_SWN = True\n",
    "except Exception:\n",
    "    _HAVE_SWN = False\n",
    "\n",
    "def _timm_swin_fallback_reshape(tensor):\n",
    "    \"\"\"\n",
    "    Fallback reshape for timm Swin/SwinV2.\n",
    "    Accepts [B, L, C] and returns [B, C, h, w] by inferring h*w=L.\n",
    "    If tensor is already [B, C, H, W], returns as-is.\n",
    "    Raises a clear error if L is not factorizable into a near-square grid.\n",
    "    \"\"\"\n",
    "    if tensor.dim() == 4:\n",
    "        return tensor\n",
    "    B, L, C = tensor.shape\n",
    "    h = int(round(L ** 0.5))\n",
    "    w = L // h if h > 0 else 0\n",
    "    if h * w != L:\n",
    "        raise ValueError(\n",
    "            f\"Grad-CAM reshape fallback cannot form HxW from sequence length L={L}. \"\n",
    "            \"Provide a proper reshape_transform for this backbone/IMG_SIZE.\"\n",
    "        )\n",
    "    return tensor.transpose(1, 2).reshape(B, C, h, w)\n",
    "\n",
    "# Unified reshape_transform to use with GradCAM\n",
    "reshape_transform = _swin_reshape_transform if _HAVE_SWN else _timm_swin_fallback_reshape\n",
    "\n",
    "# ---------------------------- Configuration ----------------------------\n",
    "class Config:\n",
    "    DATA_PATHS = {\n",
    "        \"train\": \"/kaggle/input/minida/mini_output1/train\",\n",
    "        \"val\":   \"/kaggle/input/minida/mini_output1/val\",\n",
    "        \"test\":  \"/kaggle/input/minida/mini_output1/test\"\n",
    "    }\n",
    "    CLASS_NAMES = sorted(os.listdir(DATA_PATHS[\"train\"]))\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "    MODEL_NAME = \"swinv2_small_window16_256\"\n",
    "    IMG_SIZE = 256\n",
    "\n",
    "    DROP_RATE = 0.2\n",
    "    DROP_PATH_RATE = 0.2\n",
    "\n",
    "    USE_MIXUP = True\n",
    "    MIXUP_ALPHA = 0.3\n",
    "    CUTMIX_ALPHA = 1.0\n",
    "\n",
    "    USE_TTA = True\n",
    "\n",
    "    ACCUM_STEPS = 2\n",
    "    TRAIN_BATCH_SIZE = 32 // ACCUM_STEPS\n",
    "    VAL_BATCH_SIZE = 64\n",
    "\n",
    "    EPOCHS = 40\n",
    "    LR = 1e-4\n",
    "    WEIGHT_DECAY = 0.05\n",
    "\n",
    "    LABEL_SMOOTHING = 0.1  # will be set to 0.0 automatically if USE_MIXUP\n",
    "\n",
    "    CONTRASTIVE_LOSS_WEIGHT = 0.3\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    NUM_WORKERS = 2\n",
    "    MIXED_PRECISION = True\n",
    "\n",
    "    OUTPUT_DIR = \"./output\"\n",
    "    MODEL_SAVE = f\"./output/best_swinv2.pth\"\n",
    "\n",
    "    EARLY_STOP_PATIENCE = 7\n",
    "    GRAD_CLIP = 1.0\n",
    "\n",
    "    LOG_FILE = \"training.log\"\n",
    "\n",
    "    # Determinism vs speed\n",
    "    DETERMINISTIC = True  # set False for speed (enables cudnn.benchmark)\n",
    "\n",
    "    def __init__(self):\n",
    "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
    "        self._set_seed()\n",
    "        self._set_timestamp()\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _set_seed(self):\n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(42)\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "        torch.backends.cudnn.deterministic = self.DETERMINISTIC\n",
    "        torch.backends.cudnn.benchmark = not self.DETERMINISTIC\n",
    "        os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "    def _set_timestamp(self):\n",
    "        self.TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        # avoid duplicate handlers across notebook reruns\n",
    "        if not self.logger.handlers:\n",
    "            file_handler = logging.FileHandler(os.path.join(self.OUTPUT_DIR, self.LOG_FILE))\n",
    "            file_handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(file_handler)\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(console_handler)\n",
    "\n",
    "cfg = Config()\n",
    "VIS_BATCH_SIZE = min(16, cfg.VAL_BATCH_SIZE)\n",
    "\n",
    "# If mixup is enabled, avoid double-smoothing\n",
    "if cfg.USE_MIXUP:\n",
    "    cfg.LABEL_SMOOTHING = 0.0\n",
    "\n",
    "def config_to_serializable_dict(cfg):\n",
    "    skip_types = (logging.Logger,)\n",
    "    out = {}\n",
    "    for k, v in cfg.__dict__.items():\n",
    "        if k.startswith(\"_\") or isinstance(v, skip_types) or callable(v):\n",
    "            continue\n",
    "        try:\n",
    "            json.dumps(v)\n",
    "            out[k] = v\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, f'run_config_{cfg.TIMESTAMP}.json'), 'w') as f:\n",
    "    json.dump(config_to_serializable_dict(cfg), f, indent=4)\n",
    "\n",
    "# --------- Supervised Contrastive Loss ----------\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def forward(self, features, labels):\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        similarity_matrix = torch.div(torch.matmul(features, features.T), self.temperature)\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "        logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)\n",
    "        logits = similarity_matrix - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(labels.shape[0], device=features.device))\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1).clamp(min=1)\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "# --------- Augmentations ----------\n",
    "class PathologyAugment:\n",
    "    @staticmethod\n",
    "    def get_train_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(cfg.IMG_SIZE, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "            transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
    "            transforms.RandomApply([transforms.RandomRotation(15)], p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_test_transform():\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(int(cfg.IMG_SIZE * 1.15)),\n",
    "            transforms.CenterCrop(cfg.IMG_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tta_transforms():\n",
    "        base = PathologyAugment.get_test_transform()\n",
    "        hflip = transforms.Compose([\n",
    "            transforms.Resize(int(cfg.IMG_SIZE * 1.15)),\n",
    "            transforms.CenterCrop(cfg.IMG_SIZE),\n",
    "            transforms.RandomHorizontalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        vflip = transforms.Compose([\n",
    "            transforms.Resize(int(cfg.IMG_SIZE * 1.15)),\n",
    "            transforms.CenterCrop(cfg.IMG_SIZE),\n",
    "            transforms.RandomVerticalFlip(p=1.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return [base, hflip, vflip]\n",
    "\n",
    "# --------- Metrics Tracker ----------\n",
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.losses, self.preds, self.targets, self.probs = [], [], [], []\n",
    "    def update(self, loss, outputs, targets):\n",
    "        self.losses.append(loss)\n",
    "        probs = outputs.float().softmax(1).detach().cpu()\n",
    "        self.probs.append(probs)\n",
    "        self.preds.append(probs.argmax(1))\n",
    "        self.targets.append(targets.detach().cpu())\n",
    "    def compute(self):\n",
    "        probs = torch.cat(self.probs).numpy()\n",
    "        preds = torch.cat(self.preds).numpy()\n",
    "        targets = torch.cat(self.targets).numpy()\n",
    "        probs = probs / (probs.sum(axis=1, keepdims=True) + 1e-10)\n",
    "        try:\n",
    "            auc_score = roc_auc_score(targets, probs, multi_class='ovo')\n",
    "        except Exception:\n",
    "            auc_score = float('nan')\n",
    "        return {\n",
    "            \"loss\": np.mean(self.losses),\n",
    "            \"accuracy\": accuracy_score(targets, preds),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(targets, preds),\n",
    "            \"f1_macro\": f1_score(targets, preds, average='macro'),\n",
    "            \"auc\": auc_score,\n",
    "            \"targets\": targets,\n",
    "            \"preds\": preds,\n",
    "            \"probs\": probs\n",
    "        }\n",
    "\n",
    "# --------- Data helpers ----------\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    indices = [label for _, label in dataset.samples]\n",
    "    class_counts = np.bincount(indices, minlength=len(dataset.classes))\n",
    "    class_counts[class_counts == 0] = 1  # avoid div-by-zero\n",
    "    weights_per_class = 1.0 / class_counts\n",
    "    sample_weights = np.array([weights_per_class[label] for label in indices], dtype=np.float64)\n",
    "    sample_weights = torch.as_tensor(sample_weights, dtype=torch.double)\n",
    "    return WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# --------- Model ----------\n",
    "\n",
    "def create_model():\n",
    "    model = timm.create_model(\n",
    "        cfg.MODEL_NAME, pretrained=True, num_classes=cfg.NUM_CLASSES,\n",
    "        drop_rate=cfg.DROP_RATE, drop_path_rate=cfg.DROP_PATH_RATE, img_size=cfg.IMG_SIZE\n",
    "    ).to(cfg.DEVICE)\n",
    "\n",
    "    # Torch 2.x compile: skip on older GPUs (e.g., Tesla P100, CC 6.0) and fall back on eager safely\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        try:\n",
    "            if cfg.DEVICE.type == 'cuda':\n",
    "                major, minor = torch.cuda.get_device_capability()\n",
    "                if major < 7:\n",
    "                    cfg.logger.info(\"Skipping torch.compile: CUDA capability < 7.0; falling back to eager.\")\n",
    "                    return model\n",
    "            model = torch.compile(model)\n",
    "        except Exception as e:\n",
    "            cfg.logger.info(f\"torch.compile failed; falling back to eager. Reason: {e}\")\n",
    "            # keep eager model\n",
    "            pass\n",
    "    return model\n",
    "\n",
    "# --------- Eval / Plots ----------\n",
    "\n",
    "def evaluate(model, loader, criterion=None):\n",
    "    model.eval()\n",
    "    tracker = MetricsTracker()\n",
    "    with torch.inference_mode(), autocast(device_type='cuda' if cfg.DEVICE.type=='cuda' else 'cpu', enabled=cfg.MIXED_PRECISION):\n",
    "        for inputs, targets in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "            inputs, targets = inputs.to(cfg.DEVICE), targets.to(cfg.DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets).item() if criterion else 0\n",
    "            tracker.update(loss, outputs, targets)\n",
    "    return tracker.compute()\n",
    "\n",
    "def save_confusion_matrix(metrics, phase):\n",
    "    cm = confusion_matrix(metrics['targets'], metrics['preds'])\n",
    "    cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-9)\n",
    "    plt.figure(figsize=(8,7))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
    "                xticklabels=cfg.CLASS_NAMES, yticklabels=cfg.CLASS_NAMES)\n",
    "    plt.xlabel(\"Predicted\", fontsize=14)\n",
    "    plt.ylabel(\"True\", fontsize=14)\n",
    "    plt.title(f\"{phase.capitalize()} Confusion Matrix (Normalized)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"{cfg.OUTPUT_DIR}/{phase}_confusion_matrix_{cfg.TIMESTAMP}.png\"\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    # also save raw counts as CSV\n",
    "    np.savetxt(f\"{cfg.OUTPUT_DIR}/{phase}_confusion_matrix_raw_{cfg.TIMESTAMP}.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "\n",
    "def save_classification_report(metrics, phase):\n",
    "    report = classification_report(\n",
    "        metrics['targets'], metrics['preds'], target_names=cfg.CLASS_NAMES, digits=4\n",
    "    )\n",
    "    save_path = f\"{cfg.OUTPUT_DIR}/{phase}_report_{cfg.TIMESTAMP}.txt\"\n",
    "    with open(save_path, \"w\") as f:\n",
    "        f.write(f\"{phase} Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "        f.write(\"\\nBalanced Accuracy: {:.4f}\\n\".format(metrics['balanced_accuracy']))\n",
    "        f.write(\"AUC: {:.4f}\\n\".format(metrics.get('auc', float('nan'))))\n",
    "    print(f\"\\n{phase.capitalize()} Classification Report:\\n{report}\")\n",
    "\n",
    "\n",
    "def plot_curves(history, timestamp, output_dir):\n",
    "    plt.figure()\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/loss_curve_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/acc_curve_{timestamp}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_roc_auc(targets, probs, phase):\n",
    "    targets_bin = label_binarize(targets, classes=list(range(cfg.NUM_CLASSES)))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, class_name in enumerate(cfg.CLASS_NAMES):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(targets_bin[:, i], probs[:, i])\n",
    "            auc_ = roc_auc_score(targets_bin[:, i], probs[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"{class_name} (AUC={auc_:.2f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"ROC Curve failed for class {class_name}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - {phase.capitalize()}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{cfg.OUTPUT_DIR}/{phase}_roc_{cfg.TIMESTAMP}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def tta_distribution_plot(model, dataset):\n",
    "    tta_transforms = PathologyAugment.get_tta_transforms()\n",
    "    loader = DataLoader(dataset, batch_size=VIS_BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    fig, axs = plt.subplots(len(tta_transforms), 1, figsize=(8, 4*len(tta_transforms)))\n",
    "    with torch.inference_mode():\n",
    "        for idx, tta_tf in enumerate(tta_transforms):\n",
    "            dataset.transform = tta_tf\n",
    "            all_probs = []\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(cfg.DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.softmax(dim=1).detach().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "            all_probs = np.concatenate(all_probs, axis=0)\n",
    "            mean_probs = all_probs.mean(axis=0)\n",
    "            axs[idx].bar(cfg.CLASS_NAMES, mean_probs)\n",
    "            axs[idx].set_title(f\"TTA Transform {idx+1}: Mean Class Probs\")\n",
    "            axs[idx].set_ylabel(\"Probability\")\n",
    "            axs[idx].set_ylim([0, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{cfg.OUTPUT_DIR}/tta_mean_probs_{cfg.TIMESTAMP}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_tta_variance(model, dataset):\n",
    "    tta_transforms = PathologyAugment.get_tta_transforms()\n",
    "    loader = DataLoader(dataset, batch_size=VIS_BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    all_probs = []\n",
    "    with torch.inference_mode():\n",
    "        for tta_tf in tta_transforms:\n",
    "            dataset.transform = tta_tf\n",
    "            probs_run = []\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(cfg.DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.softmax(dim=1).detach().cpu().numpy()\n",
    "                probs_run.append(probs)\n",
    "            all_probs.append(np.concatenate(probs_run, axis=0))\n",
    "    all_probs = np.stack(all_probs, axis=0)\n",
    "    max_class_idx = all_probs.mean(axis=0).argmax(axis=1)\n",
    "    tta_var = []\n",
    "    for i, idx in enumerate(max_class_idx):\n",
    "        tta_var.append(np.var(all_probs[:, i, idx]))\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(tta_var, bins=30)\n",
    "    plt.title('Variance in Predicted Probability (Most Confident Class) Across TTA')\n",
    "    plt.xlabel('Variance')\n",
    "    plt.ylabel('Num Samples')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{cfg.OUTPUT_DIR}/tta_variance_hist_{cfg.TIMESTAMP}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def show_tta_flip_examples(model, dataset, num_examples=6):\n",
    "    tta_transforms = PathologyAugment.get_tta_transforms()\n",
    "    loader = DataLoader(dataset, batch_size=VIS_BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Base\n",
    "        dataset.transform = tta_transforms[0]\n",
    "        base_probs = []\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(cfg.DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probs = outputs.softmax(dim=1).detach().cpu().numpy()\n",
    "            base_probs.append(probs)\n",
    "        base_probs = np.concatenate(base_probs, axis=0)\n",
    "        base_preds = np.argmax(base_probs, axis=1)\n",
    "\n",
    "        # All TTA runs\n",
    "        all_probs = []\n",
    "        for tta_tf in tta_transforms:\n",
    "            dataset.transform = tta_tf\n",
    "            tta_probs = []\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(cfg.DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.softmax(dim=1).detach().cpu().numpy()\n",
    "                tta_probs.append(probs)\n",
    "            all_probs.append(np.concatenate(tta_probs, axis=0))\n",
    "\n",
    "    mean_probs = np.mean(np.stack(all_probs, axis=0), axis=0)\n",
    "    tta_preds = np.argmax(mean_probs, axis=1)\n",
    "    changed = np.where(base_preds != tta_preds)[0]\n",
    "    if len(changed) == 0:\n",
    "        print(\"No TTA flip examples found!\")\n",
    "        return\n",
    "    sample_idxs = np.random.choice(changed, size=min(num_examples, len(changed)), replace=False)\n",
    "    plt.figure(figsize=(15, 3 * len(sample_idxs)))\n",
    "    for i, idx in enumerate(sample_idxs):\n",
    "        img_path, true_label = dataset.samples[idx]\n",
    "        img = plt.imread(img_path)\n",
    "        plt.subplot(len(sample_idxs), 1, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {cfg.CLASS_NAMES[true_label]}, Base Pred: {cfg.CLASS_NAMES[base_preds[idx]]}, TTA Pred: {cfg.CLASS_NAMES[tta_preds[idx]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{cfg.OUTPUT_DIR}/tta_flip_examples_{cfg.TIMESTAMP}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --------- Grad-CAM (Swin-safe) ----------\n",
    "\n",
    "def gradcam_for_all_classes(model, dataset, output_dir, timestamp):\n",
    "    class2idx = {cls: [] for cls in range(cfg.NUM_CLASSES)}\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class2idx[label].append(idx)\n",
    "\n",
    "    # choose robust target layer for timm SwinV2\n",
    "    try:\n",
    "        target_layers = [model.stages[-1].blocks[-1].norm2]\n",
    "    except Exception:\n",
    "        # fallback\n",
    "        target_layers = [getattr(model, 'norm', None) or list(model.modules())[-1]]\n",
    "\n",
    "    model.eval()\n",
    "    # Grad-CAM needs gradients; enable grad context (do NOT use inference_mode here)\n",
    "    with torch.enable_grad():\n",
    "        for cls in range(cfg.NUM_CLASSES):\n",
    "            if len(class2idx[cls]) == 0:\n",
    "                continue\n",
    "            idx = random.choice(class2idx[cls])\n",
    "            img_path, _ = dataset.samples[idx]\n",
    "            img = cv2.imread(img_path)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img_disp = cv2.resize(img_rgb, (cfg.IMG_SIZE, cfg.IMG_SIZE)) / 255.0\n",
    "            img_pil = Image.fromarray(img_rgb)\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize(int(cfg.IMG_SIZE*1.15)),\n",
    "                transforms.CenterCrop(cfg.IMG_SIZE),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "            ])\n",
    "            img_tensor = transform(img_pil).unsqueeze(0).to(cfg.DEVICE)\n",
    "            img_tensor.requires_grad_(True)\n",
    "            model.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type='cuda' if cfg.DEVICE.type=='cuda' else 'cpu', enabled=False):\n",
    "                cam = GradCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform)\n",
    "                grayscale_cam = cam(input_tensor=img_tensor, targets=[ClassifierOutputTarget(cls)])\n",
    "            del cam\n",
    "            cam_image = show_cam_on_image(img_disp.astype(np.float32), grayscale_cam[0], use_rgb=True)\n",
    "            plt.imshow(cam_image)\n",
    "            plt.title(f'Grad-CAM for class: {cfg.CLASS_NAMES[cls]}')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            fname = f\"{output_dir}/gradcam_{cfg.CLASS_NAMES[cls]}_{timestamp}.png\"\n",
    "            plt.savefig(fname, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "            print(f\"Saved Grad-CAM for class {cfg.CLASS_NAMES[cls]}: {fname}\")\n",
    "\n",
    "\n",
    "# --------- Train/Eval loops ----------\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, scaler, mixup_fn, contrastive_loss, epoch, use_contrastive):\n",
    "    model.train()\n",
    "    tracker = MetricsTracker()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.EPOCHS}\", dynamic_ncols=True)):\n",
    "        inputs = inputs.to(cfg.DEVICE)\n",
    "        orig_targets = targets.to(cfg.DEVICE)\n",
    "\n",
    "        if mixup_fn is not None:\n",
    "            inputs, mixed_targets = mixup_fn(inputs, orig_targets)\n",
    "        else:\n",
    "            mixed_targets = orig_targets\n",
    "\n",
    "        with autocast(device_type='cuda' if cfg.DEVICE.type=='cuda' else 'cpu', enabled=cfg.MIXED_PRECISION):\n",
    "            outputs = model(inputs)\n",
    "            ce_loss = criterion(outputs, mixed_targets)\n",
    "\n",
    "            # features for contrastive (no pooling => pool if needed)\n",
    "            if use_contrastive:\n",
    "                features = model.forward_features(inputs)\n",
    "                if isinstance(features, (tuple, list)):\n",
    "                    features = features[0]\n",
    "                if features.dim() > 2:\n",
    "                    features = features.mean(dim=(2, 3))\n",
    "                con_loss = contrastive_loss(features, orig_targets)\n",
    "            else:\n",
    "                con_loss = torch.tensor(0.0, device=cfg.DEVICE)\n",
    "\n",
    "            loss = (1 - cfg.CONTRASTIVE_LOSS_WEIGHT) * ce_loss + cfg.CONTRASTIVE_LOSS_WEIGHT * con_loss\n",
    "            # correct scaling for grad accumulation\n",
    "            loss = loss / cfg.ACCUM_STEPS\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % cfg.ACCUM_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if (step + 1) % cfg.ACCUM_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.GRAD_CLIP)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Track using ORIGINAL targets (hard labels)\n",
    "        tracker.update((loss.item() * cfg.ACCUM_STEPS), outputs, orig_targets)\n",
    "\n",
    "    return tracker.compute()\n",
    "\n",
    "\n",
    "def tta_predict(model, dataset):\n",
    "    tta_transforms = PathologyAugment.get_tta_transforms()\n",
    "    loader = DataLoader(dataset, batch_size=VIS_BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
    "    all_probs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for tta_tf in tta_transforms:\n",
    "            dataset.transform = tta_tf\n",
    "            probs_run = []\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(cfg.DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                probs = outputs.softmax(dim=1).detach().cpu().numpy()\n",
    "                probs_run.append(probs)\n",
    "            all_probs.append(np.concatenate(probs_run, axis=0))\n",
    "    avg_probs = np.mean(np.stack(all_probs, axis=0), axis=0)\n",
    "    targets = np.array([label for _, label in dataset.samples])\n",
    "    preds = np.argmax(avg_probs, axis=1)\n",
    "    return avg_probs, preds, targets\n",
    "\n",
    "\n",
    "# === OOM-SAFE MAIN ===\n",
    "\n",
    "def main():\n",
    "    cfg.logger.info(f\"Starting Swin Transformer training at {cfg.TIMESTAMP}\")\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = datasets.ImageFolder(cfg.DATA_PATHS['train'], PathologyAugment.get_train_transform())\n",
    "    val_ds   = datasets.ImageFolder(cfg.DATA_PATHS['val'],   PathologyAugment.get_test_transform())\n",
    "    test_ds  = datasets.ImageFolder(cfg.DATA_PATHS['test'],  PathologyAugment.get_test_transform())\n",
    "\n",
    "    # Log dataset sizes for reproducibility\n",
    "    cfg.logger.info(f\"Dataset sizes | train: {len(train_ds)} | val: {len(val_ds)} | test: {len(test_ds)}\")\n",
    "\n",
    "    # Sampler and Loaders\n",
    "    sampler = create_weighted_sampler(train_ds)\n",
    "    train_loader = DataLoader(train_ds, batch_size=cfg.TRAIN_BATCH_SIZE, sampler=sampler,\n",
    "                              num_workers=cfg.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=cfg.VAL_BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=cfg.VAL_BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # Model & Optimizer\n",
    "    model = create_model()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY)\n",
    "\n",
    "    # Losses\n",
    "    if cfg.USE_MIXUP:\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=cfg.MIXUP_ALPHA, cutmix_alpha=cfg.CUTMIX_ALPHA,\n",
    "            label_smoothing=0.0, num_classes=cfg.NUM_CLASSES\n",
    "        )\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)\n",
    "        mixup_fn = None\n",
    "\n",
    "    val_criterion = nn.CrossEntropyLoss()  # hard labels for val/test\n",
    "    contrastive_loss = SupConLoss(temperature=0.07)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.EPOCHS)\n",
    "    scaler = GradScaler(device='cuda', enabled=(cfg.MIXED_PRECISION and cfg.DEVICE.type == 'cuda'))\n",
    "\n",
    "    # Disable contrastive if mixup is on (consistency)\n",
    "    use_contrastive = (cfg.CONTRASTIVE_LOSS_WEIGHT > 0) and (not cfg.USE_MIXUP)\n",
    "\n",
    "    # History & logging helpers\n",
    "    history = {'train_loss':[], 'val_loss':[], 'train_acc':[], 'val_acc':[]}\n",
    "    best_auc = -np.inf\n",
    "    best_epoch = -1\n",
    "    metrics_csv = os.path.join(cfg.OUTPUT_DIR, f\"metrics_{cfg.TIMESTAMP}.csv\")\n",
    "    with open(metrics_csv, 'w') as f:\n",
    "        f.write(\"epoch,train_loss,train_acc,val_loss,val_acc,val_auc\\n\")\n",
    "\n",
    "    # Train loop\n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        train_metrics = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, scaler, mixup_fn, contrastive_loss, epoch, use_contrastive\n",
    "        )\n",
    "        val_metrics = evaluate(model, val_loader, val_criterion)\n",
    "\n",
    "        cfg.logger.info(\n",
    "            f\"Epoch {epoch+1} Train | Loss: {train_metrics['loss']:.4f} | Acc: {train_metrics['accuracy']:.4f} | AUC: {train_metrics['auc']:.4f}\"\n",
    "        )\n",
    "        cfg.logger.info(\n",
    "            f\"Epoch {epoch+1} Val   | Loss: {val_metrics['loss']:.4f} | Acc: {val_metrics['accuracy']:.4f} | AUC: {val_metrics['auc']:.4f}\"\n",
    "        )\n",
    "\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['train_acc'].append(train_metrics['accuracy'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "\n",
    "        with open(metrics_csv, 'a') as f:\n",
    "            f.write(f\"{epoch+1},{train_metrics['loss']:.6f},{train_metrics['accuracy']:.6f},\"\n",
    "                    f\"{val_metrics['loss']:.6f},{val_metrics['accuracy']:.6f},{val_metrics['auc']:.6f}\\n\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_metrics['auc'] > best_auc:\n",
    "            best_auc = val_metrics['auc']\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_auc': best_auc,\n",
    "                'config': config_to_serializable_dict(cfg)\n",
    "            }, cfg.MODEL_SAVE)\n",
    "            # Also save a pure state_dict for safe loading with weights_only=True\n",
    "            torch.save(model.state_dict(), cfg.MODEL_SAVE.replace('.pth', '_weights.pth'))\n",
    "            cfg.logger.info(f\"Saved best model with AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve = (no_improve + 1) if 'no_improve' in locals() else 1\n",
    "            if no_improve >= cfg.EARLY_STOP_PATIENCE:\n",
    "                cfg.logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    plot_curves(history, cfg.TIMESTAMP, cfg.OUTPUT_DIR)\n",
    "    cfg.logger.info(f\"Best Val AUC: {best_auc:.4f} (epoch {best_epoch}) | checkpoint: {cfg.MODEL_SAVE}\")\n",
    "\n",
    "    # Load best weights\n",
    "    state = torch.load(cfg.MODEL_SAVE, map_location=cfg.DEVICE, weights_only=False)\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    # ----- Test & TTA -----\n",
    "    if cfg.USE_TTA:\n",
    "        avg_probs, preds, targets = tta_predict(model, test_ds)\n",
    "        f1 = f1_score(targets, preds, average='macro')\n",
    "        try:\n",
    "            auc_ = roc_auc_score(targets, avg_probs, multi_class='ovo')\n",
    "        except Exception:\n",
    "            auc_ = float('nan')\n",
    "        print(f\"\\nTest (TTA) F1 Macro: {f1:.4f} | AUC: {auc_:.4f}\")\n",
    "        print(classification_report(targets, preds, target_names=cfg.CLASS_NAMES))\n",
    "        save_confusion_matrix({'targets': targets, 'preds': preds}, \"test_tta\")\n",
    "        save_classification_report({'targets': targets, 'preds': preds, 'balanced_accuracy': balanced_accuracy_score(targets, preds), 'auc': auc_}, \"test_tta\")\n",
    "        plot_roc_auc(targets, avg_probs, phase=\"test_tta\")\n",
    "\n",
    "        subset_size = min(200, len(test_ds))\n",
    "        subset_ds = torch.utils.data.Subset(test_ds, range(subset_size))\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        tta_distribution_plot(model, subset_ds)\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        plot_tta_variance(model, subset_ds)\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        show_tta_flip_examples(model, subset_ds)\n",
    "    else:\n",
    "        test_metrics = evaluate(model, test_loader, val_criterion)\n",
    "        save_confusion_matrix(test_metrics, \"test\")\n",
    "        save_classification_report(test_metrics, \"test\")\n",
    "        plot_roc_auc(test_metrics['targets'], test_metrics['probs'], phase=\"test\")\n",
    "        print(f\"\\nFinal Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"AUC: {test_metrics['auc']:.4f}\")\n",
    "\n",
    "    # Cleanup & Grad-CAM visualizations\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    gradcam_for_all_classes(model, test_ds, cfg.OUTPUT_DIR, cfg.TIMESTAMP)\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-27T04:28:21.488747Z",
     "iopub.status.busy": "2025-08-27T04:28:21.488427Z",
     "iopub.status.idle": "2025-08-27T05:15:51.039397Z",
     "shell.execute_reply": "2025-08-27T05:15:51.038391Z",
     "shell.execute_reply.started": "2025-08-27T04:28:21.488723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "==== Running: NoNoise ====\n",
      "Dataset sizes | train: 473 | val: 99 | test: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Acc 0.5660 | Val Acc 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: Train Acc 0.8098 | Val Acc 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: Train Acc 0.7879 | Val Acc 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: Train Acc 0.8190 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: Train Acc 0.8151 | Val Acc 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: Train Acc 0.7755 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: Train Acc 0.8630 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: Train Acc 0.8404 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: Train Acc 0.8777 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc 0.8187 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc 0.8741 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc 0.8710 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc 0.8945 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc 0.8876 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc 0.8677 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Acc 0.9251 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc 0.8602 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc 0.9197 | Val Acc 0.9697\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n",
      "F1 Macro: 1.0000\n",
      "ROC-AUC (macro): 1.0000\n",
      "ECE: 6.29%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      1.00      1.00        37\n",
      "Healthy Leaf       1.00      1.00      1.00        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00        99\n",
      "   macro avg       1.00      1.00      1.00        99\n",
      "weighted avg       1.00      1.00      1.00        99\n",
      "\n",
      "Intermediate results saved to ablation_results.csv\n",
      "\n",
      "==== Running: GaussianOnly ====\n",
      "Dataset sizes | train: 473 | val: 99 | test: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Acc 0.5723 | Val Acc 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: Train Acc 0.7400 | Val Acc 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: Train Acc 0.7705 | Val Acc 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: Train Acc 0.7840 | Val Acc 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: Train Acc 0.8374 | Val Acc 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: Train Acc 0.8745 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: Train Acc 0.8705 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: Train Acc 0.8707 | Val Acc 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: Train Acc 0.8703 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc 0.8378 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc 0.8634 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc 0.9036 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc 0.8973 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc 0.8840 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc 0.8869 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Acc 0.8753 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc 0.8944 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc 0.8944 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Acc 0.8996 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc 0.8721 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Acc 0.8409 | Val Acc 0.9899\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n",
      "F1 Macro: 1.0000\n",
      "ROC-AUC (macro): 1.0000\n",
      "ECE: 9.77%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      1.00      1.00        37\n",
      "Healthy Leaf       1.00      1.00      1.00        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           1.00        99\n",
      "   macro avg       1.00      1.00      1.00        99\n",
      "weighted avg       1.00      1.00      1.00        99\n",
      "\n",
      "Intermediate results saved to ablation_results.csv\n",
      "\n",
      "==== Running: SaltPepperOnly ====\n",
      "Dataset sizes | train: 473 | val: 99 | test: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Acc 0.5999 | Val Acc 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: Train Acc 0.7921 | Val Acc 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: Train Acc 0.7635 | Val Acc 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: Train Acc 0.7956 | Val Acc 0.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: Train Acc 0.8132 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: Train Acc 0.7281 | Val Acc 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: Train Acc 0.8716 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: Train Acc 0.8808 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: Train Acc 0.8485 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc 0.8868 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc 0.8314 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc 0.8180 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc 0.8302 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc 0.8861 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc 0.8782 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Acc 0.8901 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc 0.8914 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc 0.8756 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Acc 0.9091 | Val Acc 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc 0.8815 | Val Acc 0.9798\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9899\n",
      "F1 Macro: 0.9901\n",
      "ROC-AUC (macro): 1.0000\n",
      "ECE: 4.45%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.97      0.99        37\n",
      "Healthy Leaf       0.97      1.00      0.98        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.99        99\n",
      "   macro avg       0.99      0.99      0.99        99\n",
      "weighted avg       0.99      0.99      0.99        99\n",
      "\n",
      "Intermediate results saved to ablation_results.csv\n",
      "\n",
      "==== Running: BothNoises ====\n",
      "Dataset sizes | train: 473 | val: 99 | test: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Acc 0.4968 | Val Acc 0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: Train Acc 0.7163 | Val Acc 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: Train Acc 0.7636 | Val Acc 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: Train Acc 0.7415 | Val Acc 0.8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: Train Acc 0.8149 | Val Acc 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: Train Acc 0.8243 | Val Acc 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: Train Acc 0.8070 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: Train Acc 0.8738 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: Train Acc 0.8362 | Val Acc 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Acc 0.9234 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Acc 0.8202 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Acc 0.8946 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Acc 0.9090 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Acc 0.8688 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Acc 0.8992 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Acc 0.8755 | Val Acc 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Acc 0.8657 | Val Acc 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Acc 0.8520 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Acc 0.9096 | Val Acc 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Acc 0.8760 | Val Acc 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Acc 0.8925 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Acc 0.8948 | Val Acc 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Acc 0.8693 | Val Acc 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Acc 0.9141 | Val Acc 0.9596\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9798\n",
      "F1 Macro: 0.9803\n",
      "ROC-AUC (macro): 1.0000\n",
      "ECE: 6.69%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.95      0.97        37\n",
      "Healthy Leaf       0.94      1.00      0.97        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.98        99\n",
      "   macro avg       0.98      0.98      0.98        99\n",
      "weighted avg       0.98      0.98      0.98        99\n",
      "\n",
      "Intermediate results saved to ablation_results.csv\n",
      "\n",
      "All ablation runs complete.\n",
      "             name  test_acc  f1_macro  roc_auc_macro       ece use_gaussian  \\\n",
      "0         NoNoise  1.000000  1.000000            1.0  6.290144          NaN   \n",
      "1    GaussianOnly  1.000000  1.000000            1.0  9.767211         True   \n",
      "2  SaltPepperOnly  0.989899  0.990143            1.0  4.445658          NaN   \n",
      "3      BothNoises  0.979798  0.980324            1.0  6.691181         True   \n",
      "\n",
      "   gaussian_std use_saltpepper  salt_prob  pepper_prob  \n",
      "0           NaN            NaN        NaN          NaN  \n",
      "1          0.05            NaN        NaN          NaN  \n",
      "2           NaN           True       0.01         0.01  \n",
      "3          0.05           True       0.01         0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    }
   ],
   "source": [
    "import os, random, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from timm import create_model\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Modern AMP API\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# ----------- Reproducibility ----------\n",
    "def seed_all(seed=42, deterministic=True):\n",
    "    torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = deterministic\n",
    "    torch.backends.cudnn.benchmark = not deterministic\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "# ----------- Custom Noise -------------\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.05): self.mean = float(mean); self.std = float(std)\n",
    "    def __call__(self, tensor): return tensor + torch.randn_like(tensor) * self.std + self.mean\n",
    "\n",
    "class AddSaltPepperNoise(object):\n",
    "    def __init__(self, salt_prob=0.01, pepper_prob=0.01):\n",
    "        self.salt_prob = float(salt_prob); self.pepper_prob = float(pepper_prob)\n",
    "    def __call__(self, tensor):\n",
    "        c, h, w = tensor.shape\n",
    "        mask = torch.rand((h, w), device=tensor.device)\n",
    "        salt = (mask < self.salt_prob).float(); pepper = (mask > 1 - self.pepper_prob).float()\n",
    "        for i in range(c):\n",
    "            tensor[i] = tensor[i] * (1 - salt - pepper) + salt + 0.0 * pepper\n",
    "        return tensor\n",
    "\n",
    "# ----------- Mixup --------------------\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha and alpha > 0 else 1.0\n",
    "    b = x.size(0)\n",
    "    index = torch.randperm(b, device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, float(lam)\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ----------- Supervised Contrastive Loss -------------\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07): super().__init__(); self.temperature = temperature\n",
    "    def forward(self, features, labels):\n",
    "        z = nn.functional.normalize(features, dim=1)\n",
    "        sim = torch.matmul(z, z.T) / self.temperature\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(z.device)\n",
    "        logits_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
    "        logits = sim - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(labels.shape[0], device=z.device))\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1).clamp(min=1)\n",
    "        return -mean_log_prob_pos.mean()\n",
    "\n",
    "# ----------- Model with Progressive Unfreezing --------\n",
    "class FineTuneSWIN(nn.Module):\n",
    "    def __init__(self, model_name=\"swinv2_small_window16_256\", num_classes=3, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        in_features = self.backbone.num_features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        return self.classifier(feats)\n",
    "    def forward_features(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# ----------- Data Loader -------------\n",
    "def get_loaders(batch_size=32, noise_cfg=None, img_size=256, data_root=\"/kaggle/input/minida/mini_output1\"):\n",
    "    train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "\n",
    "    tfms = [\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "    if noise_cfg:\n",
    "        if noise_cfg.get('use_gaussian'): tfms.append(AddGaussianNoise(std=float(noise_cfg.get('gaussian_std', 0.05))))\n",
    "        if noise_cfg.get('use_saltpepper'):\n",
    "            tfms.append(AddSaltPepperNoise(salt_prob=float(noise_cfg.get('salt_prob', 0.01)), pepper_prob=float(noise_cfg.get('pepper_prob', 0.01))))\n",
    "    tfms.append(transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    train_transform = transforms.Compose(tfms)\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(int(img_size * 1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds   = ImageFolder(val_dir,   val_transform)\n",
    "    test_ds  = ImageFolder(test_dir,  val_transform)\n",
    "\n",
    "    counts = np.bincount(np.array(train_ds.targets), minlength=len(train_ds.classes))\n",
    "    counts[counts == 0] = 1\n",
    "    weights_per_class = 1.0 / counts\n",
    "    weights = weights_per_class[np.array(train_ds.targets)]\n",
    "    sampler = WeightedRandomSampler(torch.as_tensor(weights, dtype=torch.double), num_samples=len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, train_ds.classes, train_ds, val_ds, test_ds\n",
    "\n",
    "# ----------- Temperature Scaling -----------\n",
    "class _TempScaleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.T = nn.Parameter(torch.ones(()))\n",
    "    def forward(self, logits):\n",
    "        return logits / self.T.clamp_min(1e-3)\n",
    "\n",
    "def _gather_logits_labels(model, loader, device):\n",
    "    model.eval(); all_logits=[]; all_labels=[]\n",
    "    with torch.inference_mode(), autocast(device_type='cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')):\n",
    "        for x, y in loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            logits = model(x)\n",
    "            all_logits.append(logits.detach().float().cpu())\n",
    "            all_labels.append(y.detach().cpu())\n",
    "    return torch.cat(all_logits), torch.cat(all_labels)\n",
    "\n",
    "def fit_temperature(model, val_loader, device):\n",
    "    logits, labels = _gather_logits_labels(model, val_loader, device)\n",
    "    Tmod = _TempScaleModule()\n",
    "    Tmod.to(device)\n",
    "    # LBFGS tends to work well for 1D; fall back to Adam if needed\n",
    "    optimizer = torch.optim.LBFGS(Tmod.parameters(), lr=0.1, max_iter=50)\n",
    "    nll = nn.CrossEntropyLoss()\n",
    "\n",
    "    logits = logits.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll(Tmod(logits), labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        optimizer.step(closure)\n",
    "    except Exception:\n",
    "        opt = torch.optim.Adam(Tmod.parameters(), lr=0.01)\n",
    "        for _ in range(200):\n",
    "            opt.zero_grad(); loss = nll(Tmod(logits), labels); loss.backward(); opt.step()\n",
    "\n",
    "    T_value = float(Tmod.T.detach().cpu().item())\n",
    "    return T_value\n",
    "\n",
    "# ----------- Training & Eval Loop (Hybrid Loss, Mixup, AMP) -------------\n",
    "def train_epoch(model, loader, ce_criterion, con_criterion, optimizer, scaler, use_mixup, supcon_weight=0.3, device=torch.device(\"cpu\")):\n",
    "    use_contrastive = (supcon_weight > 0) and (not use_mixup)\n",
    "    model.train(); total_loss=0.0; correct=0.0\n",
    "    for imgs, labels in tqdm(loader, desc='Train', leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=0.2)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type='cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')):\n",
    "            outputs = model(imgs)\n",
    "            ce_loss = mixup_criterion(ce_criterion, outputs, y_a, y_b, lam) if use_mixup else ce_criterion(outputs, labels)\n",
    "            if use_contrastive:\n",
    "                feats = model.forward_features(imgs)\n",
    "                if isinstance(feats, (tuple, list)): feats = feats[0]\n",
    "                con_loss = con_criterion(feats, labels)\n",
    "            else:\n",
    "                con_loss = torch.tensor(0.0, device=device)\n",
    "            loss = (1 - supcon_weight) * ce_loss + supcon_weight * con_loss\n",
    "        if device.type == 'cuda':\n",
    "            scaler.scale(loss).backward(); scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        if use_mixup:\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            correct += preds.eq(labels).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, ce_criterion, device=torch.device(\"cpu\"), temperature: float | None = None):\n",
    "    model.eval(); total_loss=0.0; correct=0.0; all_labels=[]; all_probs=[]\n",
    "    with torch.inference_mode(), autocast(device_type='cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')):\n",
    "        for imgs, labels in tqdm(loader, desc='Eval', leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            if temperature is not None:\n",
    "                logits = logits / max(temperature, 1e-3)\n",
    "            probs = torch.softmax(logits.float(), dim=1)\n",
    "            loss = ce_criterion(logits, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += (logits.argmax(1) == labels).sum().item()\n",
    "            all_labels.extend(labels.detach().cpu().numpy())\n",
    "            all_probs.extend(probs.detach().cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ----------- Test-Time Augmentation (optional) -------------\n",
    "def tta_predict(model, dataset, transforms_list, batch_size=32, device=torch.device(\"cpu\"), temperature: float | None = None):\n",
    "    model.eval(); all_probs = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    with torch.inference_mode(), autocast(device_type='cuda' if device.type=='cuda' else 'cpu', enabled=(device.type=='cuda')):\n",
    "        for tfm in transforms_list:\n",
    "            dataset.transform = tfm\n",
    "            batch_probs = []\n",
    "            for imgs, _ in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                logits = model(imgs)\n",
    "                if temperature is not None:\n",
    "                    logits = logits / max(temperature, 1e-3)\n",
    "                probs = torch.softmax(logits.float(), dim=1).cpu().numpy()\n",
    "                batch_probs.append(probs)\n",
    "            all_probs.append(np.concatenate(batch_probs, axis=0))\n",
    "    avg_probs = np.mean(np.stack(all_probs, axis=0), axis=0)\n",
    "    return avg_probs\n",
    "\n",
    "# ----------- ECE and Reliability Diagram ----------\n",
    "def compute_ece(labels, probs, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    confidences = np.max(probs, axis=1); predictions = np.argmax(probs, axis=1); accuracies = predictions == labels\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
    "        if np.any(mask):\n",
    "            bin_acc = np.mean(accuracies[mask]); bin_conf = np.mean(confidences[mask])\n",
    "            ece += np.abs(bin_acc - bin_conf) * np.sum(mask) / len(probs)\n",
    "    return 100 * ece\n",
    "\n",
    "def plot_reliability_diagram(labels, probs, phase, out_dir=\".\", n_bins=10, dpi=300):\n",
    "    confidences = np.max(probs, axis=1); predictions = np.argmax(probs, axis=1); accuracies = predictions == labels\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1); bin_accs, bin_confs = [], []\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i + 1])\n",
    "        if np.any(mask):\n",
    "            bin_accs.append(np.mean(accuracies[mask])); bin_confs.append(np.mean(confidences[mask]))\n",
    "        else:\n",
    "            bin_accs.append(0.0); bin_confs.append((bins[i] + bins[i+1]) / 2)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.bar(bin_confs, bin_accs, width=0.08, align='center', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\"); plt.title(f\"Reliability Diagram - {phase}\")\n",
    "    plt.tight_layout(); save_path = os.path.join(out_dir, f\"{phase}_reliability_diagram.png\")\n",
    "    plt.savefig(save_path, dpi=dpi); plt.close()\n",
    "\n",
    "# ----------- Confusion, ROC -------------\n",
    "def plot_confusion_matrix(labels, preds, phase, class_names, out_dir=\".\", dpi=300):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-9)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"viridis\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(f\"{phase} Confusion Matrix (Normalized)\")\n",
    "    plt.tight_layout(); save_path = os.path.join(out_dir, f\"{phase}_confusion_matrix.png\")\n",
    "    plt.savefig(save_path, dpi=dpi); plt.close()\n",
    "\n",
    "def plot_roc_curve(labels, probs, phase, class_names, out_dir=\".\", dpi=300):\n",
    "    labels_onehot = np.eye(len(class_names))[labels]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, cls in enumerate(class_names):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(labels_onehot[:, i], probs[:, i])\n",
    "            auc_ = roc_auc_score(labels_onehot[:, i], probs[:, i])\n",
    "            plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc_:.2f})\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    plt.plot([0, 1], [0, 1], \"k--\"); plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve - {phase}\"); plt.legend(); plt.tight_layout(); save_path = os.path.join(out_dir, f\"{phase}_roc_curve.png\")\n",
    "    plt.savefig(save_path, dpi=dpi); plt.close()\n",
    "\n",
    "# ----------- Early Stopping ---------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = int(patience); self.counter = 0; self.best_acc = None; self.best_state = None\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = float(val_acc)\n",
    "            self.best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0; return False\n",
    "        self.counter += 1; return self.counter >= self.patience\n",
    "\n",
    "# ----------- Main Ablation Runner -------------\n",
    "def run_ablation_experiments(ablation_configs, epochs=50, batch_size=32, patience=10, supcon_weight=0.3,\n",
    "                             data_root=\"/kaggle/input/minida/mini_output1\", img_size=256, use_tempscale=True):\n",
    "    results = []\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "    seed_all()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    for cfg in ablation_configs:\n",
    "        print(f\"\\n==== Running: {cfg.get('name','exp')} ====\")\n",
    "        train_loader, val_loader, test_loader, class_names, train_ds, val_ds, test_ds = \\\n",
    "            get_loaders(batch_size, noise_cfg=cfg, img_size=img_size, data_root=data_root)\n",
    "        print(f\"Dataset sizes | train: {len(train_ds)} | val: {len(val_ds)} | test: {len(test_ds)}\")\n",
    "\n",
    "        model = FineTuneSWIN(num_classes=len(class_names)).to(device)\n",
    "        for p in model.backbone.parameters(): p.requires_grad = False\n",
    "        for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "        ce_criterion  = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "        con_criterion = SupConLoss(temperature=0.07)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        scaler = GradScaler(device='cuda', enabled=(device.type=='cuda'))\n",
    "        early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch == 5:\n",
    "                for p in model.backbone.parameters(): p.require_grad = True if hasattr(p, 'require_grad') else None\n",
    "                for p in model.backbone.parameters(): p.requires_grad = True\n",
    "\n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, ce_criterion, con_criterion, optimizer, scaler,\n",
    "                use_mixup=True, supcon_weight=supcon_weight, device=device\n",
    "            )\n",
    "            val_loss, val_acc, _, _ = eval_epoch(model, val_loader, ce_criterion, device=device)\n",
    "            print(f\"Epoch {epoch+1:02d}: Train Acc {train_acc:.4f} | Val Acc {val_acc:.4f}\")\n",
    "            if early_stopper(val_acc, model):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "        # Temperature scaling on val (optional)\n",
    "        T_value = None\n",
    "        if use_tempscale:\n",
    "            try:\n",
    "                T_value = fit_temperature(model, val_loader, device)\n",
    "                print(f\"Fitted temperature: T = {T_value:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Temperature scaling failed: {e}\")\n",
    "                T_value = None\n",
    "\n",
    "        # Test\n",
    "        test_loss, test_acc, labels, probs = eval_epoch(model, test_loader, ce_criterion, device=device, temperature=T_value)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        try:\n",
    "            labels_onehot = np.eye(len(class_names))[labels]\n",
    "            roc_macro = roc_auc_score(labels_onehot, probs, average='macro', multi_class='ovr')\n",
    "        except Exception:\n",
    "            roc_macro = None\n",
    "        f1_macro = f1_score(labels, preds, average='macro')\n",
    "        ece = compute_ece(labels, probs)\n",
    "\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "        print(f\"ROC-AUC (macro): {roc_macro:.4f}\" if roc_macro is not None else \"ROC-AUC N/A\")\n",
    "        print(f\"ECE: {ece:.2f}%\")\n",
    "        print(classification_report(labels, preds, target_names=class_names))\n",
    "\n",
    "        phase = cfg.get('name','exp')\n",
    "        plot_confusion_matrix(labels, preds, phase=phase, class_names=class_names, out_dir=\"plots\")\n",
    "        plot_roc_curve(labels, probs, phase=phase, class_names=class_names, out_dir=\"plots\")\n",
    "        plot_reliability_diagram(labels, probs, phase=phase, out_dir=\"plots\")\n",
    "\n",
    "        row = dict(cfg)\n",
    "        row.update({\"test_acc\": test_acc, \"f1_macro\": f1_macro, \"roc_auc_macro\": roc_macro, \"ece\": ece, \"T\": T_value})\n",
    "        results.append(row)\n",
    "\n",
    "        # --- NaN-safe printing/saving block ---\n",
    "        df = pd.DataFrame(results)\n",
    "        # Keep metrics numeric; convert config columns to object to avoid NaN float formatting warnings\n",
    "        for col in [\"use_gaussian\", \"use_saltpepper\", \"gaussian_std\", \"salt_prob\", \"pepper_prob\", \"T\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(\"object\")\n",
    "        with pd.option_context(\"display.float_format\", \"{:.6f}\".format):\n",
    "            print(\"\\nIntermediate results:\\n\", df.fillna(\"\"))\n",
    "        df.to_csv(\"ablation_results.csv\", index=False)\n",
    "        print(\"Intermediate results saved to ablation_results.csv\")\n",
    "\n",
    "    print(\"\\nAll ablation runs complete.\")\n",
    "    df = pd.DataFrame(results)\n",
    "    for col in [\"use_gaussian\", \"use_saltpepper\", \"gaussian_std\", \"salt_prob\", \"pepper_prob\", \"T\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(\"object\")\n",
    "    with pd.option_context(\"display.float_format\", \"{:.6f}\".format):\n",
    "        print(df.fillna(\"\"))\n",
    "    df.to_csv(\"ablation_results.csv\", index=False)\n",
    "    return results\n",
    "\n",
    "# ----------- Example configs -------------\n",
    "ablation_configs = [\n",
    "    {\"name\": \"NoNoise\"},\n",
    "    {\"name\": \"GaussianOnly\", \"use_gaussian\": True, \"gaussian_std\": 0.05},\n",
    "    {\"name\": \"SaltPepperOnly\", \"use_saltpepper\": True, \"salt_prob\": 0.01, \"pepper_prob\": 0.01},\n",
    "    {\"name\": \"BothNoises\", \"use_gaussian\": True, \"gaussian_std\": 0.05, \"use_saltpepper\": True, \"salt_prob\": 0.01, \"pepper_prob\": 0.01},\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    _ = run_ablation_experiments(ablation_configs, epochs=50, batch_size=32, patience=10, supcon_weight=0.3)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
