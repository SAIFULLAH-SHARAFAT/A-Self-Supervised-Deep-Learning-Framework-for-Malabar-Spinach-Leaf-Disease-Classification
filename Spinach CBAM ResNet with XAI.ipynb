{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBAM SIMSIAM RESNET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T23:52:54.248006Z",
     "iopub.status.busy": "2025-07-05T23:52:54.247226Z",
     "iopub.status.idle": "2025-07-05T23:52:54.254118Z",
     "shell.execute_reply": "2025-07-05T23:52:54.253343Z",
     "shell.execute_reply.started": "2025-07-05T23:52:54.247980Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cbam_resnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cbam_resnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(x_cat)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAMBottleneck(Bottleneck):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        planes = self.conv3.out_channels\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention()\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "def cbam_resnet50(**kwargs):\n",
    "    model = ResNet(CBAMBottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T00:04:18.547649Z",
     "iopub.status.busy": "2025-07-06T00:04:18.547051Z",
     "iopub.status.idle": "2025-07-06T00:04:18.555266Z",
     "shell.execute_reply": "2025-07-06T00:04:18.554282Z",
     "shell.execute_reply.started": "2025-07-06T00:04:18.547619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simsiam_cbam_pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simsiam_cbam_pretrain.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from PIL import Image\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "# --- Seed for reproducibility ---\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "\n",
    "# --- SimSiam Model ---\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=2048, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim, bias=False),\n",
    "            nn.BatchNorm1d(out_dim, affine=False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hidden_dim=512, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.projector = MLPHead()\n",
    "        self.predictor = PredictionHead()\n",
    "        for m in self.backbone.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "                m.requires_grad_(False)\n",
    "    def _forward_backbone(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return torch.flatten(x, 1)\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.projector(self._forward_backbone(x1))\n",
    "        z2 = self.projector(self._forward_backbone(x2))\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "\n",
    "# --- Dataset ---\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.filepaths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir)\n",
    "                          if fname.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filepaths[idx]).convert('RGB')\n",
    "        return self.transform(img), self.transform(img)\n",
    "\n",
    "# --- Loss ---\n",
    "def negative_cosine_similarity(p, z):\n",
    "    p = nn.functional.normalize(p, dim=1)\n",
    "    z = nn.functional.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "# --- Pretraining function ---\n",
    "def pretrain(root_path=\"/kaggle/input/minida/mini_output1/pretrain\",\n",
    "             checkpoint_dir=\"/kaggle/working/\",\n",
    "             epochs=150, batch_size=32, accumulation_steps=4):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=os.path.join(checkpoint_dir, \"logs_cbam\"))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = UnlabeledDataset(root_path, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    model = SimSiam().to(device)\n",
    "    base_lr = 0.05 * batch_size / 256\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for idx, (x1, x2) in enumerate(dataloader):\n",
    "            x1, x2 = x1.to(device), x2.to(device)\n",
    "            p1, p2, z1, z2 = model(x1, x2)\n",
    "            loss = 0.5 * (negative_cosine_similarity(p1, z2) + negative_cosine_similarity(p2, z1))\n",
    "            loss.backward()\n",
    "            if (idx + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        scheduler.step()\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"simsiam_cbam_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"simsiam_cbam_pretrained_final.pth\"))\n",
    "    print(\"Pretraining complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pretrain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python simsiam_cbam_pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CBAM-ResNet fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T22:42:34.794601Z",
     "iopub.status.busy": "2025-07-06T22:42:34.793908Z",
     "iopub.status.idle": "2025-07-06T23:07:10.805318Z",
     "shell.execute_reply": "2025-07-06T23:07:10.804368Z",
     "shell.execute_reply.started": "2025-07-06T22:42:34.794581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 1.0797, Acc 0.4428 | Val Loss 1.1021, Acc 0.3131\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 1.0514, Acc 0.4315 | Val Loss 1.1091, Acc 0.3131\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 1.0452, Acc 0.4233 | Val Loss 1.0132, Acc 0.4747\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 1.0478, Acc 0.4563 | Val Loss 0.9460, Acc 0.6869\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 1.0646, Acc 0.4371 | Val Loss 0.9650, Acc 0.6162\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 1.0260, Acc 0.4961 | Val Loss 0.8190, Acc 0.6869\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.9878, Acc 0.5293 | Val Loss 0.7635, Acc 0.7273\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.9395, Acc 0.5544 | Val Loss 0.5978, Acc 0.8384\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.9318, Acc 0.5686 | Val Loss 0.5133, Acc 0.8788\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.8408, Acc 0.6399 | Val Loss 0.4725, Acc 0.8384\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.8235, Acc 0.6594 | Val Loss 0.5732, Acc 0.7778\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.8280, Acc 0.6499 | Val Loss 0.4551, Acc 0.8485\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.8032, Acc 0.6788 | Val Loss 0.4415, Acc 0.8889\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.7189, Acc 0.7159 | Val Loss 0.4325, Acc 0.8889\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 0.8371, Acc 0.6578 | Val Loss 0.4500, Acc 0.8990\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 0.7392, Acc 0.7146 | Val Loss 0.4594, Acc 0.8182\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 0.8212, Acc 0.6590 | Val Loss 0.4256, Acc 0.8990\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 0.7327, Acc 0.7387 | Val Loss 0.4640, Acc 0.8283\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 0.7776, Acc 0.6827 | Val Loss 0.3873, Acc 0.9091\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 0.7947, Acc 0.7034 | Val Loss 0.3819, Acc 0.9293\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 0.6738, Acc 0.7706 | Val Loss 0.4699, Acc 0.8485\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 0.7487, Acc 0.7139 | Val Loss 0.3828, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 0.7398, Acc 0.7155 | Val Loss 0.4218, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 0.7984, Acc 0.6800 | Val Loss 0.3936, Acc 0.9091\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 0.7769, Acc 0.6762 | Val Loss 0.4109, Acc 0.9091\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 0.7512, Acc 0.7150 | Val Loss 0.4405, Acc 0.8788\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 0.7297, Acc 0.7033 | Val Loss 0.3844, Acc 0.8788\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 0.6994, Acc 0.7555 | Val Loss 0.3401, Acc 0.9495\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 0.7122, Acc 0.7373 | Val Loss 0.3407, Acc 0.9394\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss 0.7144, Acc 0.7273 | Val Loss 0.3748, Acc 0.9091\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss 0.6796, Acc 0.7512 | Val Loss 0.3562, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss 0.7132, Acc 0.7086 | Val Loss 0.3366, Acc 0.9293\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss 0.6956, Acc 0.7399 | Val Loss 0.3485, Acc 0.9192\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss 0.6700, Acc 0.7500 | Val Loss 0.3634, Acc 0.9091\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss 0.7068, Acc 0.7411 | Val Loss 0.3515, Acc 0.9192\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss 0.6281, Acc 0.7872 | Val Loss 0.3346, Acc 0.9293\n",
      "EarlyStopping counter: 8 / 8\n",
      "Early stopping at epoch 36\n",
      "\n",
      "Test set results (CBAM):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2851, Test Acc: 0.9697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.92      0.96        37\n",
      "Healthy Leaf       0.91      1.00      0.95        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.97      0.97      0.97        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n",
      "Test ROC-AUC (macro): 0.9982\n",
      "\n",
      "Test-Time Augmentation (TTA) Evaluation (CBAM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       0.97      0.95      0.96        37\n",
      "Healthy Leaf       0.94      1.00      0.97        31\n",
      "  straw_mite       1.00      0.97      0.98        31\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.97      0.97      0.97        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "from cbam_resnet import cbam_resnet50  # Make sure this is in your path\n",
    "\n",
    "# ---------- SEED & DEVICE ----------\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- PATHS & CLASSES ----------\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- MIXUP ----------\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ---------- DATALOADERS ----------\n",
    "def get_loaders(batch_size=32):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, test_ds\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ---------- LR SCHEDULER ----------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------- TRAINING & EVAL ----------\n",
    "def train_epoch(model, loader, criterion, optimizer, use_mixup=True, mixup_alpha=0.3):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "            outputs = model(imgs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ---------- EARLY STOPPING ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.verbose = verbose\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            if self.verbose: print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ---------- PLOTTING ----------\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix (CBAM)')\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_per_class(y_true, y_score, n_classes, class_names, save_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(n_classes):\n",
    "        if np.sum(y_true==i) == 0: continue\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve((y_true==i).astype(int), y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"ROC error for class {class_names[i]}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Per-class ROC Curves (CBAM)')\n",
    "    plt.legend()\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_reliability(y_true, y_prob, n_bins=10):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i, name in enumerate(class_names):\n",
    "        try:\n",
    "            prob_true, prob_pred = calibration_curve((y_true==i).astype(int), y_prob[:,i], n_bins=n_bins, strategy='uniform')\n",
    "            plt.plot(prob_pred, prob_true, marker='o', label=f\"{name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Reliability curve failed for {name}: {e}\")\n",
    "    plt.plot([0,1],[0,1],'--', color='gray')\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.title(\"Reliability Diagram (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reliability_diagram_cbam.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curves (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_acc_curves_cbam.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ---------- TTA ----------\n",
    "def tta_eval(model, test_ds, batch_size, class_names):\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomVerticalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomRotation(15),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(280), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.GaussianBlur(3),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    ]\n",
    "    tta_probs = []\n",
    "    model.eval()\n",
    "    for t in tta_transforms:\n",
    "        test_ds.transform = t\n",
    "        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "        tta_probs.append(np.concatenate(preds))\n",
    "    tta_probs = np.array(tta_probs)\n",
    "    mean_probs = np.mean(tta_probs, axis=0)\n",
    "    final_preds = np.argmax(mean_probs, axis=1)\n",
    "    return mean_probs, final_preds\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3):\n",
    "    train_loader, val_loader, test_loader, test_ds = get_loaders(batch_size)\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # (OPTIONAL) Freeze backbone for warmup\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 5:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        t_loss, t_acc = train_epoch(model, train_loader, criterion, optimizer, use_mixup=True, mixup_alpha=mixup_alpha)\n",
    "        v_loss, v_acc, _, _ = eval_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        train_losses.append(t_loss)\n",
    "        train_accs.append(t_acc)\n",
    "        val_losses.append(v_loss)\n",
    "        val_accs.append(v_acc)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {t_loss:.4f}, Acc {t_acc:.4f} | Val Loss {v_loss:.4f}, Acc {v_acc:.4f}\")\n",
    "        if early_stopper(v_acc, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs)\n",
    "    model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    print(\"\\nTest set results (CBAM):\")\n",
    "    test_loss, test_acc, test_labels, test_probs = eval_epoch(model, test_loader, criterion)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_labels, test_preds, class_names, save_path=\"cbam_norm_confmat.png\")\n",
    "\n",
    "    try:\n",
    "        test_labels_onehot = np.eye(num_classes)[test_labels]\n",
    "        roc_macro = roc_auc_score(test_labels_onehot, test_probs, average='macro', multi_class='ovr')\n",
    "        print(f\"Test ROC-AUC (macro): {roc_macro:.4f}\")\n",
    "        plot_roc_per_class(test_labels, test_probs, num_classes, class_names, save_path=\"cbam_perclass_roc.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "\n",
    "    plot_reliability(test_labels, test_probs)\n",
    "\n",
    "    # -------- TTA --------\n",
    "    print(\"\\nTest-Time Augmentation (TTA) Evaluation (CBAM):\")\n",
    "    mean_probs, final_preds = tta_eval(model, test_ds, batch_size, class_names)\n",
    "    print(classification_report(test_ds.targets, final_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_ds.targets, final_preds, class_names, save_path=\"cbam_tta_confmat.png\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hybrid SupCon+CE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T23:14:23.080483Z",
     "iopub.status.busy": "2025-07-06T23:14:23.079690Z",
     "iopub.status.idle": "2025-07-06T23:43:45.456002Z",
     "shell.execute_reply": "2025-07-06T23:43:45.455110Z",
     "shell.execute_reply.started": "2025-07-06T23:14:23.080455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 2.2487, Acc 0.3898 | Val Loss 1.1088, Acc 0.3131\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 2.2073, Acc 0.4732 | Val Loss 1.1009, Acc 0.3131\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 2.1854, Acc 0.4149 | Val Loss 1.0241, Acc 0.4646\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 2.1509, Acc 0.3909 | Val Loss 0.9630, Acc 0.5556\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 2.1198, Acc 0.4880 | Val Loss 0.9547, Acc 0.5051\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 2.2027, Acc 0.4493 | Val Loss 0.9060, Acc 0.5859\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 2.1879, Acc 0.4928 | Val Loss 0.8403, Acc 0.6465\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 2.1510, Acc 0.5059 | Val Loss 0.7801, Acc 0.7677\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 2.1323, Acc 0.5324 | Val Loss 0.7574, Acc 0.7273\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 2.1112, Acc 0.5755 | Val Loss 0.7450, Acc 0.6970\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 2.1601, Acc 0.5789 | Val Loss 0.6835, Acc 0.7273\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 2.1311, Acc 0.5866 | Val Loss 0.5978, Acc 0.8384\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 2.1179, Acc 0.6101 | Val Loss 0.5575, Acc 0.8081\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 2.0786, Acc 0.6883 | Val Loss 0.4622, Acc 0.8889\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 2.0899, Acc 0.6630 | Val Loss 0.4747, Acc 0.8283\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 2.0270, Acc 0.6823 | Val Loss 0.4400, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 2.0749, Acc 0.6422 | Val Loss 0.4603, Acc 0.8283\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 2.0704, Acc 0.6573 | Val Loss 0.4909, Acc 0.8586\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 2.0773, Acc 0.6651 | Val Loss 0.4470, Acc 0.8788\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 2.0559, Acc 0.6503 | Val Loss 0.4231, Acc 0.9192\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 2.0119, Acc 0.7370 | Val Loss 0.4326, Acc 0.8990\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 2.0867, Acc 0.6377 | Val Loss 0.4136, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 2.0443, Acc 0.6928 | Val Loss 0.4408, Acc 0.9293\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 2.0399, Acc 0.6767 | Val Loss 0.4642, Acc 0.8485\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 1.9983, Acc 0.6738 | Val Loss 0.4130, Acc 0.9192\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 2.0241, Acc 0.6965 | Val Loss 0.4057, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 1.9852, Acc 0.6989 | Val Loss 0.4227, Acc 0.9091\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 1.9733, Acc 0.7095 | Val Loss 0.3969, Acc 0.9091\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 2.0262, Acc 0.6961 | Val Loss 0.3916, Acc 0.9192\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss 1.9875, Acc 0.7056 | Val Loss 0.3999, Acc 0.8889\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss 1.9295, Acc 0.7617 | Val Loss 0.3832, Acc 0.9394\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss 1.9527, Acc 0.7137 | Val Loss 0.3812, Acc 0.9293\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss 1.9037, Acc 0.7407 | Val Loss 0.3880, Acc 0.9293\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss 1.9424, Acc 0.7320 | Val Loss 0.3910, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss 1.9574, Acc 0.7187 | Val Loss 0.3765, Acc 0.9394\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss 1.8872, Acc 0.7782 | Val Loss 0.3792, Acc 0.9394\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Train Loss 1.8645, Acc 0.7565 | Val Loss 0.3805, Acc 0.9192\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Train Loss 1.9698, Acc 0.7004 | Val Loss 0.3763, Acc 0.9293\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Train Loss 1.9289, Acc 0.7224 | Val Loss 0.3786, Acc 0.9293\n",
      "EarlyStopping counter: 8 / 8\n",
      "Early stopping at epoch 39\n",
      "\n",
      "Test set results (CBAM Hybrid):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3125, Test Acc: 0.9596\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.89      0.94        37\n",
      "Healthy Leaf       0.89      1.00      0.94        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.96        99\n",
      "   macro avg       0.96      0.96      0.96        99\n",
      "weighted avg       0.96      0.96      0.96        99\n",
      "\n",
      "Test ROC-AUC (macro): 0.9988\n",
      "\n",
      "Test-Time Augmentation (TTA) Evaluation (CBAM Hybrid):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       0.97      0.89      0.93        37\n",
      "Healthy Leaf       0.88      0.97      0.92        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.95        99\n",
      "   macro avg       0.95      0.95      0.95        99\n",
      "weighted avg       0.95      0.95      0.95        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, roc_curve, auc\n",
    "\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "# ---------- SEED & DEVICE ----------\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- PATHS & CLASSES ----------\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- MIXUP ----------\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ---------- SUPERVISED CONTRASTIVE LOSS ----------\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.eps = 1e-8\n",
    "    def forward(self, features, labels):\n",
    "        device = features.device\n",
    "        batch_size = features.size(0)\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "        exp_logits = torch.exp(logits) * (1 - torch.eye(batch_size, device=device))\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + self.eps)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + self.eps)\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        return loss\n",
    "\n",
    "# ---------- DATALOADERS ----------\n",
    "def get_loaders(batch_size=32):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, test_ds\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.feature_layer = nn.Linear(2048, 128)  # For SupCon\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        feats = self.backbone(x).flatten(1)\n",
    "        features = F.normalize(self.feature_layer(feats), dim=1)\n",
    "        logits = self.classifier(feats)\n",
    "        if return_features:\n",
    "            return logits, features\n",
    "        return logits\n",
    "\n",
    "# ---------- LR SCHEDULER ----------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------- TRAINING & EVAL ----------\n",
    "def train_epoch(model, loader, ce_loss_fn, supcon_loss_fn, optimizer,\n",
    "                use_mixup=True, mixup_alpha=0.3, supcon_weight=0.5):\n",
    "    model.train()\n",
    "    total_loss, total_ce, total_supcon, correct = 0, 0, 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "            logits, features = model(imgs, return_features=True)\n",
    "            loss_ce = mixup_criterion(ce_loss_fn, logits, y_a, y_b, lam)\n",
    "            loss_supcon = supcon_loss_fn(features, labels)\n",
    "            loss = (1 - supcon_weight) * loss_ce + supcon_weight * loss_supcon\n",
    "            preds = logits.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            logits, features = model(imgs, return_features=True)\n",
    "            loss_ce = ce_loss_fn(logits, labels)\n",
    "            loss_supcon = supcon_loss_fn(features, labels)\n",
    "            loss = (1 - supcon_weight) * loss_ce + supcon_weight * loss_supcon\n",
    "            correct += logits.argmax(1).eq(labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        total_ce += loss_ce.item() * imgs.size(0)\n",
    "        total_supcon += loss_supcon.item() * imgs.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, correct / n, total_ce / n, total_supcon / n\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ---------- EARLY STOPPING ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.verbose = verbose\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            if self.verbose: print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ---------- PLOTTING ----------\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "    plt.title('Normalized Confusion Matrix (CBAM)')\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_per_class(y_true, y_score, n_classes, class_names, save_path=None):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(n_classes):\n",
    "        if np.sum(y_true==i) == 0: continue\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve((y_true==i).astype(int), y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "        except Exception as e:\n",
    "            print(f\"ROC error for class {class_names[i]}: {e}\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Per-class ROC Curves (CBAM)')\n",
    "    plt.legend()\n",
    "    if save_path: plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_reliability(y_true, y_prob, n_bins=10):\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    plt.figure(figsize=(5,5))\n",
    "    for i, name in enumerate(class_names):\n",
    "        try:\n",
    "            prob_true, prob_pred = calibration_curve((y_true==i).astype(int), y_prob[:,i], n_bins=n_bins, strategy='uniform')\n",
    "            plt.plot(prob_pred, prob_true, marker='o', label=f\"{name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Reliability curve failed for {name}: {e}\")\n",
    "    plt.plot([0,1],[0,1],'--', color='gray')\n",
    "    plt.xlabel(\"Mean Predicted Probability\")\n",
    "    plt.ylabel(\"Fraction of Positives\")\n",
    "    plt.title(\"Reliability Diagram (CBAM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reliability_diagram_cbam.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs, train_ce_losses, train_supcon_losses):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Total')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.plot(train_ce_losses, label='Train CE')\n",
    "    plt.plot(train_supcon_losses, label='Train SupCon')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss Curves (CBAM Hybrid)\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_accs, label='Train Acc')\n",
    "    plt.plot(val_accs, label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy Curves (CBAM Hybrid)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"loss_acc_curves_cbam_hybrid.png\")\n",
    "    plt.close()\n",
    "\n",
    "# ---------- TTA ----------\n",
    "def tta_eval(model, test_ds, batch_size, class_names):\n",
    "    tta_transforms = [\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomVerticalFlip(1.0),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomRotation(15),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(280), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "        transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.GaussianBlur(3),\n",
    "                            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    ]\n",
    "    tta_probs = []\n",
    "    model.eval()\n",
    "    for t in tta_transforms:\n",
    "        test_ds.transform = t\n",
    "        loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, _ in loader:\n",
    "                imgs = imgs.to(device)\n",
    "                outputs = model(imgs)\n",
    "                preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "        tta_probs.append(np.concatenate(preds))\n",
    "    tta_probs = np.array(tta_probs)\n",
    "    mean_probs = np.mean(tta_probs, axis=0)\n",
    "    final_preds = np.argmax(mean_probs, axis=1)\n",
    "    return mean_probs, final_preds\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3, supcon_weight=0.5):\n",
    "    train_loader, val_loader, test_loader, test_ds = get_loaders(batch_size)\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    supcon_loss_fn = SupConLoss(temperature=0.07)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # (OPTIONAL) Freeze backbone for warmup\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.feature_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "    train_ce_losses, train_supcon_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 5:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        t_loss, t_acc, t_ce, t_sup = train_epoch(\n",
    "            model, train_loader, ce_loss_fn, supcon_loss_fn, optimizer,\n",
    "            use_mixup=True, mixup_alpha=mixup_alpha, supcon_weight=supcon_weight)\n",
    "        v_loss, v_acc, _, _ = eval_epoch(model, val_loader, ce_loss_fn)\n",
    "        scheduler.step()\n",
    "        train_losses.append(t_loss)\n",
    "        train_accs.append(t_acc)\n",
    "        train_ce_losses.append(t_ce)\n",
    "        train_supcon_losses.append(t_sup)\n",
    "        val_losses.append(v_loss)\n",
    "        val_accs.append(v_acc)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {t_loss:.4f}, Acc {t_acc:.4f} | Val Loss {v_loss:.4f}, Acc {v_acc:.4f}\")\n",
    "        if early_stopper(v_acc, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    plot_loss_acc_curves(train_losses, val_losses, train_accs, val_accs, train_ce_losses, train_supcon_losses)\n",
    "    model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "    print(\"\\nTest set results (CBAM Hybrid):\")\n",
    "    test_loss, test_acc, test_labels, test_probs = eval_epoch(model, test_loader, ce_loss_fn)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_labels, test_preds, class_names, save_path=\"cbam_norm_confmat_hybrid.png\")\n",
    "\n",
    "    try:\n",
    "        test_labels_onehot = np.eye(num_classes)[test_labels]\n",
    "        roc_macro = roc_auc_score(test_labels_onehot, test_probs, average='macro', multi_class='ovr')\n",
    "        print(f\"Test ROC-AUC (macro): {roc_macro:.4f}\")\n",
    "        plot_roc_per_class(test_labels, test_probs, num_classes, class_names, save_path=\"cbam_perclass_roc_hybrid.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"ROC-AUC calculation failed: {e}\")\n",
    "\n",
    "    plot_reliability(test_labels, test_probs)\n",
    "\n",
    "    # -------- TTA --------\n",
    "    print(\"\\nTest-Time Augmentation (TTA) Evaluation (CBAM Hybrid):\")\n",
    "    mean_probs, final_preds = tta_eval(model, test_ds, batch_size, class_names)\n",
    "    print(classification_report(test_ds.targets, final_preds, target_names=class_names))\n",
    "    plot_confusion_matrix(test_ds.targets, final_preds, class_names, save_path=\"cbam_tta_confmat_hybrid.png\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3, supcon_weight=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fine-tuning training pipeline with Gaussian noise and Salt and Pepper noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T21:30:29.694011Z",
     "iopub.status.busy": "2025-07-06T21:30:29.693654Z",
     "iopub.status.idle": "2025-07-06T22:26:00.390868Z",
     "shell.execute_reply": "2025-07-06T22:26:00.389989Z",
     "shell.execute_reply.started": "2025-07-06T21:30:29.693985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running: NoNoise ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.4464, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train 0.4424, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train 0.4197, Val 0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train 0.4436, Val 0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train 0.4606, Val 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train 0.5252, Val 0.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train 0.5449, Val 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train 0.5732, Val 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train 0.5855, Val 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train 0.6835, Val 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train 0.6723, Val 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train 0.7151, Val 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train 0.6953, Val 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train 0.7013, Val 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train 0.7098, Val 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train 0.7051, Val 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train 0.7088, Val 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train 0.7235, Val 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train 0.7281, Val 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train 0.7111, Val 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train 0.7653, Val 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train 0.7803, Val 0.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train 0.7134, Val 0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train 0.7387, Val 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train 0.7328, Val 0.9091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train 0.7194, Val 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train 0.7011, Val 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train 0.7489, Val 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train 0.7699, Val 0.9293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train 0.7423, Val 0.8889\n",
      "Early stopping at epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9596\n",
      "ROC-AUC (macro): 0.9992\n",
      "ECE: 15.68%\n",
      "Results saved to ablation_results.csv\n",
      "\n",
      "==== Running: GaussianOnly ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.4270, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train 0.4421, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train 0.3956, Val 0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train 0.4600, Val 0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train 0.4598, Val 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train 0.4616, Val 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train 0.5562, Val 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train 0.5846, Val 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train 0.5611, Val 0.7980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train 0.6435, Val 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train 0.6600, Val 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train 0.6896, Val 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train 0.6919, Val 0.8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train 0.7071, Val 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train 0.7316, Val 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train 0.6717, Val 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train 0.7083, Val 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train 0.7259, Val 0.7980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train 0.7310, Val 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train 0.7007, Val 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train 0.7600, Val 0.8586\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9293\n",
      "ROC-AUC (macro): 0.9956\n",
      "ECE: 19.45%\n",
      "Results saved to ablation_results.csv\n",
      "\n",
      "==== Running: SaltPepperOnly ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.4504, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train 0.4199, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train 0.4287, Val 0.3737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train 0.4198, Val 0.6364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train 0.4635, Val 0.5455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train 0.4535, Val 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train 0.5350, Val 0.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train 0.5412, Val 0.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train 0.5963, Val 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train 0.6179, Val 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train 0.6398, Val 0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train 0.6537, Val 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train 0.6900, Val 0.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train 0.6655, Val 0.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train 0.6947, Val 0.7980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train 0.6382, Val 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train 0.6881, Val 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train 0.7041, Val 0.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train 0.7009, Val 0.7677\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8384\n",
      "ROC-AUC (macro): 0.9614\n",
      "ECE: 22.08%\n",
      "Results saved to ablation_results.csv\n",
      "\n",
      "==== Running: BothNoises ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train 0.4414, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train 0.4440, Val 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train 0.4231, Val 0.3838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train 0.4768, Val 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train 0.4423, Val 0.5556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train 0.4916, Val 0.6566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train 0.5181, Val 0.6869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train 0.5507, Val 0.7374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train 0.5409, Val 0.8182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train 0.5880, Val 0.7980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train 0.6519, Val 0.7980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train 0.7015, Val 0.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train 0.6640, Val 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train 0.6725, Val 0.5556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train 0.6482, Val 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train 0.6535, Val 0.5657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train 0.6710, Val 0.6768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train 0.7345, Val 0.6465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train 0.7127, Val 0.7273\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8889\n",
      "ROC-AUC (macro): 0.9876\n",
      "ECE: 30.33%\n",
      "Results saved to ablation_results.csv\n",
      "\n",
      "All ablation runs complete.\n",
      "             name  test_acc  roc_auc_macro        ece use_gaussian  \\\n",
      "0         NoNoise  0.959596       0.999248  15.679369          NaN   \n",
      "1    GaussianOnly  0.929293       0.995645  19.453843         True   \n",
      "2  SaltPepperOnly  0.838384       0.961374  22.078380          NaN   \n",
      "3      BothNoises  0.888889       0.987576  30.332287         True   \n",
      "\n",
      "   gaussian_std  gaussian_p use_saltpepper  salt_prob  pepper_prob  \\\n",
      "0           NaN         NaN            NaN        NaN          NaN   \n",
      "1          0.05         1.0            NaN        NaN          NaN   \n",
      "2           NaN         NaN           True       0.01         0.01   \n",
      "3          0.05         1.0           True       0.01         0.01   \n",
      "\n",
      "   saltpepper_p  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           1.0  \n",
      "3           1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Paths and Classes ---\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# --- Custom Noise Transforms ---\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.05):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn_like(tensor) * self.std + self.mean\n",
    "    def __repr__(self):\n",
    "        return f\"AddGaussianNoise(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "class AddSaltPepperNoise(object):\n",
    "    def __init__(self, salt_prob=0.01, pepper_prob=0.01):\n",
    "        self.salt_prob = salt_prob\n",
    "        self.pepper_prob = pepper_prob\n",
    "    def __call__(self, tensor):\n",
    "        c, h, w = tensor.shape\n",
    "        mask = torch.rand((h, w))\n",
    "        salt = (mask < self.salt_prob).float()\n",
    "        pepper = (mask > 1 - self.pepper_prob).float()\n",
    "        for i in range(c):\n",
    "            tensor[i] = tensor[i] * (1 - salt - pepper) + salt + 0 * pepper\n",
    "        return tensor\n",
    "    def __repr__(self):\n",
    "        return f\"AddSaltPepperNoise(salt_prob={self.salt_prob}, pepper_prob={self.pepper_prob})\"\n",
    "\n",
    "# --- Mixup ---\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# --- Data Loader with Configurable Noise ---\n",
    "def get_loaders(batch_size=32, noise_cfg=None):\n",
    "    if noise_cfg is None: noise_cfg = {}\n",
    "    tfms = [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    if noise_cfg.get('use_gaussian', False):\n",
    "        tfms.append(transforms.RandomApply([AddGaussianNoise(std=noise_cfg.get('gaussian_std', 0.05))], p=noise_cfg.get('gaussian_p', 1.0)))\n",
    "    if noise_cfg.get('use_saltpepper', False):\n",
    "        tfms.append(transforms.RandomApply([AddSaltPepperNoise(salt_prob=noise_cfg.get('salt_prob',0.01), pepper_prob=noise_cfg.get('pepper_prob',0.01))], p=noise_cfg.get('saltpepper_p',1.0)))\n",
    "    tfms.append(transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "    train_transform = transforms.Compose(tfms)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# --- CBAM Fine-tune Model ---\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# --- LR Scheduler with Warmup ---\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs - warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# --- Training and Evaluation ---\n",
    "def train_epoch(model, loader, criterion, optimizer, use_mixup=True):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc='Train', leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc='Eval', leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# --- Early Stopping ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# --- Reliability Diagram (ECE) ---\n",
    "def compute_ece(labels, probs, n_bins=10):\n",
    "    \"\"\"Expected Calibration Error for multi-class softmax\"\"\"\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
    "        if np.any(mask):\n",
    "            bin_acc = np.mean(accuracies[mask])\n",
    "            bin_conf = np.mean(confidences[mask])\n",
    "            ece += np.abs(bin_acc - bin_conf) * np.sum(mask) / len(probs)\n",
    "    return 100 * ece  # Percentage\n",
    "\n",
    "# --- Main Experiment Runner ---\n",
    "def run_ablation_experiments(ablation_configs, epochs=50, batch_size=32, patience=10, csv_path=\"ablation_results.csv\"):\n",
    "    results = []\n",
    "    for cfg in ablation_configs:\n",
    "        print(f\"\\n==== Running: {cfg['name']} ====\")\n",
    "        seed_all()  # Reset seed for reproducibility\n",
    "        train_loader, val_loader, test_loader = get_loaders(batch_size, noise_cfg=cfg)\n",
    "        model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "        early_stopper = EarlyStopping(patience=patience)\n",
    "        for param in model.backbone.parameters(): param.requires_grad = False\n",
    "        for param in model.classifier.parameters(): param.requires_grad = True\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if epoch == 5:  # Unfreeze backbone after warmup\n",
    "                for param in model.backbone.parameters(): param.requires_grad = True\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, use_mixup=True)\n",
    "            val_loss, val_acc, _, _ = eval_epoch(model, val_loader, criterion)\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}: Train {train_acc:.4f}, Val {val_acc:.4f}\")\n",
    "            if early_stopper(val_acc, model):\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Load best state\n",
    "        model.load_state_dict(early_stopper.best_state)\n",
    "\n",
    "        # ---- Final Test & Evaluation ----\n",
    "        _, test_acc, labels, probs = eval_epoch(model, test_loader, criterion)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        # ROC-AUC macro (one-vs-rest)\n",
    "        try:\n",
    "            labels_onehot = np.eye(num_classes)[labels]\n",
    "            roc_macro = roc_auc_score(labels_onehot, probs, average='macro', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            roc_macro = None\n",
    "        # ECE\n",
    "        ece = compute_ece(labels, probs)\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"ROC-AUC (macro): {roc_macro:.4f}\" if roc_macro else \"ROC-AUC N/A\")\n",
    "        print(f\"ECE: {ece:.2f}%\")\n",
    "        # Save results\n",
    "        row = dict(cfg)\n",
    "        row.update({\n",
    "            \"test_acc\": test_acc,\n",
    "            \"roc_auc_macro\": roc_macro,\n",
    "            \"ece\": ece,\n",
    "        })\n",
    "        results.append(row)\n",
    "        pd.DataFrame(results).to_csv(csv_path, index=False)\n",
    "        print(f\"Results saved to {csv_path}\")\n",
    "    print(\"\\nAll ablation runs complete.\")\n",
    "    print(pd.DataFrame(results))\n",
    "    return results\n",
    "\n",
    "# --- Define Ablation Study Conditions ---\n",
    "ablation_configs = [\n",
    "    {\"name\": \"NoNoise\"},\n",
    "    {\"name\": \"GaussianOnly\", \"use_gaussian\": True, \"gaussian_std\": 0.05, \"gaussian_p\": 1.0},\n",
    "    {\"name\": \"SaltPepperOnly\", \"use_saltpepper\": True, \"salt_prob\": 0.01, \"pepper_prob\": 0.01, \"saltpepper_p\": 1.0},\n",
    "    {\"name\": \"BothNoises\", \"use_gaussian\": True, \"gaussian_std\": 0.05, \"gaussian_p\": 1.0, \"use_saltpepper\": True, \"salt_prob\": 0.01, \"pepper_prob\": 0.01, \"saltpepper_p\": 1.0},\n",
    "]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_ablation_experiments(ablation_configs, epochs=50, batch_size=32, patience=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images after Noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T22:41:38.358886Z",
     "iopub.status.busy": "2025-07-06T22:41:38.358109Z",
     "iopub.status.idle": "2025-07-06T22:42:13.073582Z",
     "shell.execute_reply": "2025-07-06T22:42:13.072840Z",
     "shell.execute_reply.started": "2025-07-06T22:41:38.358861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /kaggle/working/noised_samples/Alternaria__alternaria (1)_ORIGINAL.png\n",
      "Saved: /kaggle/working/noised_samples/Alternaria__alternaria (1)_GAUSSIAN.png\n",
      "Saved: /kaggle/working/noised_samples/Alternaria__alternaria (1)_SALTPEPPER.png\n",
      "Saved: /kaggle/working/noised_samples/Alternaria__alternaria (1)_BOTHNOISES.png\n",
      "Saved: /kaggle/working/noised_samples/Healthy Leaf__healthy (10)_ORIGINAL.png\n",
      "Saved: /kaggle/working/noised_samples/Healthy Leaf__healthy (10)_GAUSSIAN.png\n",
      "Saved: /kaggle/working/noised_samples/Healthy Leaf__healthy (10)_SALTPEPPER.png\n",
      "Saved: /kaggle/working/noised_samples/Healthy Leaf__healthy (10)_BOTHNOISES.png\n",
      "Saved: /kaggle/working/noised_samples/straw_mite__straw_mite (10)_ORIGINAL.png\n",
      "Saved: /kaggle/working/noised_samples/straw_mite__straw_mite (10)_GAUSSIAN.png\n",
      "Saved: /kaggle/working/noised_samples/straw_mite__straw_mite (10)_SALTPEPPER.png\n",
      "Saved: /kaggle/working/noised_samples/straw_mite__straw_mite (10)_BOTHNOISES.png\n",
      "All versions saved in /kaggle/working/noised_samples/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Define noise transforms ----\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0.0, std=0.07):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn_like(tensor) * self.std + self.mean\n",
    "\n",
    "class AddSaltPepperNoise(object):\n",
    "    def __init__(self, salt_prob=0.03, pepper_prob=0.03):\n",
    "        self.salt_prob = salt_prob\n",
    "        self.pepper_prob = pepper_prob\n",
    "    def __call__(self, tensor):\n",
    "        c, h, w = tensor.shape\n",
    "        mask = torch.rand((h, w))\n",
    "        salt = (mask < self.salt_prob).float()\n",
    "        pepper = (mask > 1 - self.pepper_prob).float()\n",
    "        for i in range(c):\n",
    "            tensor[i] = tensor[i] * (1 - salt - pepper) + salt + 0 * pepper\n",
    "        return tensor\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# ----- Output directory -----\n",
    "output_dir = '/kaggle/working/noised_samples/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----- Dataset: one image per class -----\n",
    "train_dir = '/kaggle/input/minida/mini_output1/train'\n",
    "ds = ImageFolder(train_dir)\n",
    "class_to_idx = ds.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "imgs_per_class = {}\n",
    "for img_path, label in ds.samples:\n",
    "    if label not in imgs_per_class:\n",
    "        imgs_per_class[label] = img_path\n",
    "    if len(imgs_per_class) == len(idx_to_class):\n",
    "        break\n",
    "\n",
    "# ----- Define the 4 noise pipelines -----\n",
    "def no_noise_pipeline():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "def gaussian_pipeline():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomApply([AddGaussianNoise(std=0.07)], p=1.0)\n",
    "    ])\n",
    "def saltpepper_pipeline():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomApply([AddSaltPepperNoise(salt_prob=0.03, pepper_prob=0.03)], p=1.0)\n",
    "    ])\n",
    "def both_pipeline():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomApply([AddGaussianNoise(std=0.07)], p=1.0),\n",
    "        transforms.RandomApply([AddSaltPepperNoise(salt_prob=0.03, pepper_prob=0.03)], p=1.0)\n",
    "    ])\n",
    "\n",
    "pipelines = [\n",
    "    (\"ORIGINAL\", no_noise_pipeline()),\n",
    "    (\"GAUSSIAN\", gaussian_pipeline()),\n",
    "    (\"SALTPEPPER\", saltpepper_pipeline()),\n",
    "    (\"BOTHNOISES\", both_pipeline()),\n",
    "]\n",
    "\n",
    "# ----- Save all versions -----\n",
    "for label, img_path in imgs_per_class.items():\n",
    "    class_name = idx_to_class[label]\n",
    "    orig_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    for ver_name, pipeline in pipelines:\n",
    "        tensor_img = pipeline(img)\n",
    "        tensor_img = tensor_img.clamp(0,1)\n",
    "        out_img = to_pil(tensor_img)\n",
    "        save_path = os.path.join(\n",
    "            output_dir, f\"{class_name}__{orig_name}_{ver_name}.png\"\n",
    "        )\n",
    "        out_img.save(save_path)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"All versions saved in\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T18:32:55.697084Z",
     "iopub.status.busy": "2025-07-12T18:32:55.695517Z",
     "iopub.status.idle": "2025-07-12T18:58:56.416937Z",
     "shell.execute_reply": "2025-07-12T18:58:56.415853Z",
     "shell.execute_reply.started": "2025-07-12T18:32:55.697039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 1.0797, Acc 0.4428 | Val Loss 1.1021, Acc 0.3131\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 1.0514, Acc 0.4315 | Val Loss 1.1091, Acc 0.3131\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss 1.0452, Acc 0.4233 | Val Loss 1.0132, Acc 0.4747\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss 1.0478, Acc 0.4563 | Val Loss 0.9460, Acc 0.6869\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss 1.0646, Acc 0.4371 | Val Loss 0.9650, Acc 0.6162\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss 1.0260, Acc 0.4961 | Val Loss 0.8190, Acc 0.6869\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss 0.9878, Acc 0.5293 | Val Loss 0.7635, Acc 0.7273\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss 0.9395, Acc 0.5544 | Val Loss 0.5978, Acc 0.8384\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss 0.9318, Acc 0.5686 | Val Loss 0.5133, Acc 0.8788\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss 0.8408, Acc 0.6399 | Val Loss 0.4725, Acc 0.8384\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss 0.8235, Acc 0.6594 | Val Loss 0.5732, Acc 0.7778\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss 0.8280, Acc 0.6499 | Val Loss 0.4551, Acc 0.8485\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss 0.8032, Acc 0.6788 | Val Loss 0.4415, Acc 0.8889\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss 0.7189, Acc 0.7159 | Val Loss 0.4325, Acc 0.8889\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss 0.8371, Acc 0.6578 | Val Loss 0.4500, Acc 0.8990\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss 0.7392, Acc 0.7146 | Val Loss 0.4594, Acc 0.8182\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss 0.8212, Acc 0.6590 | Val Loss 0.4256, Acc 0.8990\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss 0.7327, Acc 0.7387 | Val Loss 0.4640, Acc 0.8283\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss 0.7776, Acc 0.6827 | Val Loss 0.3873, Acc 0.9091\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss 0.7947, Acc 0.7034 | Val Loss 0.3819, Acc 0.9293\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss 0.6738, Acc 0.7706 | Val Loss 0.4699, Acc 0.8485\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss 0.7487, Acc 0.7139 | Val Loss 0.3828, Acc 0.8889\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss 0.7398, Acc 0.7155 | Val Loss 0.4218, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss 0.7984, Acc 0.6800 | Val Loss 0.3936, Acc 0.9091\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss 0.7769, Acc 0.6762 | Val Loss 0.4109, Acc 0.9091\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss 0.7512, Acc 0.7150 | Val Loss 0.4405, Acc 0.8788\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss 0.7297, Acc 0.7033 | Val Loss 0.3844, Acc 0.8788\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss 0.6994, Acc 0.7555 | Val Loss 0.3401, Acc 0.9495\n",
      "Validation accuracy improved, saving best state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss 0.7122, Acc 0.7373 | Val Loss 0.3407, Acc 0.9394\n",
      "EarlyStopping counter: 1 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss 0.7144, Acc 0.7273 | Val Loss 0.3748, Acc 0.9091\n",
      "EarlyStopping counter: 2 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Train Loss 0.6796, Acc 0.7512 | Val Loss 0.3562, Acc 0.9192\n",
      "EarlyStopping counter: 3 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Train Loss 0.7132, Acc 0.7086 | Val Loss 0.3366, Acc 0.9293\n",
      "EarlyStopping counter: 4 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Train Loss 0.6956, Acc 0.7399 | Val Loss 0.3485, Acc 0.9192\n",
      "EarlyStopping counter: 5 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Train Loss 0.6700, Acc 0.7500 | Val Loss 0.3634, Acc 0.9091\n",
      "EarlyStopping counter: 6 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Train Loss 0.7068, Acc 0.7411 | Val Loss 0.3515, Acc 0.9192\n",
      "EarlyStopping counter: 7 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Train Loss 0.6281, Acc 0.7872 | Val Loss 0.3346, Acc 0.9293\n",
      "EarlyStopping counter: 8 / 8\n",
      "Early stopping at epoch 36\n",
      "Model saved as best_finetuned_cbam.pth\n",
      "\n",
      "Test set results (CBAM):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2851, Test Acc: 0.9697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Alternaria       1.00      0.92      0.96        37\n",
      "Healthy Leaf       0.91      1.00      0.95        31\n",
      "  straw_mite       1.00      1.00      1.00        31\n",
      "\n",
      "    accuracy                           0.97        99\n",
      "   macro avg       0.97      0.97      0.97        99\n",
      "weighted avg       0.97      0.97      0.97        99\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import RandAugment\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from cbam_resnet import cbam_resnet50  # Ensure this file is in your working directory\n",
    "\n",
    "# ---------- SEED & DEVICE ----------\n",
    "def seed_all(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- PATHS & CLASSES ----------\n",
    "data_root = \"/kaggle/input/minida/mini_output1\"\n",
    "pretrained_path = \"/kaggle/working/simsiam_cbam_pretrained_final.pth\"\n",
    "train_dir, val_dir, test_dir = [os.path.join(data_root, x) for x in [\"train\", \"val\", \"test\"]]\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# ---------- DATALOADERS ----------\n",
    "def get_loaders(batch_size=32):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        RandAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = ImageFolder(train_dir, train_transform)\n",
    "    val_ds = ImageFolder(val_dir, val_transform)\n",
    "    test_ds = ImageFolder(test_dir, val_transform)\n",
    "\n",
    "    class_counts = np.bincount(train_ds.targets)\n",
    "    weights = 1. / class_counts[train_ds.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_ds), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader, test_ds\n",
    "\n",
    "# ---------- MODEL ----------\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        ckpt = torch.load(pretrained_path, map_location=device)\n",
    "        if 'backbone' in ckpt:\n",
    "            self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "        else:\n",
    "            self.backbone.load_state_dict(ckpt, strict=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ---------- LR SCHEDULER ----------\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "def get_scheduler(optimizer, total_epochs, warmup_epochs=5):\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs)\n",
    "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_epochs])\n",
    "\n",
    "# ---------- MIXUP ----------\n",
    "def mixup_data(x, y, alpha=0.3):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ---------- TRAINING & EVAL ----------\n",
    "def train_epoch(model, loader, criterion, optimizer, use_mixup=True, mixup_alpha=0.3):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for imgs, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if use_mixup:\n",
    "            imgs, y_a, y_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "            outputs = model(imgs)\n",
    "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (lam * preds.eq(y_a).sum().item() + (1 - lam) * preds.eq(y_b).sum().item())\n",
    "        else:\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, all_labels, all_probs = 0, 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            correct += outputs.argmax(1).eq(labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader.dataset), acc, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# ---------- EARLY STOPPING ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_state = None\n",
    "        self.verbose = verbose\n",
    "    def __call__(self, val_acc, model):\n",
    "        if (self.best_acc is None) or (val_acc > self.best_acc):\n",
    "            self.best_acc = val_acc\n",
    "            self.best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            self.counter = 0\n",
    "            if self.verbose: print(\"Validation accuracy improved, saving best state.\")\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ---------- MAIN ----------\n",
    "def main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3):\n",
    "    train_loader, val_loader, test_loader, test_ds = get_loaders(batch_size)\n",
    "    model = FineTuneCBAM(pretrained_path, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = get_scheduler(optimizer, epochs, warmup_epochs=5)\n",
    "    early_stopper = EarlyStopping(patience=patience)\n",
    "\n",
    "    # Freeze backbone for warmup\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 5:\n",
    "            for param in model.backbone.parameters():\n",
    "                param.requires_grad = True\n",
    "        t_loss, t_acc = train_epoch(model, train_loader, criterion, optimizer, use_mixup=True, mixup_alpha=mixup_alpha)\n",
    "        v_loss, v_acc, _, _ = eval_epoch(model, val_loader, criterion)\n",
    "        scheduler.step()\n",
    "        train_losses.append(t_loss)\n",
    "        train_accs.append(t_acc)\n",
    "        val_losses.append(v_loss)\n",
    "        val_accs.append(v_acc)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss {t_loss:.4f}, Acc {t_acc:.4f} | Val Loss {v_loss:.4f}, Acc {v_acc:.4f}\")\n",
    "        if early_stopper(v_acc, model):\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # --- Always restore best state and eval before testing ---\n",
    "    model.load_state_dict(early_stopper.best_state)\n",
    "    model.eval()\n",
    "    # --- Save model at its best state ---\n",
    "    torch.save(model.state_dict(), \"best_finetuned_cbam.pth\")\n",
    "    print(\"Model saved as best_finetuned_cbam.pth\")\n",
    "\n",
    "    # --- Test evaluation ---\n",
    "    print(\"\\nTest set results (CBAM):\")\n",
    "    test_loss, test_acc, test_labels, test_probs = eval_epoch(model, test_loader, criterion)\n",
    "    test_preds = np.argmax(test_probs, axis=1)\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "    # (Add your plotting here as needed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(epochs=40, batch_size=32, patience=8, mixup_alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- XAI-Ready Loader + GradCAMs All-in-One Cell ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# ---- Model definition (match your finetune) ----\n",
    "from cbam_resnet import cbam_resnet50\n",
    "\n",
    "class FineTuneCBAM(nn.Module):\n",
    "    def __init__(self, pretrained_path=None, num_classes=3):\n",
    "        super().__init__()\n",
    "        backbone = cbam_resnet50(num_classes=1000)\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        if pretrained_path:\n",
    "            ckpt = torch.load(pretrained_path, map_location='cpu')\n",
    "            if 'backbone' in ckpt:\n",
    "                self.backbone.load_state_dict(ckpt['backbone'], strict=False)\n",
    "            else:\n",
    "                self.backbone.load_state_dict(ckpt, strict=False)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# --- Load model weights (set this to your actual path if needed) ---\n",
    "finetuned_ckpt_path = \"best_finetuned_cbam.pth\"\n",
    "num_classes = 3\n",
    "class_names = ['Alternaria', 'Healthy Leaf', 'straw_mite']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FineTuneCBAM(pretrained_path=None, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(finetuned_ckpt_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded fine-tuned model.\")\n",
    "\n",
    "# --- Test dataset loader ---\n",
    "test_dir = \"/kaggle/input/minida/mini_output1/test\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "inv_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0., 0., 0.], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "    transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1., 1., 1.])\n",
    "])\n",
    "dataset = ImageFolder(test_dir, transform=transform)\n",
    "print(f\"Loaded {len(dataset)} test images.\")\n",
    "\n",
    "# --- XAI classes: GradCAM, GradCAM++, LayerCAM ---\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "    def _register_hooks(self):\n",
    "        def fw_hook(_, __, output):\n",
    "            self.activations = output.detach()\n",
    "        def bw_hook(_, __, grad_output):\n",
    "            self.gradients = grad_output[0].detach()\n",
    "        self.target_layer.register_forward_hook(fw_hook)\n",
    "        self.target_layer.register_full_backward_hook(bw_hook)\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward(retain_graph=True)\n",
    "        weights = self.gradients.mean(dim=(2,3))[0]\n",
    "        cam = torch.zeros(self.activations.shape[2:], device=input_tensor.device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * self.activations[0, i]\n",
    "        cam = F.relu(cam)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-10)\n",
    "        return cam.cpu().numpy(), class_idx\n",
    "\n",
    "class GradCAMPlusPlus(GradCAM):\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward(retain_graph=True)\n",
    "        g = self.gradients\n",
    "        a = self.activations\n",
    "        alpha_num = g.pow(2)\n",
    "        alpha_denom = 2 * g.pow(2) + (a * g.pow(3)).sum(dim=(2, 3), keepdim=True)\n",
    "        alpha = alpha_num / (alpha_denom + 1e-7)\n",
    "        weights = (alpha * F.relu(g)).sum(dim=(2, 3))[0]\n",
    "        cam = torch.zeros(a.shape[2:], device=input_tensor.device)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * a[0, i]\n",
    "        cam = F.relu(cam)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-10)\n",
    "        return cam.cpu().numpy(), class_idx\n",
    "\n",
    "class LayerCAM(GradCAM):\n",
    "    def generate(self, input_tensor, class_idx=None):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1).item()\n",
    "        self.model.zero_grad()\n",
    "        output[0, class_idx].backward(retain_graph=True)\n",
    "        cam = (self.gradients * self.activations).sum(dim=1)[0]\n",
    "        cam = F.relu(cam)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-10)\n",
    "        return cam.cpu().numpy(), class_idx\n",
    "\n",
    "# --- Choose a target layer (typically last conv, but -2/-3 sometimes better) ---\n",
    "target_layer = model.backbone[-1]  # You can try -2, -3 for more spatial\n",
    "print(\"Target layer for CAM:\", target_layer)\n",
    "\n",
    "# --- Pick one sample per class ---\n",
    "seen = set()\n",
    "samples = []\n",
    "for img, label in dataset:\n",
    "    if label not in seen:\n",
    "        samples.append((img, label))\n",
    "        seen.add(label)\n",
    "    if len(seen) == num_classes:\n",
    "        break\n",
    "\n",
    "# --- Visualize CAMs for each class ---\n",
    "for img_tensor, label in samples:\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    # Get the original image for overlay\n",
    "    original_img = inv_transform(img_tensor[0].cpu()).permute(1,2,0).numpy()\n",
    "    original_img = np.clip(original_img * 255, 0, 255).astype(np.uint8)\n",
    "    # Generate CAMs\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    cam1, _ = gradcam.generate(img_tensor)\n",
    "    gradcampp = GradCAMPlusPlus(model, target_layer)\n",
    "    cam2, _ = gradcampp.generate(img_tensor)\n",
    "    layercam = LayerCAM(model, target_layer)\n",
    "    cam3, _ = layercam.generate(img_tensor)\n",
    "    # Overlay utility\n",
    "    def overlay(img, cam):\n",
    "        cam = cv2.resize(cam, (224,224))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255*cam), cv2.COLORMAP_JET)\n",
    "        overlayed = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
    "        return overlayed\n",
    "    # Plot all\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(f\"Original\\nClass: {class_names[label]}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(overlay(original_img, cam1))\n",
    "    plt.title(\"GradCAM\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(overlay(original_img, cam2))\n",
    "    plt.title(\"GradCAM++\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(overlay(original_img, cam3))\n",
    "    plt.title(\"LayerCAM\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7808913,
     "sourceId": 12383979,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
